PS D:\soft\ik_llama.cpp\llama-b3667-bin-win-avx512-x64> ./llama-sweep-bench -m "D:\Models\download\unsloth\Qwen3-30B-A3B-128K-GGUF\files\Qwen3-30B-A3B-128K-Q8_0.gguf" -c 16384 -t 20 -ngl 0 -fa
warning: not compiled with GPU offload support, --gpu-layers option will be ignored
warning: see main README.md for information on enabling GPU BLAS support
llama_model_loader: loaded meta data with 47 key-value pairs and 579 tensors from D:\Models\download\unsloth\Qwen3-30B-A3B-128K-GGUF\files\Qwen3-30B-A3B-128K-Q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3moe
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3-30B-A3B-128K
llama_model_loader: - kv   3:                           general.finetune str              = 128k
llama_model_loader: - kv   4:                           general.basename str              = Qwen3-30B-A3B-128K
llama_model_loader: - kv   5:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   6:                         general.size_label str              = 30B-A3B
llama_model_loader: - kv   7:                            general.license str              = apache-2.0
llama_model_loader: - kv   8:                       general.license.link str              = https://huggingface.co/Qwen/Qwen3-30B...
llama_model_loader: - kv   9:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv  10:                   general.base_model.count u32              = 1
llama_model_loader: - kv  11:                  general.base_model.0.name str              = Qwen3 30B A3B
llama_model_loader: - kv  12:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  13:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen3-30B...
llama_model_loader: - kv  14:                               general.tags arr[str,2]       = ["unsloth", "text-generation"]
llama_model_loader: - kv  15:                       qwen3moe.block_count u32              = 48
llama_model_loader: - kv  16:                    qwen3moe.context_length u32              = 131072
llama_model_loader: - kv  17:                  qwen3moe.embedding_length u32              = 2048
llama_model_loader: - kv  18:               qwen3moe.feed_forward_length u32              = 6144
llama_model_loader: - kv  19:              qwen3moe.attention.head_count u32              = 32
llama_model_loader: - kv  20:           qwen3moe.attention.head_count_kv u32              = 4
llama_model_loader: - kv  21:                    qwen3moe.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  22:  qwen3moe.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  23:                 qwen3moe.expert_used_count u32              = 8
llama_model_loader: - kv  24:              qwen3moe.attention.key_length u32              = 128
llama_model_loader: - kv  25:            qwen3moe.attention.value_length u32              = 128
llama_model_loader: - kv  26:                      qwen3moe.expert_count u32              = 128
llama_model_loader: - kv  27:        qwen3moe.expert_feed_forward_length u32              = 768
llama_model_loader: - kv  28:                 qwen3moe.rope.scaling.type str              = yarn
llama_model_loader: - kv  29:               qwen3moe.rope.scaling.factor f32              = 4.000000
llama_model_loader: - kv  30: qwen3moe.rope.scaling.original_context_length u32              = 32768
llama_model_loader: - kv  31:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  32:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  33:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  34:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  35:                      tokenizer.ggml.merges arr[str,151387]  = ["臓 臓", "臓臓 臓臓", "i n", "臓 t",...
llama_model_loader: - kv  36:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  37:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  38:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  39:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  40:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  41:               general.quantization_version u32              = 2
llama_model_loader: - kv  42:                          general.file_type u32              = 7
llama_model_loader: - kv  43:                      quantize.imatrix.file str              = Qwen3-30B-A3B-128K-GGUF/imatrix_unslo...
llama_model_loader: - kv  44:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-30B-A3B-128...
llama_model_loader: - kv  45:             quantize.imatrix.entries_count i32              = 384
llama_model_loader: - kv  46:              quantize.imatrix.chunks_count i32              = 32
llama_model_loader: - type  f32:  241 tensors
llama_model_loader: - type q8_0:  338 tensors
llm_load_vocab: special tokens cache size = 26
llm_load_vocab: token to piece cache size = 0.9311 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen3moe
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 151936
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 48
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 4
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_swa_pattern    = 1
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 8
llm_load_print_meta: n_embd_k_gqa     = 512
llm_load_print_meta: n_embd_v_gqa     = 512
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 6144
llm_load_print_meta: n_expert         = 128
llm_load_print_meta: n_expert_used    = 8
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = yarn
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 0.25
llm_load_print_meta: n_ctx_orig_yarn  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 30.532 B
llm_load_print_meta: model size       = 30.247 GiB (8.510 BPW)
llm_load_print_meta: repeating layers = 29.632 GiB (8.510 BPW, 29.910 B parameters)
llm_load_print_meta: general.name     = Qwen3-30B-A3B-128K
llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'
llm_load_print_meta: EOS token        = 151645 '<|im_end|>'
llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'
llm_load_print_meta: LF token         = 148848 '脛默'
llm_load_print_meta: EOT token        = 151645 '<|im_end|>'
llm_load_print_meta: max token length = 256
llm_load_print_meta: n_ff_exp         = 768
llm_load_tensors: ggml ctx size =    0.25 MiB
llm_load_tensors:        CPU buffer size = 30973.40 MiB
....................................................................................................
llama_new_context_with_model: n_ctx      = 16384
llama_new_context_with_model: n_batch    = 2048
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 1
llama_new_context_with_model: mla_attn   = 0
llama_new_context_with_model: attn_max_b = 0
llama_new_context_with_model: fused_moe  = 0
llama_new_context_with_model: ser        = -1, 0
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 0.25
llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB
llama_new_context_with_model:        CPU compute buffer size =   304.75 MiB
llama_new_context_with_model: graph nodes  = 1974
llama_new_context_with_model: graph splits = 1

main: n_kv_max = 16384, n_batch = 2048, n_ubatch = 512, flash_attn = 1, n_gpu_layers = 0, n_threads = 20, n_threads_batch = 20

|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |
|-------|--------|--------|----------|----------|----------|----------|
|   512 |    128 |      0 |    8.519 |    60.10 |    9.924 |    12.90 |
|   512 |    128 |    512 |    8.950 |    57.21 |   10.045 |    12.74 |
|   512 |    128 |   1024 |    9.279 |    55.18 |   10.204 |    12.54 |
|   512 |    128 |   1536 |    9.648 |    53.07 |   10.613 |    12.06 |
|   512 |    128 |   2048 |   10.097 |    50.71 |   10.722 |    11.94 |
|   512 |    128 |   2560 |   10.486 |    48.83 |   11.015 |    11.62 |
|   512 |    128 |   3072 |   10.999 |    46.55 |   11.164 |    11.47 |
|   512 |    128 |   3584 |   11.336 |    45.17 |   11.139 |    11.49 |
|   512 |    128 |   4096 |   12.480 |    41.03 |   11.718 |    10.92 |
|   512 |    128 |   4608 |   12.244 |    41.82 |   11.725 |    10.92 |
|   512 |    128 |   5120 |   12.551 |    40.79 |   12.213 |    10.48 |
|   512 |    128 |   5632 |   13.537 |    37.82 |   12.453 |    10.28 |
|   512 |    128 |   6144 |   13.356 |    38.34 |   12.584 |    10.17 |
|   512 |    128 |   6656 |   13.847 |    36.98 |   12.603 |    10.16 |
|   512 |    128 |   7168 |   14.128 |    36.24 |   12.656 |    10.11 |
|   512 |    128 |   7680 |   14.631 |    34.99 |   13.198 |     9.70 |
|   512 |    128 |   8192 |   15.002 |    34.13 |   13.520 |     9.47 |
|   512 |    128 |   8704 |   15.356 |    33.34 |   13.095 |     9.77 |
|   512 |    128 |   9216 |   16.050 |    31.90 |   13.614 |     9.40 |
|   512 |    128 |   9728 |   16.395 |    31.23 |   13.093 |     9.78 |
|   512 |    128 |  10240 |   16.790 |    30.49 |   14.537 |     8.80 |
|   512 |    128 |  10752 |   17.052 |    30.03 |   14.793 |     8.65 |
|   512 |    128 |  11264 |   17.668 |    28.98 |   13.957 |     9.17 |
|   512 |    128 |  11776 |   18.276 |    28.02 |   15.028 |     8.52 |
|   512 |    128 |  12288 |   18.335 |    27.92 |   15.267 |     8.38 |
|   512 |    128 |  12800 |   19.061 |    26.86 |   15.272 |     8.38 |
|   512 |    128 |  13312 |   19.379 |    26.42 |   15.310 |     8.36 |
|   512 |    128 |  13824 |   19.764 |    25.91 |   15.000 |     8.53 |
|   512 |    128 |  14336 |   20.432 |    25.06 |   15.612 |     8.20 |
|   512 |    128 |  14848 |   21.632 |    23.67 |   15.587 |     8.21 |
|   512 |    128 |  15360 |   22.311 |    22.95 |   17.303 |     7.40 |
|   512 |    128 |  15872 |   21.767 |    23.52 |   16.894 |     7.58 |