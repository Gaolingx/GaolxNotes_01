# AI集群高可用设计

### 🧑‍💻 User

万卡训练集群 可能平均每几个小时就会出现掉卡情况，从基础设施（网络、存储、计算等）到上层应用（训练框架、平台监控等）是如何保证ai集群高可用的？

---

### 🤖 Assistant

在万卡（10,000+ GPUs）级别的AI训练集群中，硬件故障不仅是“可能”发生的，而是**必然**发生的常态。

假设单个GPU的平均无故障时间（MTBF）为3年（约26,000小时），那么在一个拥有10,000张卡的集群中，集群层面的MTBF计算如下：

$$ MTBF_{cluster} = \frac{MTBF_{GPU}}{N} = \frac{26,000}{10,000} \approx 2.6 \text{ hours} $$

这意味着，平均每2.6小时就会有一张卡出现故障。如果不做容错处理，大模型训练将永远无法完成。

保证高可用的核心策略不是“消除故障”，而是**“快速检测、快速隔离、快速恢复”**，将故障对训练进度的影响降到最低。这需要从基础设施到应用层的全栈协同。

---

### 一、 基础设施层 (Infrastructure Layer)

基础设施层是高可用的基石，重点在于**冗余（Redundancy）**和**隔离（Isolation）**。

#### 1. 计算与节点 (Compute & Nodes)
*   **硬件健康检查 (Health Checks):** 在作业启动前，通过自动化脚本（如 `Brief NCCL Test`）对节点进行快速体检。如果发现显存ECC错误、PCIe带宽异常，直接标记为不可用（Drain），不让其进入作业调度。
*   **热备节点 (Hot Spares):** 万卡集群通常会保留 3% - 5% 的空闲算力作为“热备池”。当训练任务中的某个节点宕机时，不需要等待维修，而是立即从热备池中调度一个健康节点顶替。
*   **硬件隔离:** 针对GPU掉卡（如 NVIDIA Xid 79 错误），服务器BMC（基板管理控制器）需具备快速复位或下电隔离故障卡的能力，防止故障蔓延导致整机死机。

#### 2. 网络互联 (Network)
网络故障是导致训练中断的第二大原因（仅次于GPU故障）。
*   **多轨网络 (Rail-optimized Network):** 构建多层网络架构。如果某条链路拥塞或断开，流量可以通过其他路径传输。
*   **自适应路由 (Adaptive Routing):** 在InfiniBand或RoCE网络中，交换机具备动态感知链路状态的能力。如果某条路径断开，硬件层直接将数据包重新路由，上层应用几乎无感。
*   **光模块监控:** 光模块（Optical Transceiver）在大规模集群中极易损坏。通过通过监控收光功率（RX Power）和误码率（BER），提前预测并更换即将失效的线缆。

#### 3. 存储系统 (Storage)
存储不仅要存数据，更是容错的“救生圈”（用于存Checkpoint）。
*   **高性能并行文件系统:** 使用 Lustre, GPFS (IBM Spectrum Scale) 或专门针对AI优化的存储（如WEKA, DAOS）。
*   **Burst Buffer 技术:** 在内存和磁盘之间引入NVMe SSD层。Checkpoint写入时先由Burst Buffer极速承接（秒级写入），再异步刷写到机械硬盘（HDD），减少GPU等待I/O的时间。

---

### 二、 平台与调度层 (Platform & Scheduling Layer)

这一层负责指挥，核心是**自动化运维（AIOps）**。

#### 1. 故障感知与自动拓扑感知调度
*   **K8s Operator / Slurm:** 调度器需要感知物理拓扑。当替换坏节点时，新节点必须位于网络拓扑中相近的位置（例如同一个Tor交换机下），以保证通信效率，避免因跨交换机跳数过多导致训练性能下降。
*   **节点自动驱逐:** 监控系统（如Prometheus + DCGM）一旦检测到特定错误代码（如 Xid 48, 61, 79），Kubernetes 会自动 Taint 该节点，触发 Pod 驱逐和重新调度。

#### 2. 慢节点检测 (Straggler Detection)
*   在大规模同步训练中，整个集群的速度取决于**最慢的那张卡**。
*   平台需实时监控每个Rank的计算和通信耗时。如果发现某张卡显著慢于平均值（可能是由于降频、散热问题或网络丢包），平台会将其视为“亚健康”故障，主动触发迁移，防止拖慢整体进度。

---

### 三、 训练框架与应用层 (Framework & Application Layer)

这是应对故障的最后一道防线，核心是**检查点（Checkpointing）**和**弹性训练（Elastic Training）**。

#### 1. 高频且高效的 Checkpointing
由于故障不可避免，训练必须频繁保存状态（权重、优化器状态、学习率等）。
*   **异步 Checkpoint:** 训练计算不停止，将模型状态拷贝到CPU内存，由CPU异步写入磁盘。
*   **增量 Checkpoint:** 只保存自上次保存以来发生变化的部分（虽然在全量更新的训练中较少用，但在特定场景有效）。
*   **内存快照 (In-Memory Checkpoint):** 利用多副本机制，将Checkpoint分散保存在集群中其他节点的内存里。如果节点A挂了，可以从节点B的内存中极速恢复，无需读盘。

#### 2. 弹性训练框架 (e.g., PyTorch Elastic / TorchElastic)
*   传统训练中，1个Rank挂掉，所有进程报错退出。
*   **TorchElastic** 允许训练任务在节点数量变化时动态调整。
*   **自动重启 (Auto-Resume):** 框架捕获到 NCCL 通信超时或报错后，不完全退出程序，而是重新初始化通信环（Re-init Process Group），加载最近的 Checkpoint 继续训练。整个过程自动化，无需人工干预。

#### 3. 3D 并行策略优化
*   在万卡集群中，通常混合使用**数据并行 (DP)、张量并行 (TP)、流水线并行 (PP)**。
*   框架需要优化故障恢复的粒度。例如，如果只是做数据并行的一组节点挂了，理论上不影响其他数据并行组，只需要恢复这一组即可（虽然目前主流框架大多还是全局重启，但这是优化方向）。

---

### 四、 故障恢复全流程示例 (The Recovery Workflow)

当万卡集群中某张 GPU 突然掉线（Off the bus），系统会自动执行以下流程：

1.  **检测 (Detection):**
    *   DCGM 抛出 `Xid 79` 错误。
    *   应用层 NCCL 通信超时（Watchdog Timeout）。
    *   耗时：**秒级**。

2.  **隔离与上报 (Isolation):**
    *   监控系统捕获错误，通知 K8s/调度器。
    *   该节点被标记为 `Unscheduled`，并触发工单通知运维更换硬件。

3.  **重新调度 (Rescheduling):**
    *   训练任务暂停（或者Crash）。
    *   调度器从**热备池**中选取一个网络位置最优的健康节点填补空缺。
    *   耗时：**分钟级**（主要取决于Pod启动和环境加载时间）。

4.  **状态恢复 (Recovery):**
    *   新起的 Pod 加入训练组。
    *   所有节点从共享存储加载最近一次的 Checkpoint（可能损失过去几十分钟的迭代成果）。
    *   耗时：**分钟级**（取决于模型大小和存储带宽，TB级模型加载需要时间）。

5.  **继续训练 (Resume):**
    *   训练继续进行。

### 总结

万卡集群的高可用本质上是一个**数学博弈**：

$$ \text{有效训练时间} = \text{总时间} - (\text{故障检测时间} + \text{恢复启动时间} + \text{回滚损失的时间}) $$

要保证高可用，就是通过**更好的硬件（减少故障率）、更快的网络与存储（加快Checkpoints读写）、更智能的调度（热备秒级切换）**来极大地压缩公式后半部分的耗时。

目前业界最优秀的万卡集群（如OpenAI, Meta, Google），其目标是将**有效训练时间占比（MFU, Model FLOPs Utilization 之外的 Availability）** 维持在 **95% 以上**。

---

### 🧑‍💻 User

掉卡肯定意味着task会挂，我看有方案是按照时间轮次进行保存，监控节点状态，掉了就重新按照存活的节点进行任务分配加载之前保存的参数进行再训练 ，现在都是这么干的还是有更高级的用法？

---

### 🤖 Assistant

你说的那种“掉卡 -> 任务挂掉 -> 重新调度 -> 加载Checkpoint -> 继续训练”的流程，确实是目前业界的**主流基线方案**，被称为 **Check-pointing & Restart (C/R)** 机制。

但是，对于万卡集群来说，仅仅依靠这种原始的“冷启动”方式是远远不够的。因为随着卡数增加，故障频率会呈指数级上升，如果每次恢复都要走一遍漫长的“排队、初始化、读盘加载”，有效训练时间（Goodput）会跌到无法接受的程度。

因此，现在的“高级用法”核心不在于颠覆这个流程，而在于**把这个流程压缩到极致**，甚至做到**故障对上层“近乎透明”**。

以下是目前大厂（如OpenAI, Google, Meta, 阿里云, 字节跳动等）在万卡集群中采用的进阶方案，按“高级程度”递增排序：

---

### Level 1: 异步与内存级 Checkpoint (不仅是写磁盘)

传统的做法是每隔几小时停下来，把几TB的模型参数写到硬盘（分布式存储）上。这很慢且阻塞训练。

**高级用法：**

1.  **异步 Checkpoint (Async Checkpointing):**
    *   训练（GPU计算）不停止。系统利用 PCIe 带宽，将 GPU 显存中的模型状态快速拷贝到 CPU 内存（Pinned Memory），然后由 CPU 线程在后台慢慢写入磁盘。
    *   **效果：** 保存 Checkpoint 的开销接近于 0。

2.  **多级 Checkpoint (Hierarchical / In-Memory Checkpointing):**
    *   **原理：** 只有在整个数据中心断电时才需要从硬盘恢复。如果是单卡故障，为什么要读硬盘？
    *   **做法：** 引入类似 HPC 领域的 SCR (Scalable Checkpoint/Restart) 机制。每隔 10 分钟做一个**内存快照**，并将这份快照**复制到邻近节点的内存中**（互为备份）。
    *   **恢复：** 当 Node A 挂了，顶替上来的 Node A' 不需要去读慢速的云存储，而是直接通过极速的网络（InfiniBand/RoCE）从 Node B 的内存里拉取备份。
    *   **速度：** 恢复速度提升 10 倍以上。

---

### Level 2: 热备池与原地复活 (Hot Spares & Warm Restart)

你提到的“重新按照存活节点分配”，在大模型训练中其实很痛苦。因为大模型通常采用 3D 并行（TP+PP+DP），如果少了一个节点，拓扑结构变了，模型切分方式就得变，这非常复杂且耗时。

**高级用法：**

1.  **Hot Spares (热备节点):**
    *   **不改变拓扑：** 集群里始终预留几百张卡空转（或跑低优先级任务）。一旦监控发现某张卡挂了，调度器**不重新计算并行策略**，而是直接把一个热备节点“映射”到故障节点的位置（IP飘移或逻辑重组）。
    *   **效果：** 维持 $N$ 个节点不变，避免了复杂的模型重切分（Resharding）。

2.  **TorchElastic / 弹性容错框架:**
    *   传统作业挂了是整个 Process Group 销毁，回到 Slurm/K8s 队列重新排队。
    *   高级框架支持**代理模式**。主控进程（Agent）不退，发现 Worker 挂了，Agent 直接拉起新 Worker，在这个已有的 Job 内部完成重组。
    *   **效果：** 省去了作业调度器的排队时间（Queue Time），实现秒级重连。

---

### Level 3: 确定性回放与快速跳过 (Deterministic Replay)

当加载了 30 分钟前的 Checkpoint 后，通常需要重新跑这 30 分钟的数据。

**高级用法：**

1.  **Data Loader 的状态保存:**
    *   不仅保存模型参数，还严格保存 Data Loader 的游标（处理到了第几个文件的第几行）。
2.  **快速跳过 (Fast-forward):**
    *   在恢复训练的最初阶段，只加载数据不进行计算，快速把随机数生成器（RNG）的状态“空转”到故障发生的时刻，确保数学上的严格一致性，防止因为随机性导致 Loss 波动。

---

### Level 4: 冗余计算 (Redundant Computation) —— 极少数场景

这是最暴力但也最高级的方法，通常用于极大规模（如几万卡）且对稳定性要求极高的冲刺阶段。

**高级用法：**

*   **原理：** 假设你有 10,000 张卡，你只用 9,000 张跑训练，剩下 1,000 张卡作为**实时镜像**。
*   **做法：** 关键计算任务在两组不同的硬件上同时跑（类似飞机的双发引擎）。如果组 A 的卡挂了，系统直接丢弃组 A 的结果，使用组 B 的结果继续，**训练完全不中断**。
*   **代价：** 算力浪费巨大，一般商业公司玩不起，但在冲击万亿参数模型的关键时刻可能会局部使用。

---

### Level 5: 硬件级与算子级容错 (The "Holy Grail")

这是目前 NVIDIA 和 Google TPU 正在攻克的方向，试图让软件层完全无感。

**高级用法：**

1.  **算子级重试:**
    *   如果 GPU 在做矩阵乘法时发生了 ECC 校验错误（比特翻转），以前是直接抛出 Xid 错误并宕机。
    *   现在的思路是：硬件或底层 Driver 捕获这个错误，**仅在当前指令上进行重试**。如果重试成功，上层 PyTorch 根本不知道发生过故障。
2.  **UD (User-Defined) Checkpointing:**
    *   在显存内部做局部的 Checkpoint。比如在前向传播（Forward）结束时，把 Activation 存在显存的安全区域。如果反向传播（Backward）时出错了，直接从显存加载 Activation 重算，而不需要回滚到上一个小时。

### 总结

现在的万卡集群方案，虽然逻辑上还是 **“挂了 -> 恢复”**，但在实现细节上已经进化了：

1.  **以前：** 挂了 -> 报警 -> 人工介入/重新排队 -> 读云存储 -> 慢速恢复（耗时：小时级）。
2.  **现在：** 挂了 -> 自动热备替补 -> 读邻居内存 -> 快速追赶（耗时：**分钟级甚至秒级**）。

目标是将故障带来的**算力损失（MFU loss）**控制在 2%-5% 以内。如果还是像以前那样“重新分配再训练”，在万卡规模下，模型可能永远都训练不完（因为恢复的时间比下一次故障到来的时间还长）。