# PyTorchè°ƒç”¨CUDAç®—å­å®ç°æŒ‡å—

### ğŸ§‘â€ğŸ’» User

å¦‚ä½•ç”¨torch.utils.cpp_extensionå®ç°pytorché‡Œé¢è°ƒç”¨cudaç®—å­ï¼Ÿ

---

### ğŸ¤– Assistant

# ä½¿ç”¨torch.utils.cpp_extensionå®ç°PyTorchä¸­è°ƒç”¨CUDAç®—å­

`torch.utils.cpp_extension` æ˜¯PyTorchæä¾›çš„ä¸€ä¸ªå¼ºå¤§å·¥å…·ï¼Œå…è®¸ç”¨æˆ·ç¼–å†™C++/CUDAä»£ç å¹¶å°†å…¶ç¼–è¯‘ä¸ºPythonå¯è°ƒç”¨çš„æ¨¡å—ã€‚ä¸‹é¢æˆ‘å°†è¯¦ç»†ä»‹ç»å¦‚ä½•ä½¿ç”¨å®ƒæ¥è‡ªå®šä¹‰CUDAç®—å­ã€‚

## åŸºæœ¬æ­¥éª¤

1. ç¼–å†™CUDAå†…æ ¸ä»£ç 
2. ç¼–å†™C++åŒ…è£…å‡½æ•°
3. ç¼–å†™Pythonç»‘å®šä»£ç 
4. ä½¿ç”¨JITç¼–è¯‘æˆ–setup.pyæ–¹å¼ç¼–è¯‘

## ç¤ºä¾‹ï¼šå®ç°ä¸€ä¸ªç®€å•çš„åŠ æ³•ç®—å­

### 1. ç¼–å†™CUDAå†…æ ¸ä»£ç 

é¦–å…ˆåˆ›å»ºä¸€ä¸ªåä¸º`add_kernel.cu`çš„æ–‡ä»¶ï¼š

```cuda
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

template <typename scalar_t>
__global__ void add_cuda_kernel(
    torch::PackedTensorAccessor32<scalar_t, 1, torch::RestrictPtrTraits> a,
    torch::PackedTensorAccessor32<scalar_t, 1, torch::RestrictPtrTraits> b,
    torch::PackedTensorAccessor32<scalar_t, 1, torch::RestrictPtrTraits> out) {

  const int idx = blockIdx.x * blockDim.x + threadIdx.x;
  if (idx < a.size(0)) {
    out[idx] = a[idx] + b[idx];
  }
}

torch::Tensor add_cuda(torch::Tensor a, torch::Tensor b) {
  // æ£€æŸ¥è¾“å…¥å¼ é‡
  TORCH_CHECK(a.device().is_cuda(), "a must be a CUDA tensor");
  TORCH_CHECK(b.device().is_cuda(), "b must be a CUDA tensor");
  TORCH_CHECK(a.sizes() == b.sizes(), "a and b must have the same shape");

  // åˆ›å»ºè¾“å‡ºå¼ é‡
  auto out = torch::empty_like(a);

  // è®¾ç½®CUDAçº¿ç¨‹ç½‘æ ¼
  const int64_t num_elements = a.numel();
  const int block_size = 256;
  const int num_blocks = (num_elements + block_size - 1) / block_size;

  // æ ¹æ®æ•°æ®ç±»å‹è°ƒç”¨ä¸åŒçš„å†…æ ¸
  AT_DISPATCH_ALL_TYPES(a.type(), "add_cuda", ([&] {
    add_cuda_kernel<scalar_t><<<num_blocks, block_size>>>(
        a.packed_accessor32<scalar_t, 1, torch::RestrictPtrTraits>(),
        b.packed_accessor32<scalar_t, 1, torch::RestrictPtrTraits>(),
        out.packed_accessor32<scalar_t, 1, torch::RestrictPtrTraits>());
  }));

  return out;
}
```

### 2. ç¼–å†™C++åŒ…è£…å‡½æ•°

åˆ›å»ºä¸€ä¸ªåä¸º`add.cpp`çš„æ–‡ä»¶ï¼š

```cpp
#include <torch/extension.h>

// å£°æ˜CUDAå‡½æ•°
torch::Tensor add_cuda(torch::Tensor a, torch::Tensor b);

// CPUç‰ˆæœ¬çš„å®ç°ï¼ˆå¯é€‰ï¼‰
torch::Tensor add_cpu(torch::Tensor a, torch::Tensor b) {
  return a + b;
}

// åˆ†å‘å‡½æ•°
torch::Tensor add(torch::Tensor a, torch::Tensor b) {
  // æ ¹æ®å¼ é‡è®¾å¤‡ç±»å‹é€‰æ‹©å®ç°
  if (a.device().is_cuda()) {
    return add_cuda(a.contiguous(), b.contiguous());
  } else {
    return add_cpu(a, b);
  }
}

// Pythonç»‘å®š
PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
  m.def("add", &add, "Add two tensors (CUDA)");
}
```

### 3. ä½¿ç”¨JITç¼–è¯‘æ–¹å¼

åˆ›å»ºä¸€ä¸ªPythonè„šæœ¬`test_add.py`ï¼š

```python
import torch
from torch.utils.cpp_extension import load

# ä½¿ç”¨JITç¼–è¯‘
cuda_add_module = load(
    name="cuda_add",
    sources=["add.cpp", "add_kernel.cu"],
    verbose=True,
    extra_cuda_cflags=["-O2", "--use_fast_math"],
    extra_cflags=["-O2"]
)

# æµ‹è¯•å‡½æ•°
if __name__ == "__main__":
    # ä½¿ç”¨CPU
    a_cpu = torch.randn(10)
    b_cpu = torch.randn(10)
    result_cpu = cuda_add_module.add(a_cpu, b_cpu)
    print("CPU result:", result_cpu)
    print("CPU check:", torch.allclose(result_cpu, a_cpu + b_cpu))
  
    # ä½¿ç”¨CUDA
    if torch.cuda.is_available():
        a_cuda = a_cpu.cuda()
        b_cuda = b_cpu.cuda()
        result_cuda = cuda_add_module.add(a_cuda, b_cuda)
        print("CUDA result:", result_cuda)
        print("CUDA check:", torch.allclose(result_cuda, a_cuda + b_cuda))
```

### 4. ä½¿ç”¨setup.pyæ–¹å¼ï¼ˆæ›´æ­£å¼çš„é¡¹ç›®ï¼‰

åˆ›å»º`setup.py`æ–‡ä»¶ï¼š

```python
from setuptools import setup
from torch.utils.cpp_extension import CUDAExtension, BuildExtension

setup(
    name="cuda_add",
    ext_modules=[
        CUDAExtension(
            name="cuda_add",
            sources=["add.cpp", "add_kernel.cu"],
            extra_compile_args={
                "cxx": ["-O2"],
                "nvcc": ["-O2", "--use_fast_math"]
            }
        )
    ],
    cmdclass={"build_ext": BuildExtension}
)
```

ç„¶åé€šè¿‡ä»¥ä¸‹å‘½ä»¤å®‰è£…ï¼š

```
python setup.py install
```

## é«˜çº§æŠ€å·§

### 1. è‡ªå®šä¹‰ç¼–è¯‘é€‰é¡¹

```python
torch.utils.cpp_extension.load(
    name="custom_extension",
    sources=["source1.cpp", "source2.cu"],
    extra_cuda_cflags=["-arch=sm_70", "-O3", "--use_fast_math"],
    extra_cflags=["-O3", "-march=native"],
    extra_ldflags=["-L/usr/local/cuda/lib64"],
    build_directory="./build",  # è‡ªå®šä¹‰æ„å»ºç›®å½•
    verbose=True  # æ‰“å°ç¼–è¯‘ä¿¡æ¯
)
```

### 2. æ¡ä»¶ç¼–è¯‘

å¯ä»¥æ£€æŸ¥æ˜¯å¦ä¸ºWindowsæˆ–CUDAå¯ç”¨ç­‰æ¡ä»¶ï¼š

```cpp
#ifdef _WIN32
#  define API extern "C" __declspec(dllexport)
#else
#  define API extern "C"
#endif
```

### 3. è°ƒè¯•æŠ€å·§

åœ¨CUDAä»£ç ä¸­ä½¿ç”¨ï¼š

```cuda
#define CUDA_KERNEL_LOOP(i, n) \
  for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < (n); i += blockDim.x * gridDim.x)

template <typename scalar_t>
__global__ void kernel(...) {
  CUDA_KERNEL_LOOP(i, n) {
    // å†…æ ¸ä»£ç 
  }
}
```

### 4. æ€§èƒ½ä¼˜åŒ–æŠ€å·§

- ä½¿ç”¨å‘é‡åŒ–åŠ è½½/å­˜å‚¨
- åˆå¹¶å…¨å±€å†…å­˜è®¿é—®
- é¿å…çº¿ç¨‹åˆ†æ­§
- ä½¿ç”¨å…±äº«å†…å­˜å‡å°‘å…¨å±€å†…å­˜è®¿é—®
- ä½¿ç”¨å¸¸é‡å†…å­˜å­˜å‚¨åªè¯»æ•°æ®

## å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

1. **ç¼–è¯‘é”™è¯¯ï¼šæ‰¾ä¸åˆ°å¤´æ–‡ä»¶**
   - ç¡®ä¿æ­£ç¡®è®¾ç½®äº†`CXX`å’Œ`CUDA_HOME`ç¯å¢ƒå˜é‡
   - ä½¿ç”¨`torch.utils.cpp_extension.CUDAExtension`è€Œä¸æ˜¯æ™®é€šçš„Extension

2. **è¿è¡Œæ—¶é”™è¯¯ï¼šæ— æ•ˆçš„è®¾å¤‡å‡½æ•°**
   - æ£€æŸ¥CUDAæ¶æ„æ˜¯å¦åŒ¹é…ï¼ˆå¦‚sm_70ï¼‰
   - ç¡®ä¿è®¾å¤‡æ”¯æŒçš„CUDAç‰ˆæœ¬ä¸ä»£ç å…¼å®¹

3. **æ€§èƒ½ä¸ä½³**
   - ä½¿ç”¨`nvprof`æˆ–`nsight`åˆ†ææ€§èƒ½ç“¶é¢ˆ
   - ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼
   - æ£€æŸ¥æ˜¯å¦å……åˆ†åˆ©ç”¨äº†GPUå¹¶è¡Œæ€§

é€šè¿‡è¿™ç§æ–¹å¼ï¼Œä½ å¯ä»¥å°†é«˜æ€§èƒ½çš„CUDAä»£ç æ— ç¼é›†æˆåˆ°PyTorchä¸­ï¼Œå……åˆ†åˆ©ç”¨GPUçš„è®¡ç®—èƒ½åŠ›ã€‚