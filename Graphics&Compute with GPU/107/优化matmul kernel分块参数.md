# ä¼˜åŒ–matmul kernelåˆ†å—å‚æ•°

### ğŸ§‘â€ğŸ’» User

ç»“åˆhopperæ¶æ„ç‰¹ç‚¹ï¼Œè¯´è¯´ä¼˜åŒ–matmul kernelçš„æ—¶å€™ï¼Œå¦‚ä½•æ ¹æ®æ¶æ„ç»†èŠ‚æ¨ç®—å‡ºç†è®ºä¸Šèƒ½è¾¾åˆ°å³°å€¼çš„åˆ†å—å‚æ•°

---

### ğŸ¤– Assistant

å¾ˆå¥½çš„é—®é¢˜ â€” åœ¨é’ˆå¯¹ NVIDIA Hopper æ¶æ„ï¼ˆä¾‹å¦‚ H100 / H200ï¼‰ä¼˜åŒ– matmulï¼ˆGEMMï¼‰kernel æ—¶ï¼Œé€šè¿‡æ¶æ„ç»†èŠ‚ç¡®å®å¯ä»¥å¤§è‡´æ¨ç®—å‡ºâ€œç†è®ºä¸Šèƒ½æ¥è¿‘å³°å€¼ï¼ˆpeak FLOPSï¼‰â€æ—¶æ‰€éœ€çš„åˆ†å—ï¼ˆtiling / blockingï¼‰å‚æ•°è®¾å®šã€‚ä¸‹é¢æ˜¯æ€è·¯ + æ¨æ–­è¿‡ç¨‹ + ä¸€äº›å»ºè®®å‚æ•°ã€‚

---

## âœ… Hopper æ¶æ„å…³é”®ç‰¹æ€§ï¼ˆå¯¹ matmul ä¼˜åŒ–é‡è¦æ€§ï¼‰

é¦–å…ˆï¼ŒHopper çš„å‡ ä¸ªç‰¹æ€§å†³å®šäº† kernel è®¾è®¡çš„é«˜æ•ˆæ€§å’Œå³°å€¼å¯è¾¾æ€§ã€‚

* Tensor Core åŠ é€Ÿ + æ··åˆç²¾åº¦æ”¯æŒï¼šHopper çš„ç¬¬å››ä»£ Tensor Cores æ”¯æŒ FP16ã€FP8ã€BF16ã€INT8 ç­‰ç²¾åº¦ã€‚å¯¹ FP8ï¼Œç†è®ºååæœ‰å¤§å¹…æå‡ï¼ˆç›¸æ¯”å‰ä»£åœ¨ç­‰ä»·ç²¾åº¦ä¸‹çº¦ 2Ã— throughputï¼‰ã€‚ ([NVIDIA Developer][1])
* æ›´é«˜çš„ç®—åŠ›ä¸ååï¼å¸¦å®½é…å¹³ï¼šç›¸æ¯”å‰ä»£ï¼Œæ¯ä¸ª SM çš„ tensor-MMA throughput å¤§å¹…æå‡ï¼Œä¸”æ•´ä½“ GPU æ‹¥æœ‰æ›´å¤š SM + æ›´é«˜æ—¶é’Ÿ + æ›´å¤§ shared-memory / L1 ç¼“å­˜ã€‚ ([NVIDIA Developer][1])
* æ›´å¿«/æ›´å¤§æ•°æ®ä¼ è¾“èƒ½åŠ›ï¼šHopper é…åˆ HBM3 + é«˜ memory bandwidthï¼Œä»¥åŠæ–°çš„å†…å­˜å­ç³»ç»Ÿï¼TMA (Tensor Memory Accelerator) â€”â€” å¯é«˜æ•ˆåœ°åœ¨ global memory â†” shared memory ä¹‹é—´åšå¤§å—å¼‚æ­¥æ¬è¿ã€‚ ([NVIDIA Developer][1])
* æ”¯æŒ block-based scaling / dynamic scaling when using FP8ï¼ˆä¾‹å¦‚ cuBLAS é‡Œå¯¹ 128Ã—128 2D å—çš„ç¼©æ”¾æœºåˆ¶ï¼‰ â€”â€” è¿™å½±å“äº†åˆ†å— (blocking) çš„ç²’åº¦é€‰æ‹©ã€‚ ([NVIDIA å¼€å‘è€…][2])

å› æ­¤ï¼Œè¦é€¼è¿‘ç†è®ºå³°å€¼ï¼Œå°±éœ€è¦è®¾è®¡ kernel ä½¿å¾—ï¼š

* Tensor Core çš„ç®—åŠ›è¢«é¥±æ»¡åˆ©ç”¨ï¼ˆé«˜ occupancy + æ¯ä¸ª warp/block è¦†ç›–è®¡ç®—ï¼‰
* æ•°æ®ä¼ è¾“ (global â†’ shared â†’ registers) ä¸æˆä¸ºç“¶é¢ˆ
* æ•°æ®å¸ƒå±€ / tiling ä¸ tensor-core æœ€ä¼˜å¤„ç†å•å…ƒ (tile size) å¯¹é½

---

## ğŸ§® ç†è®ºæ¨å¯¼ â€” å¦‚ä½•ä¼°è®¡åˆé€‚çš„åˆ†å— (tile/block) å¤§å°

ä»¥ä¸‹æ˜¯åŸºäºæ¶æ„ + ç¡¬ä»¶/è½¯ä»¶ç‰¹æ€§å¯¹åˆ†å—å‚æ•° (M, N, K) çš„æ¨ç®—æ€è·¯ã€‚

### 1. ä» Tensor-Core åŸå­æ“ä½œ (MMA tile) å‡ºå‘

Hopper çš„ Tensor Core åœ¨åº•å±‚å®é™…æ‰§è¡Œçš„ MMA æŒ‡ä»¤ (warp-level / tile-level) æœ‰æœ€ä¼˜çš„ tile å¤§å° (tile shape) â€” é€šå¸¸æ˜¯ 16Ã—16ã€32Ã—8ã€8Ã—32ã€32Ã—32 ç­‰ï¼ˆå…·ä½“ä¾èµ– DataType + warp mapping + implementationï¼Œè‹¥ç”¨åº“/assembler ä¼šæ›´æ˜ç¡®ï¼‰ã€‚å¾ˆå¤šé«˜æ€§èƒ½ GEMM åº“ (å¦‚ CUTLASS) éƒ½åŸºäºè¿™äº› tile å¤§å°åšè®¾è®¡ã€‚

å› æ­¤ï¼Œåœ¨ä½ çš„ matmul kernel é‡Œï¼Œåº”è¯¥é€‰æ‹©çŸ©é˜µåˆ†å—ä½¿å¾—æ¯ä¸ª block (tile) çš„ç»´åº¦æ˜¯è¿™äº›â€œnative MMA tile size Ã— warp countâ€çš„æ•´æ•°å€ã€‚ä¸¾ä¾‹ï¼š

* å¯¹äº FP16/FP8ï¼šä»¥ 16Ã—16 (per warp) ä¸ºåŸºç¡€ï¼Œblock å†… M, N éƒ½è€ƒè™‘ 128ã€256ã€512 â€¦ (å³ warp_tile Ã— å¤šä¸ª warp / thread-block)
* K (reduction dim) åŒæ ·æœ€å¥½æ˜¯  multiple of warp_tileï¼Œä¸”é€šå¸¸åš K-blockingï¼Œé¿å…è¿‡å°å¯¼è‡´ occupancy ä½ / overhead é«˜ï¼Œè¿‡å¤§å¯¼è‡´ shared memory / register è¶…é™

ä¹Ÿå°±æ˜¯è¯´ï¼Œä½ çš„ blockï¼ˆtileï¼‰å½¢çŠ¶ (BM, BN, BK) æ¨èæ»¡è¶³ï¼š

```
BM mod warp_tile_M == 0  
BN mod warp_tile_N == 0  
BK mod warp_tile_K == 0
```

ä¾‹å¦‚ (BM, BN, BK) = (128, 128, 32/64/128)

---

### 2. è€ƒè™‘ Shared-memory / Cache ä»¥åŠ memory bandwidth

* Shared memory + L1 cache per SM: Hopper å¯¹æ¯”å‰ä»£æœ‰æ›´å¤§çš„ shared/L1 å®¹é‡ã€‚æ ¹æ®å®˜æ–¹èµ„æ–™ï¼ŒH100 SM çš„ shared + L1 data cache ç»„åˆçº¦ 256 KBï¼Œæ¯” A100 æœ‰æå‡ã€‚ ([NVIDIA Developer][1])
* åˆ©ç”¨ TMA / asynchronous copy æŠŠ global memory â†’ shared memory çš„æ¬è¿å’Œ compute é‡å ï¼šè¿™æ„å‘³ç€ block size ä¸åº”å¤ªå° (å¯¼è‡´ overhead å æ¯”é«˜)ï¼Œä¹Ÿä¸å®œå¤ªå¤§ (shared memory/register é™åˆ¶ + æ•°æ®é‡ç”¨æ•ˆç‡ä¸‹é™)

å› æ­¤ï¼Œåˆç†çš„ block size è¦å…¼é¡¾ï¼š

* æ¯ block çš„ A_sub, B_sub (tile) èƒ½æ”¾å…¥ shared memory + registersï¼ˆåŒ…æ‹¬å¯¹é½ + paddingï¼‰
* è®¡ç®—çš„å·¥ä½œé‡è¶³å¤Ÿå¤§ï¼Œä»¥ amortize global â†” shared copies çš„æˆæœ¬

å…·ä½“æ¨æ–­ï¼šå‡è®¾æ¯ä¸ª block ç”¨ shared memory å®¹çº³ A_sub (BM Ã— BK) + B_sub (BK Ã— BN)ï¼ŒåŠ ä¸Šä¸€äº›ä¸´æ—¶ accumulation / lanes dataã€‚å¦‚æœ BM = BN = 128, BK = 128ï¼Œåˆ™ A_sub + B_sub å¤§å°çº¦æ˜¯ 128Ã—128 + 128Ã—128 = 32 k elements â€”â€” è‹¥ FP16 (2 å­—èŠ‚/elem)ï¼Œæ€»çº¦ 64 kBã€‚åŠ ä¸Šä¸€äº› overhead (padding, double buffering, alignment)ï¼Œå¯èƒ½ < 100 kBã€‚å¯¹ 256 kB shared+L1ï¼Œè¿™æ˜æ˜¾æ˜¯å¯è¡Œçš„ â€”â€” è€Œä¸”è¿˜æœ‰ä½™é‡å…è®¸ double-/ping-pong buffering + prefetchingã€‚

å› æ­¤ (128, 128, 128) æ˜¯ä¸€ä¸ªåˆç†çš„ starting tileï¼›ä¹Ÿå¯ä»¥å°è¯• (256, 256, 64) / (256, 128, 128) / (128, 256, 128) ç­‰ï¼Œæ ¹æ®ä½ çš„çŸ©é˜µç»´åº¦ (M, N, K) ä¸ç¡¬ä»¶èµ„æº (shared memory / registers / occupancy) è°ƒæ•´ã€‚

---

### 3. è€ƒè™‘ Occupancy (SM / warp / thread-block è°ƒåº¦)

ä¸ºäº†è®©æ•´ä¸ª GPU æ‰€æœ‰ SM éƒ½è¢«å……åˆ†åˆ©ç”¨ï¼Œéœ€è¦è®¾è®¡ thread-block (block) å¤§å° + grid å¸ƒå±€ï¼Œä½¿å¾—ï¼š

* block ä¸­åŒ…å«è‹¥å¹² warps (warp per tensor-MMA tiles) â€” warp æ•°é‡ä¸èƒ½å¤ªå°‘ (å¦åˆ™ under-utilize SM)ï¼Œä¹Ÿä¸èƒ½å¤ªå¤š (register / shared memory é™åˆ¶é™ä½ occupancy)
* grid ä¸­ block æ•° (å³ number of blocks) è¶³å¤Ÿè¦†ç›–æ‰€æœ‰ SM å¹¶ä¸”å½“æŸäº› block ç­‰å¾… memory æ—¶å…¶ä»– block è¿˜èƒ½è·‘ (hiding latency)

åœ¨å®é™… kernel / library (å¦‚ CUTLASS) ä¸­ï¼Œé€šå¸¸ä¼šé€‰æ‹©æ¯ block åŒ…å« 4â€“8 ä¸ª warps (å³ 128â€“256 threads)ï¼Œè¿™æ ·æ—¢èƒ½ saturate tensor coresï¼Œä¹Ÿä¸è¿‡åº¦å ç”¨èµ„æºã€‚å¯¹åº”ä½ çš„ BM/BN åˆ†å—ï¼Œblock-level tile (e.g. 128Ã—128) + æ¯ block warp æ•° 4â€“8 æ˜¯ä¸€ä¸ªå¸¸è§è®¾è®¡ã€‚

---

## ğŸ“Š ç¤ºä¾‹æ¨æ–­ â€” ç»™å®šä¸åŒçŸ©é˜µè§„æ¨¡æ—¶çš„æ¨èåˆ†å—ç­–ç•¥

å‡è®¾ä½ è¦åš GEMM: C = A_{MxK} Ã— B_{KxN}

| çŸ©é˜µè§„æ¨¡ /ç‰¹ç‚¹                                 | æ¨è block (tile) é…ç½® (M_block, N_block, K_block)                                   | è¯´æ˜ /åŸå›                                                                               |
| ---------------------------------------- | -------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------- |
| å¤§å‹ square çŸ©é˜µ (M,N,K éƒ½å¾ˆå¤§, æ¯”å¦‚ > 4096)      | (128, 128, 128) æˆ– (256, 256, 64) æˆ– (256,128,128)                                 | å¹³è¡¡ shared-mem ä½¿ç”¨ä¸ compute/data reuseï¼›large enough to amortize overhead              |
| éæ–¹å½¢ / éå¯¹ç§° (e.g. tall & skinny: M â‰« N, K) | M_block = 256~512, N_block = 128, K_block = 64~128                               | å‘åˆ—ä¸» (column-major) / row-major çš„ memory å¸ƒå±€ + coalescing ä¼˜åŒ–ï¼›ç¡®ä¿ B_sub fits shared mem |
| K å¾ˆå¤§ (reduction dim å¾ˆé•¿)                  | K åˆ†å—æˆ 128 æˆ– 256 çš„ chunkï¼Œç§¯ç´¯ partial sumsï¼Œç„¶åå† reduction                            | é¿å…ä¸€æ¬¡æ€§è¯»å–è¿‡å¤§ chunk å¯¼è‡´ register / shared mem ä¸è¶³ï¼›åˆ†å—å¯ overlap memory & compute            |
| Mixed-precision (FP16 / FP8)             | ä½¿ç”¨ native warp-tile (16Ã—16 per warp) Ã— block warp count, e.g. block of (128,128) | å¯¹é½ tensor-core æœ€ä½³ tileï¼Œå‘æŒ¥æœ€å¤§ throughput                                              |

---

## ğŸ”§ ç»“åˆä½ å½“å‰èƒŒæ™¯ (é‡åŒ– + CPU æ··åˆ / GPU offload) çš„å»ºè®®

ä½ ä¹‹å‰æåˆ°ä½ åœ¨åšæ··åˆé‡åŒ– (Q4_K, IQ4_NL ç­‰) + åœ¨ CPU ä¸Šä¼˜åŒ– matrix mulï¼fused up-gateã€‚è‹¥åæ¥è€ƒè™‘å°† matmul offload åˆ° GPUï¼ˆHopperï¼‰ + åˆ©ç”¨å…¶ tensor-core / mixed-precision ä¼˜åŠ¿ï¼Œé‚£ä¹ˆï¼š

* æ¨èå…ˆå®ç° FP16 æˆ– FP8 + FP32 accumulate çš„ç‰ˆæœ¬ (å› ä¸º Hopper å¯¹è¿™ç§æ··åˆç²¾åº¦æ”¯æŒéå¸¸å¥½) ([NVIDIA Developer][1])
* åœ¨ kernel ä¸­æ˜ç¡® tile/block sizeï¼Œå¦‚ (128,128,128) æˆ– (256,128,128)ï¼Œç„¶åé€šè¿‡ shared memory + TMA / async copy å®ç°æ•°æ®é¢„å– (prefetch) + double-bufferingï¼Œä»¥ overlap memory & compute
* Use åº“ (å¦‚ CUTLASS) + / æˆ–è€…ç±»ä¼¼äº DeepGEMM çš„é«˜æ€§èƒ½å®ç°åšå‚è€ƒ â€” æ ¹æ®æŠ¥é“ï¼ŒDeepGEMM åœ¨ Hopper ä¸Šå°±èƒ½å¾—åˆ°éå¸¸é«˜ throughputã€‚ ([AIå·¥å…·ç®±å®˜ç½‘][3])
* å¯¹äºé‡åŒ– (Q4/K ç­‰) â€”â€” å¦‚æœä½ æƒ³è‡ªå®šä¹‰ kernelï¼Œéœ€è¦è€ƒè™‘å°†é‡åŒ– + dequantization (to FP16/FP8) + matmul + re-quantization æµæ°´çº¿èåˆ (fuse)ï¼Œå‡å°‘ memory traffic + æ•°æ®æ ¼å¼è½¬æ¢ overheadï¼Œè¿™æ ·æ‰èƒ½æ›´æ¥è¿‘ç†è®ºå³°å€¼ã€‚

---

## âš ï¸ ä¸ºä»€ä¹ˆå®è·µä¸­å¯èƒ½è¾¾ä¸åˆ°ç†è®ºå³°å€¼ (Peak)

å³ä½¿ä½ è®¾è®¡å‡ºçœ‹ä¼¼åˆç†çš„ tile / blockingï¼Œä¹Ÿå¯èƒ½å› ä¸ºä»¥ä¸‹åŸå› æ— æ³•çœŸæ­£è¾¾åˆ°ç†è®ºå³°å€¼ï¼š

* Memory-bandwidth or memory-latency é™åˆ¶ï¼šå¦‚æœ global â†” shared â†” register çš„æ•°æ®æ¬è¿ä¸èƒ½è¢«å¾ˆå¥½éšè— (overlap)ï¼Œcompute units ä¼šç©ºé—²ã€‚
* Non-ideal matrix shapes / non multiples of tile size å¯¼è‡´ padding / wasted compute or memory.
* Control flow divergenceã€bank conflictsï¼ˆshared memoryï¼‰ã€imperfect coalescingã€suboptimal occupancy éƒ½ä¼šå‰Šå¼±æ€§èƒ½ã€‚
* å¦‚æœæ··åˆç²¾åº¦ + é‡åŒ– + dequantization / re-quantization çš„ overhead æ¯”è¾ƒå¤§ï¼Œä¹Ÿä¼šæ‹‰ä½ throughputã€‚

è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆæˆç†Ÿåº“ (å¦‚ CUTLASS / cuBLAS) è¦å¯¹å¤šç§çŸ©é˜µå½¢çŠ¶ + precision + tile size åš heuristics/autotuneï¼ˆå¦‚ nvMatmulHeuristicsï¼‰ â€”â€” ä»¥é€‰æ‹©æœ€ä¼˜é…ç½®ã€‚ ([NVIDIA Developer][4])

---

## ğŸ§  æ€»ç»“ & æˆ‘çš„å»ºè®®

* åœ¨é’ˆå¯¹ Hopper ä¼˜åŒ– matmul kernel æ—¶ï¼Œåº”**ä» Tensor-core native tile size å‡ºå‘ + è€ƒè™‘ shared-memory / cache + memory-bandwidth / prefetch / asynchronous copy + occupancy / warp/block mapping**ï¼Œé€šè¿‡è¿™äº›ç»´åº¦ç»¼åˆè®¾è®¡åˆ†å— (tiling) å‚æ•°ã€‚
* å®é™…æ¨èçš„ tile (block) size åƒ (128,128,128), (256,256,64), (256,128,128) ç­‰ï¼Œé€šå¸¸æ˜¯æ¯”è¾ƒä¿å®ˆã€å®ç”¨çš„èµ·ç‚¹ã€‚
* è‹¥ä½ çš„è¾“å…¥çŸ©é˜µç»´åº¦ / precision / sparsity /é‡åŒ–ç‰¹æ®Š (e.g. Q4_K / IQ4_NL)ï¼Œå»ºè®®è€ƒè™‘æŠŠé‡åŒ–/dequant + matmul + re-quant æµæ°´çº¿èåˆ (fuse)ï¼Œå‡å°‘å†—ä½™æ¬è¿ / è½¬æ¢ã€‚
* æœ€ç»ˆå»ºè®®ä½¿ç”¨ autotune / heuristicsï¼ˆå¦‚ nvMatmulHeuristics / CUTLASS autotunerï¼‰åšå¤šç»„ benchmarkï¼Œæµ‹å‡ºå¯¹ä½ å®é™… workloads æœ€å¥½çš„åˆ†å— + kernel å‚æ•°ã€‚

---

å¦‚æœä½ æ–¹ä¾¿çš„è¯ â€” æˆ‘å¯ä»¥ **å¸®ä½ é’ˆå¯¹ Hopper** å†™ä¸€ä¸ªè„šæœ¬ (pseudo-code / CUDA kernel æ¨¡æ¿) â€”â€” ç»™å®š (M, N, K, precision, quantization) è‡ªåŠ¨æšä¸¾ã€æµ‹è¯•å‡ ç§ (BM,BN,BK) tileï¼Œå¹¶è¾“å‡ºâ€œç†è®º FLOPS åˆ©ç”¨ç‡ + bandwidth åˆ©ç”¨ç‡â€æŠ¥å‘Šã€‚è¿™æ ·ä½ å¯ä»¥æ›´å¿«åœ° empirically æ‰¾åˆ°æœ€ä¼˜é…ç½®ã€‚ä½ è¦æˆ‘å¸®ä½ åšå—ï¼Ÿ

[1]: https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/?utm_source=chatgpt.com "NVIDIA Hopper Architecture In-Depth | NVIDIA Technical Blog"
[2]: https://developer.nvidia.cn/blog/boosting-matrix-multiplication-speed-and-flexibility-with-nvidia-cublas-12-9/?utm_source=chatgpt.com "å€ŸåŠ© NVIDIA cuBLAS 12.9 æé«˜çŸ©é˜µä¹˜æ³•é€Ÿåº¦å’Œçµæ´»æ€§ - NVIDIA æŠ€æœ¯åšå®¢"
[3]: https://ai-kit.cn/10636.html?utm_source=chatgpt.com "DeepGEMMï¼šä¸“ä¸ºHopperæ¶æ„GPUä¼˜åŒ–çš„é«˜æ€§èƒ½çŸ©é˜µä¹˜æ³•åº“ | AIå·¥å…·ç®±å®˜ç½‘"
[4]: https://developer.nvidia.com/zh-cn/blog/improving-gemm-kernel-auto-tuning-efficiency-on-nvidia-gpus-with-heuristics-and-cutlass-4-2/?utm_source=chatgpt.com "å€ŸåŠ©å¯å‘å¼ç®—æ³•å’Œ CUTLASS 4.2 æé«˜ NVIDIA GPU ä¸Šçš„ GEMM å†…æ ¸è‡ªåŠ¨è°ƒæ•´æ•ˆç‡ - NVIDIA æŠ€æœ¯åšå®¢"


---

### ğŸ§‘â€ğŸ’» User

æˆ‘è®°å¾—éœ€è¦æ ¹æ®å¯„å­˜å™¨æ–‡ä»¶å¤§å°ï¼Œshared memory size/å¸¦å®½/å»¶è¿Ÿï¼Œå„çº§cacheåŠglobal memoryçš„å¸¦å®½/å»¶è¿Ÿï¼Œæœ‰ä¸€å¥—æ–¹æ³•è®ºï¼Œç¡®è®¤çŸ©é˜µä¹˜åˆ†å—çš„tile m/n/kåŠè½¯æµæ°´çš„çº§æ•°ã€‚èƒ½ä¸èƒ½å±•å¼€è¯´è¯´ï¼Ÿ

---

### ğŸ¤– Assistant

å¥½â€”â€”ä¸‹é¢æŠŠé‚£å¥—**ä»å¯„å­˜å™¨ / shared / å„çº§ cache / global memoryï¼ˆå¸¦å®½ & å»¶è¿Ÿï¼‰å‡ºå‘æ¨å¯¼ tileï¼ˆBM/BN/BKï¼‰å’Œè½¯ä»¶æµæ°´çº§æ•°ï¼ˆpipeline stagesï¼‰**çš„æ–¹æ³•è®ºç³»ç»ŸåŒ–åœ°å±•å¼€ï¼Œç»™å‡ºå…¬å¼ã€æ¨å¯¼æ­¥éª¤ã€å®è·µå¯å‘ä»¥åŠè°ƒå‚ checklistã€‚å°½é‡ç”¨å¯ä»£å…¥æ•°å€¼çš„**ç¬¦å·åŒ–å…¬å¼**ï¼Œä½ å¯ä»¥æŠŠå®é™…ç¡¬ä»¶å‚æ•°ä»£è¿›å»ç®—å‡ºå…·ä½“ç»“æœã€‚

æˆ‘ä¼šæŒ‰è¿™æ ·çš„ç»“æ„æ¥è®²ï¼š

1. æ¦‚å¿µæ¸…å•ï¼ˆè¦ç”¨åˆ°çš„ç¡¬ä»¶é‡ï¼‰
2. ç›®æ ‡ä¸çº¦æŸï¼ˆè¦åŒæ—¶æ»¡è¶³çš„èµ„æº / æ€§èƒ½ç›®æ ‡ï¼‰
3. é€æ­¥æ¨å¯¼ï¼šå®¹é‡çº¦æŸ â†’ å¯„å­˜å™¨çº¦æŸ â†’ å¸¦å®½ / å¼ºåº¦ (operational intensity) â†’ pipeline depthï¼ˆçº§æ•°ï¼‰
4. å…¬å¼ä¸ç¤ºä¾‹ï¼ˆæŠŠæ­¥éª¤åˆæˆæˆå¯ç”¨å…¬å¼ï¼‰
5. å®è·µå»ºè®®ã€å¸¸è§é™·é˜±ä¸è°ƒå‚ checklist

# 1) æ¦‚å¿µæ¸…å•ï¼ˆç¡¬ä»¶é‡ & ç¬¦å·ï¼‰

å…ˆæŠŠå˜é‡åˆ—å‡ºæ¥ï¼ˆç¬¦å·åŒ–ï¼Œä¾¿äºä»£å…¥ï¼‰ï¼š

* çŸ©é˜µä¸ tileï¼š

  * M,N,Kï¼šåŸå§‹ GEMM ç»´åº¦ï¼ˆC[MÃ—N] = A[MÃ—K] Ã— B[KÃ—N]ï¼‰
  * BM, BN, BKï¼šblock/tile å¤§å°ï¼ˆæˆ‘ä»¬è¦æ±‚è§£ï¼‰
* æ•°æ®ç±»å‹ï¼š

  * b = bytes/elementï¼ˆä¾‹å¦‚ FP16 b=2, FP32 b=4, FP8 b=1ï¼‰
* è®¡ç®—èµ„æºï¼ˆper-SM / per-GPUï¼ŒæŒ‰å®é™…ç¡¬ä»¶ä»£å…¥ï¼‰ï¼š

  * FLOPS_peak = GPU çš„ peak FLOPSï¼ˆæ³¨æ„æ··åˆç²¾åº¦è¦çœ‹ tensor-core throughput & accumulate typeï¼‰
  * BW_glob = global memory sustained bandwidthï¼ˆbytes/sï¼‰
  * L_glob = global memory latencyï¼ˆcycles æˆ– nsï¼‰â€”â€” ç”¨äºä¼°ç®—éšè—å»¶è¿Ÿéœ€è¦å¤šå°‘å¹¶å‘
* SM èµ„æºï¼ˆper-SMï¼‰ï¼š

  * S_sh = shared memory available per SMï¼ˆbytesï¼‰
  * R_reg = register file per SMï¼ˆnumber of 32-bit registers OR bytesï¼‰ï¼Œæˆ–ç”¨ R_reg_totalï¼ˆregsï¼‰ä¸ reg_sizeï¼ˆ4 bytesï¼‰åŒºåˆ†
  * W_max = max warps per SMï¼ˆhardware limitï¼‰
  * blocks_max = max blocks per SM
* å®ç°/çº¿ç¨‹æ˜ å°„ï¼š

  * TperBlock = threads per blockï¼ˆé€šå¸¸ 128/256/512ï¼‰
  * warps_per_block = TperBlock / 32
* æ—¶åºç›¸å…³ï¼š

  * T_compute_tile = cycles spent computing one tile (BMÃ—BNÃ—BK work mapped to tensor cores) â€” å¯ä»¥ç”¨ç†è®º FLOPS è½¬æ¢
  * T_load_tile = cycles to load required global data for that tile (taking bandwidth & coalescing into account)
  * L_glob_cycles = latency from issuing global load to it being usable (in cycles)
* å…¶å®ƒï¼š

  * Overhead_shared_bank_conflicts ç­‰æš‚ç”¨ä¸º constant å¦åˆ™ç”¨ç»éªŒå‰Šå‡æœ‰æ•ˆ BW / capacityã€‚

# 2) ç›®æ ‡ä¸çº¦æŸï¼ˆç®€è¿°ï¼‰

ä¼˜åŒ–ç›®æ ‡é€šå¸¸æ˜¯ï¼š**è®© tensor cores / ALUs åœ¨æ•´ä½“ä¸Šæ¥è¿‘å³°å€¼ FLOPS**ã€‚ä¸ºæ­¤å¿…é¡»æ»¡è¶³ï¼š

* æ¯ä¸ª SM ä¸Šæœ‰è¶³å¤Ÿå¹¶å‘ï¼ˆactive warps/blocksï¼‰æ¥éšè— memory latencyï¼Œå¹¶ä¿æŒ tensor cores æœ‰æŒç»­ workã€‚
* æ¯ä¸ª block çš„ A_sub (BMÃ—BK) ä¸ B_sub (BKÃ—BN) ä»¥åŠå¿…è¦çš„ C accumulators èƒ½**fit**åˆ° shared memory + registers ä¸­ï¼ˆæˆ–åˆç†åˆ†é…åˆ° shared/regï¼‰ã€‚
* æ•°æ®æ¬è¿ï¼ˆglobal â†’ shared â†’ regsï¼‰å’Œ compute èƒ½å¤Ÿ**é‡å **ï¼ˆé€šè¿‡ double buffering / software pipeline / async copyï¼‰ã€‚
* æ“ä½œå¼ºåº¦ï¼ˆoperational intensity I = FLOPs / bytes_from_globalï¼‰å°½é‡å¤§ï¼Œä»¥å…è¢« global bandwidth é™åˆ¶ã€‚

å› æ­¤æˆ‘ä»¬åŒæ—¶è¦è€ƒè™‘**å®¹é‡çº¦æŸ**ï¼ˆshared/regï¼‰ä¸**æ€§èƒ½çº¦æŸ**ï¼ˆå¸¦å®½ vs è®¡ç®—ï¼‰ä¸**å¹¶å‘çº¦æŸ**ï¼ˆoccupancyï¼‰ã€‚

# 3) é€æ­¥æ¨å¯¼ï¼ˆæ ¸å¿ƒæ­¥éª¤ï¼‰

### A. å®¹é‡çº¦æŸï¼ˆshared memory + registerï¼‰

ä¸€ä¸ª block å¤„ç†çš„ tile è¦å ç”¨çš„ shared/reg å¤§å°å¿…é¡»å°äºç¡¬ä»¶å…è®¸å€¼ï¼Œå¦åˆ™ä¸èƒ½è·‘ã€‚é¦–å…ˆä¼°ç®—ä¸€ä¸ª block æ‰€éœ€çš„ shared memoryï¼š

* A_tile bytes = BM Ã— BK Ã— b
* B_tile bytes = BK Ã— BN Ã— b
* C_tile bytes (å¦‚æœåœ¨ shared ä¸­ç¼“å­˜ partial C) = BM Ã— BN Ã— b  ï¼ˆæ³¨æ„å¾ˆå¤šå®ç°è®©æ¯çº¿ç¨‹åœ¨å¯„å­˜å™¨é‡Œç´¯åŠ ï¼Œæœ€ç»ˆå†™å› globalï¼Œä¸ä¸€å®šæŠŠ C æ”¾ sharedï¼‰
* Shared_total_per_block â‰ˆ A_tile + B_tile + (optional) C_tile + overhead (alignment, double buffering)

**çº¦æŸ 1ï¼š**

```
A_tile + B_tile + optional_C <= S_sh_alloc_per_block
```

S_sh_alloc_per_block å–å†³äºä½ èƒ½åœ¨ä¸€ä¸ª SM ä¸ŠåŒæ—¶æ¿€æ´»å¤šå°‘ blockï¼ˆblocks_per_smï¼‰ï¼Œä¹Ÿå°±æ˜¯ï¼š

```
S_sh_alloc_per_block = floor(S_sh_total / blocks_per_sm_desired)
```

å¸¸è§åšæ³•ï¼šå…ˆé€‰æ‹© 1 æˆ– 2 blocks_per_smï¼ˆä»¥æé«˜ concurrencyï¼‰ï¼Œè®¡ç®—å…è®¸çš„ S_sh_alloc_per_blockã€‚

**å¯„å­˜å™¨çº¦æŸ**ï¼ˆæ›´å®¹æ˜“æˆä¸ºç“¶é¢ˆï¼‰ï¼š
æ¯ä¸ª thread ä½¿ç”¨çš„å¯„å­˜å™¨æ•°ç›® reg_per_thread å¿…é¡»æ»¡è¶³ï¼š

```
reg_per_thread Ã— TperBlock <= R_reg_total_per_SM / blocks_per_sm_desired
```

ä¼°ç®— reg_per_thread æ¥è‡ªï¼š

* accumulators per thread (number of C elements that thread keeps in reg)
* fragments of A/B loaded into registers for compute (dependent on warp-level fragment sizes)
* temporaries, indices, pointers

å¸¸è§ç­–ç•¥ï¼šå†³å®šæ¯çº¿ç¨‹ç´¯åŠ å¤šå°‘ C å…ƒç´ ï¼ˆcall it c_per_threadï¼‰ï¼Œé‚£å¯„å­˜å™¨éœ€æ±‚åŸºæœ¬ä¸ c_per_thread æˆæ­£æ¯”ã€‚é€šå¸¸å¸Œæœ›æ¯çº¿ç¨‹è‡³å°‘ç»´æŒæ•°ä¸ª C accumulatorsï¼Œä»¥ä¾¿ç”¨å¯„å­˜å™¨æ›¿ä»£ shared å†™è¯»ï¼Œå‡å°‘ shared pressureã€‚

---

### B. Operational Intensityï¼ˆç®—å­å¼ºåº¦ï¼‰â†’ åˆ¤æ–­æ˜¯å¦å¸¦å®½å—é™

ç”¨ roofline/ç®—å­å¼ºåº¦åˆ¤æ–­æ˜¯å¦èƒ½åˆ©ç”¨ç®—åŠ›ï¼š

* FLOPs per tile (F_tile) = 2 Ã— BM Ã— BN Ã— BK ï¼ˆGEMM çš„ä¹˜åŠ è®¡æ•°ï¼‰
* Bytes read from global per tile (D_tile_global) â€” å…³é”®ç‚¹ï¼š**å¦‚æœèƒ½å¤ç”¨ A_tile å’Œ B_tile å¤šæ¬¡ï¼ˆåœ¨ shared ä¸­ï¼‰**ï¼Œé‚£ä¹ˆæ¯ block ä» global è¯»çš„æ¬¡æ•°åªä¸ A_tile/B_tile çš„å¤§å°æœ‰å…³ï¼Œè€Œä¸æ˜¯æ¯æ¬¡ compute éƒ½è¯»ã€‚

æœ€ä¿å®ˆçš„å–æ³•ï¼ˆæ¯ tile åªä» global è¯»ä¸€æ¬¡ A_tile å’Œ B_tileï¼‰ï¼š

```
D_tile_global â‰ˆ (BMÃ—BK + BKÃ—BN + (if read C then BMÃ—BN) ) Ã— b
```

ï¼ˆé€šå¸¸ C åªå†™å›ä¸€æ¬¡ï¼Œè¯»å†™åˆå¹¶å¯¹å¸¦å®½å½±å“å°ï¼‰

ç®—å­å¼ºåº¦ Iï¼š

```
I = F_tile / D_tile_global   (FLOPs / byte)
```

åˆ¤æ–­ï¼š

* å¦‚æœ `I < FLOPS_peak / BW_glob` â†’ memory-boundï¼ˆglobal BW æˆç“¶é¢ˆï¼‰
* å¦‚æœ `I > FLOPS_peak / BW_glob` â†’ compute-boundï¼ˆæˆ–å—æ›´è¿‘çš„å±‚å¦‚ L2/L1 bandwidth é™åˆ¶ï¼‰

å¯é‡å†™ä¸ºé’ˆå¯¹ element bï¼š

```
I = 2*BM*BN*BK / ((BM*BK + BK*BN) * b) = 2*BM*BN / ((BM + BN) * b) * (cancel BK)
```

æ³¨æ„ BK åœ¨åˆ†å­ä¸åˆ†æ¯éƒ½å‡ºç°ï¼Œæœ€å BK åè€Œä¼šéƒ¨åˆ†æŠµæ¶ˆï¼›ä½† BK ä¼šå½±å“ A/B tile å¤§å°ä¸ shared/reg å ç”¨ï¼Œä»è€Œå½±å“èƒ½å¦æˆç«‹ã€‚

æ‰€ä»¥è¦**å¢å¤§ I**ï¼Œå¸¸ç”¨æ–¹æ³•ï¼š

* å¢å¤§ BM,BNï¼ˆæ‰©å¤§è¾“å‡º tileï¼‰ï¼Œå› ä¸º FLOPs scales ~ BMÃ—BN but bytes ~ BMÃ—BK + BKÃ—BNï¼Œæ‰©å¤§ BM/BN æ›´æœ‰åˆ©ï¼ˆç›´åˆ° shared/regé™åˆ¶ï¼‰
* é€‰æ‹©åˆé€‚ BKï¼šBK å¤ªå°ä¼šé™ä½ reuseï¼ˆä½†å‡å°‘ sharedï¼‰ï¼Œå¤ªå¤§ä¼šä½¿ A/B æ— æ³•æ”¾å…¥ shared

---

### C. å¹¶å‘ / Occupancyï¼ˆéšè— latencyï¼‰

è¦æŠŠ global latency éšè—ä½ï¼Œéœ€è¦è¶³å¤Ÿçš„å¹¶å‘å·¥ä½œï¼ˆactive warps / blocksï¼‰ã€‚è¡¡é‡ï¼š

* active_warps_per_SM = (warps_per_block Ã— blocks_per_sm_active)
* ç¡¬ä»¶èƒ½åŒæ—¶æ´»è·ƒçš„ warps æœ€å¤§å€¼æ˜¯ W_max

Latency hiding conditionï¼ˆç²—ç•¥ï¼‰ï¼š

```
active_warps_per_SM Ã— T_compute_tile_per_warp >= L_glob_cycles
```

æ›´å¸¸è§çš„åšæ³•ï¼šç¡®ä¿æœ‰è¶³å¤Ÿçš„ blocks_per_smï¼ˆé€šå¸¸ â‰¥ 2â€“4ï¼‰æ¥ overlap memory opsã€‚è‹¥æ¯ä¸ª tile è®¡ç®—æ—¶é—´å¤ªçŸ­ï¼ˆT_compute_tile smallï¼‰ï¼Œé‚£å°±éœ€è¦æ›´å¤šçš„ pipeline stages/blocksã€‚

**å¦‚ä½•ä¼°ç®— T_compute_tile**ï¼š

```
T_compute_tile â‰ˆ F_tile / FLOPS_per_SM_effective (in FLOPs/cycle)
```

FLOPS_per_SM_effective å¯ä»¥ç”¨ peak but è¦æ³¨æ„ warp-level mapping and instruction throughput (tensor core efficiency).

ç„¶åè®¡ç®—éœ€è¦çš„å¹¶å‘åº¦ S_neededï¼š

```
S_needed ~ ceil(L_glob_cycles / T_compute_tile) + 1
```

è¿™ä¸ pipeline stages çš„æƒ³æ³•ä¸€è‡´ï¼šè¦ overlapping loads for future tiles while computing current tile.

---

### D. è½¯ä»¶æµæ°´ï¼ˆsoftware pipelining / stagesï¼‰è®¾è®¡

è½¯ä»¶æµæ°´çš„ç›®çš„æ˜¯é‡å å¤šä¸ªæ­¥éª¤ï¼šglobal load â†’ L2/L1 â†’ shared copy â†’ compute â†’ write backã€‚å…¸å‹çš„è®¾è®¡æ˜¯ double-buffer æˆ– triple-bufferï¼š

* **2-stage (double-buffer)**ï¼šä¸€ç»„ A/B åœ¨è®¡ç®—ï¼Œå¦ä¸€ç»„åœ¨ prefetchã€‚ä» global åˆ° shared ç”¨ async copyï¼Œcompute overlap copyã€‚é€‚åˆå½“ T_compute_tile â‰¥ L_copy_timeï¼ˆæˆ–æ¥è¿‘ï¼‰æ—¶ã€‚
* **S-stage pipeline**ï¼šæŠŠ long latency chain æ‹†æˆ S æ®µï¼ˆä¾‹å¦‚ fetch from global, wait L2, copy to shared, compute, writebackï¼‰ï¼Œéœ€è¦ S buffersã€‚`S` çš„é€‰æ‹©ç”±ä¸‹é¢çš„å…¬å¼é©±åŠ¨ï¼š

è‹¥ L_total_cycle æ˜¯ä»å‘èµ· global load åˆ° load å®Œæˆå¯ç”¨çš„æ€»å»¶è¿Ÿï¼ˆä»¥ cyclesï¼‰ï¼Œä¸” T_compute_tile æ˜¯æ¯ä¸ª tile çš„ compute timeï¼ˆcyclesï¼‰ï¼Œåˆ™æœ€ä½éœ€è¦çš„å¹¶è¡Œ tiles æ•°ï¼ˆå³ pipeline depthï¼‰ï¼š

```
S_min = ceil(L_total_cycle / T_compute_tile) + 1
```

è§£é‡Šï¼šå¦‚æœä¸€ä¸ª tile çš„ compute æ—¶é—´å¯ä»¥â€œè¦†ç›–â€L_total_cycle çš„ä¸€éƒ¨åˆ†ï¼Œé‚£å°±éœ€è¦ S_min tiles åŒæ—¶å¤„äº pipeline çš„ä¸åŒé˜¶æ®µæ¥å®Œå…¨éšè—é‚£æ®µå»¶è¿Ÿã€‚å¸¸è§å®ç°é‡Œ L_total_cycle åŒ…å« global memory latency + TMA overhead + any memcopy latencyã€‚

å®è·µä¸Šï¼Œå¾ˆå¤šé«˜æ€§èƒ½å®ç°ä¼šé€‰æ‹© S = 2 æˆ– 3ï¼Œå¦‚æœ global latency å¾ˆé«˜æˆ– T_compute_tile å¾ˆå°ï¼ˆtile å°ï¼‰ï¼Œå°±éœ€è¦æ›´é«˜ Sï¼ˆ4â€“8ï¼‰ã€‚ä½† S çš„å¢åŠ ä¼šå¢åŠ  shared / register çš„ buffering éœ€æ±‚ï¼ˆæ¯ä¸ª stage éƒ½éœ€è¦ bufferï¼‰ï¼Œå› æ­¤ S ä¸èƒ½æ— é™å¤§â€”â€”å›åˆ°å®¹é‡çº¦æŸã€‚

---

# 4) æŠŠå®ƒä»¬åˆèµ·æ¥ï¼šå¯æ‰§è¡Œçš„æ¨ç®—æµç¨‹ï¼ˆstep-by-stepï¼‰

ç»™å‡ºä¸€ä¸ªå·¥ç¨‹å¸ˆå¯ä»¥ç›´æ¥æ‰§è¡Œçš„æµç¨‹ï¼ˆæŠŠç¡¬ä»¶æ•°å€¼ä»£å…¥ï¼‰ï¼š

**æ­¥éª¤ 0ï¼šæ”¶é›†ç¡¬ä»¶å‚æ•°**ï¼ˆä»£å…¥å®é™…å€¼ï¼‰

* S_sh_totalï¼ŒR_reg_totalï¼ˆregsï¼‰ï¼ŒW_maxï¼Œblocks_maxï¼ŒBW_globï¼ˆbytes/sï¼‰ï¼ŒL_glob_cyclesï¼ˆcyclesï¼‰ï¼ŒFLOPS_peak (FLOPs/s) æˆ– per-SM FLOPS
* é€‰æ‹©æ•°æ®ç±»å‹ b

**æ­¥éª¤ 1ï¼šé€‰ä¸€ä¸ª candidate BMÃ—BN å¸¦æ¥çš„åˆå§‹å¼ºåº¦**

* é€‰æ‹©åˆå§‹ BKï¼ˆå¸¸è§ 32/64/128ï¼‰ã€‚é€‰ BK å— shared/reg é™åˆ¶ã€‚
* è®¡ç®— F_tile = 2*BM*BN*BK
* è®¡ç®— D_tile = (BM*BK + BK*BN) * b
* ç®—å­å¼ºåº¦ I = F_tile / D_tile

**æ­¥éª¤ 2ï¼šæ£€æŸ¥å¸¦å®½çº¦æŸï¼ˆæ˜¯å¦ memory-boundï¼‰**

* ä¸´ç•Œå¼ºåº¦ I_crit = FLOPS_peak / BW_glob
* å¦‚æœ I < I_critï¼šè¯´æ˜ global BW å°†é™åˆ¶æ€§èƒ½ï¼Œéœ€å¢å¤§ BM/BN æˆ–é™ç²¾åº¦ï¼ˆå‡å°‘ bï¼‰æˆ–å¢åŠ  BK çš„ reuseï¼ˆæ³¨æ„ BK å½±å“ sharedï¼‰
* å¦‚æœ I > I_critï¼šè¯´æ˜å¯èƒ½ compute-bound æˆ–æ›´è¿‘å±‚ï¼ˆL2ï¼‰å¸¦å®½æˆä¸ºé™åˆ¶ â€” ç»§ç»­ä¸‹æ­¥

**æ­¥éª¤ 3ï¼šå…±äº«å†…å­˜ / å¯„å­˜å™¨é¢„ç®—**

* è®¡ç®— A_tile + B_tile + optional C_tile <= S_sh_alloc_per_block

  * S_sh_alloc_per_block = floor(S_sh_total / blocks_per_sm_desired)
* ä¼°ç®— reg_per_threadï¼šé¦–å…ˆå†³å®šæ¯çº¿ç¨‹ç´¯åŠ å¤šå°‘ C å…ƒç´ ï¼ˆc_per_threadï¼‰ï¼Œä»¥åŠä¸ºåŠ è½½ A/B fragments éœ€è¦å¤šå°‘å¯„å­˜å™¨ã€‚ç²—ç•¥ä¼°ï¼š

  ```
  reg_per_thread â‰ˆ c_per_thread + a_frag_regs + b_frag_regs + overhead
  ```

  éªŒè¯ reg_per_thread Ã— TperBlock â‰¤ R_reg_total_per_SM / blocks_per_sm_desired

**æ­¥éª¤ 4ï¼šå¹¶å‘ & latency hiding â†’ pipeline depth**

* ä¼°ç®— T_compute_tile_cycles = F_tile / FLOPS_per_SM_effective
* S_min = ceil(L_total_cycle / T_compute_tile_cycles) + 1
* é€‰æ‹© blocks_per_sm (â‰¥ S_min / warps_per_block ç­‰)ï¼Œä¿è¯ occupancyï¼š

  * warps_active = warps_per_block Ã— blocks_per_sm
  * warps_active â‰¤ W_max
* ç¡®è®¤ä½¿ç”¨ S_min stages çš„ bufferingï¼ˆshared/regï¼‰ä»æ»¡è¶³å®¹é‡çº¦æŸï¼ˆå› ä¸ºæ¯ stage éœ€è¦ bufferï¼‰

**æ­¥éª¤ 5ï¼šè¿­ä»£è°ƒèŠ‚**

* å¦‚æœå®¹é‡/å¯„å­˜å™¨è¶…é™ï¼Œå‡å° BM/BN æˆ– BK æˆ–å‡å°‘ c_per_threadï¼ˆå¯„å­˜å™¨æ¢ sharedï¼‰ï¼Œæˆ–è€…é™ä½ blocks_per_sm
* å¦‚æœ I < I_critï¼ˆmemory-boundï¼‰ï¼Œä¼˜å…ˆå¢åŠ  BM æˆ– BNï¼ˆæ‰©å¤§è¾“å‡º tileï¼‰ï¼Œæˆ–é‡‡ç”¨æ›´ aggressive reuseï¼ˆblock åˆ’åˆ†ç­–ç•¥ï¼‰ï¼Œæˆ–æ”¹ç”¨ lower-precision
* åœ¨æ¯è½®è°ƒæ•´åï¼Œé‡æ–°è®¡ç®—ä¸Šé¢çš„æ‰€æœ‰çº¦æŸï¼Œç›´åˆ°æ‰¾å‡ºæ»¡è¶³æ‰€æœ‰çº¦æŸçš„æœ€å¤§ BM/BN/BK ä¸ pipeline S

---

# 5) å…¬å¼æ±‡æ€»ï¼ˆä¾¿äºç›´æ¥ä»£å…¥è®¡ç®—ï¼‰

* F_tile = 2 Ã— BM Ã— BN Ã— BK  ï¼ˆFLOPsï¼‰

* D_tile_global â‰ˆ (BMÃ—BK + BKÃ—BN) Ã— b  ï¼ˆbytes from global per tileï¼‰

* I = F_tile / D_tile_global = 2Ã—BMÃ—BNÃ—BK / ((BMÃ—BK + BKÃ—BN)Ã—b)

* æ¡ä»¶æ˜¯å¦ memory-bound:

  ```
  memory_bound if I < FLOPS_peak / BW_glob
  ```

* shared constraint:

  ```
  A_bytes + B_bytes + C_bytes <= floor(S_sh_total / blocks_per_sm)
  ```

* register constraint:

  ```
  reg_per_thread Ã— TperBlock <= floor(R_reg_total / blocks_per_sm)
  ```

* latency hiding / pipeline depth:

  ```
  S_min = ceil(L_total_cycle / T_compute_tile) + 1
  T_compute_tile (cycles) â‰ˆ F_tile / FLOPS_per_SM_effective
  ```

ï¼ˆæ³¨ï¼šFLOPS_per_SM_effective = FLOPS_peak_per_SM Ã— expected_tensor_core_utilization_factorï¼‰

---

# 6) æ•°å€¼ç¤ºä¾‹ï¼ˆä¸¾ä¸€ä¸ªæŠ½è±¡çš„ã€Œä»£å…¥ç¤ºä¾‹ã€å¸®åŠ©ç†è§£ â€” ä½¿ç”¨å‡è®¾æ•°å€¼ï¼Œéç‰¹å®šå¡ï¼‰

å‡è®¾ï¼ˆç¤ºä¾‹ï¼Œä»…æ¼”ç¤ºæµç¨‹ï¼‰ï¼š

* b = 2 bytes (FP16)
* S_sh_total = 256 KB = 262144 bytes
* R_reg_total = 65536 registers (32-bit each)
* BW_glob = 1 TB/s = 1e12 bytes/s ï¼ˆæ³¨æ„ï¼šåªæ˜¯æ¼”ç¤ºï¼‰
* FLOPS_peak = 50 TFLOPS = 5e13 FLOPs/sï¼ˆæ¼”ç¤ºï¼‰
* Tclock = 1 GHz â†’ 1 cycle = 1 ns
* L_total_cycle (global) = 500 cyclesï¼ˆç¤ºæ„ï¼‰
* choose BM=128, BN=128, BK=32

  * F_tile = 2 * 128 * 128 * 32 = 1,048,576 FLOPs (~1.05e6)
  * D_tile = (128*32 + 32*128) * 2 = (4096 + 4096) * 2 = 16384 bytes = 16 KB
  * I = 1.048e6 / 16384 â‰ˆ 64 FLOPs/byte
* I_crit = FLOPS_peak / BW_glob = 5e13 / 1e12 = 50 FLOPs/byte

  * å› æ­¤ I (64) > I_crit (50) â†’ **ä¸æ˜¯ global BW ç“¶é¢ˆ**ï¼ˆå¯èƒ½ compute-bound æˆ– L2é™å®šï¼‰
* T_compute_tile â‰ˆ F_tile / FLOPS_per_SM_effective

  * å‡è®¾æ¯ SM effective FLOPS å¯¹åº”åˆ°è¿™ä¸ª block çš„ share æ˜¯ 500 GFLOPSï¼ˆç¤ºæ„ï¼‰ï¼ŒT_compute_tile â‰ˆ 1.048e6 / 5e8 = 0.0021 s? Wait å•ä½ä¸å¯¹ â€”â€” ç”¨ cycles æ›´ç›´è§‚ï¼š
  * FLOPS_per_cycle_per_SM = 500 GFLOPS / 1 GHz = 500 FLOPs/cycle â†’ T_compute_tile â‰ˆ 1.048e6 / 500 â‰ˆ 2096 cycles
* S_min = ceil(L_total_cycle / T_compute_tile) + 1 = ceil(500 / 2096) + 1 = 1 + 1 = 2 â†’ double buffering è¶³å¤Ÿ
* Shared usage per block:

  * A = 128*32*2 = 8192 bytes (8 KB)
  * B = same = 8 KB
  * total â‰ˆ 16 KB (plus buffers) â†’ S_sh_total 256 KB æ”¯æŒå¾ˆå¤š blocks
* reg estimate: å‡è®¾ reg_per_thread â‰ˆ 32 regs, TperBlock = 256 â†’ 8192 regs per block â†’ R_reg_total 65536 å¯æ”¯æ’‘ 8 blocks per SMï¼ˆå— W_max é™åˆ¶ï¼‰
* occupancy OK.

ç¤ºä¾‹è¯´æ˜ï¼šåœ¨è¿™äº›å‡è®¾ä¸‹ (128,128,32) æ˜¯å¯è¡Œä¸” double-buffer è¶³å¤Ÿã€‚ç°å®ä¸­æŠŠå…·ä½“å¡çš„ BWã€FLOPSã€L_total_cycleã€reg total ç²¾ç¡®ä»£å…¥å³å¯å¾—åˆ°æ›´å‡†ç¡®ç»“è®ºã€‚

---

# 7) å®è·µå»ºè®®ã€heuristics ä¸ checklistï¼ˆå¿«é€Ÿä¸Šæ‰‹ï¼‰

* ä¼˜å…ˆåš**ç¬¦å·åŒ–ä¼°ç®—**ï¼ˆä¸Šé¢æµç¨‹ï¼‰æ¥æ’æ‰æ˜æ˜¾ä¸å¯èƒ½çš„ tileï¼ˆä¾‹å¦‚ä¼šè¶… shared/regï¼‰ã€‚
* å¸¸ç”¨èµ·ç‚¹ tileï¼šBM=BN=128ï¼ŒBK=32 / 64ï¼ˆå¯¹ FP16/FP8 å¾ˆå¸¸è§ï¼‰ã€‚ç„¶åæŒ‰ä¸Šé¢å…¬å¼æ£€æŸ¥ Iã€shared/regã€S_minã€‚
* å¯„å­˜å™¨ä¼˜å…ˆçº§ï¼šè‹¥å¯„å­˜å™¨æˆä¸ºç“¶é¢ˆï¼ŒæŠŠéƒ¨åˆ† C accum æ”¾ sharedï¼ˆä»£ä»·ï¼šæ›´å¤š sharedï¼‰ï¼Œæˆ–è€…å‡å° c_per_threadï¼ˆæ¯çº¿ç¨‹ç´¯åŠ æ›´å°‘ï¼‰ï¼Œç„¶åå¢åŠ  thread æ•°/warp mappingã€‚
* è½¯ä»¶æµæ°´ stagesï¼š

  * å¦‚æœ T_compute_tile â‰« L_total_cycleï¼ŒS=2 å°±å¤Ÿï¼ˆdouble bufferï¼‰ã€‚
  * å¦‚æœ T_compute_tile â‰² L_total_cycleï¼Œéœ€è¦ S>2ï¼›S è¶Šé«˜ç”¨äºé®æ©è¶Šå¤š latencyï¼Œä½† buffer æˆæœ¬å¢é«˜ã€‚
  * å®é™…ä¸Šï¼Œä½¿ç”¨ `cp.async`/TMA + double-buffer + 2â€“3 stage å¸¸èƒ½ç»™å¾ˆå¤§æ”¶ç›Šã€‚
* ä¼˜åŒ– memory path çš„å¯¹é½ä¸ coalescingï¼šä¿è¯ global loads coalescedï¼ˆA/B çš„è®¿é—®æ¨¡å¼æŒ‰ warp/seg é…ç½®ï¼‰ï¼Œå‡å°‘ L2 thrashingã€‚
* é¿å… shared bank conflictsï¼špad shared rows/cols ä»¥é¿å…å†²çªã€‚
* Autotuneï¼šåœ¨å¤šä¸ªåˆç† tile ä¸Šè·‘å¾®åŸºå‡†ï¼ˆæµ‹ throughputã€bandwidth utilã€SM utilizationï¼‰å¹¶é€‰æ‹©æœ€ä½³ã€‚è®¸å¤šå®ç°ï¼ˆCUTLASSï¼‰å°±æ˜¯è¿™ä¹ˆåšçš„ã€‚
* è‹¥åšé‡åŒ–ï¼ˆQ4/K ç­‰ï¼‰ï¼šæŠŠè§£å‹ï¼è½¬ FP16 çš„å¼€é”€çº³å…¥ D_tile_globalï¼ˆå³ä» global è¯»å…¥åéœ€è¦è½¬æ¢çš„ä»£ä»·ï¼‰ï¼›æœ€ä¼˜æ˜¯æŠŠ dequant + matmul + requant èåˆï¼Œé¿å…å¤šæ¬¡å†™å›ï¼è¯»å‡ºã€‚

---

# 8) å¸¸è§è¯¯åŒºä¸é™·é˜±

* **åªçœ‹å…±äº«å†…å­˜å®¹é‡ï¼Œä¸çœ‹å¯„å­˜å™¨**ï¼šå¯„å­˜å™¨å¾€å¾€æ›´å…ˆæˆä¸ºç“¶é¢ˆï¼Œå¯¼è‡´ç¼–è¯‘å™¨æ’å…¥ spill åˆ° local memoryï¼Œæ€§èƒ½æš´è·Œã€‚ä¸€å®šå…ˆæ£€æŸ¥ reg_per_thread Ã— threads_per_blockã€‚
* **å¿½è§†å®é™… sustained bandwidth ä¸ç†è®ºå¸¦å®½å·®**ï¼šç”¨ sustained BWï¼ˆbenchmark æµ‹åˆ°çš„ï¼‰è€Œä¸æ˜¯æ ‡ç§°å€¼ã€‚
* **å¿½è§† L2 / L1 å¸¦å®½ / äº‰ç”¨**ï¼šå¦‚æœå¤šä¸ª block åŒæ—¶ä»åŒä¸€è¡Œè¯»å–ï¼Œä¼šäº§ç”Ÿ L2/L1 å†²çªï¼Œé™ä½å®é™… reuseã€‚
* **tile å°ºå¯¸ä¸æ˜¯è¶Šå¤§è¶Šå¥½**ï¼šå°½ç®¡å¢å¤§ BM/BN æé«˜ç®—å­å¼ºåº¦ï¼Œä½†ä¼šçˆ†æ‰ shared/reg/occupancyï¼Œä»è€Œé™ä½ overall throughputã€‚

---

# 9) å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥ç›´æ¥å¸®ä½ åšä¸¤ä»¶äº‹ï¼ˆé€‰å…¶ä¸€æˆ–éƒ½åšï¼‰

* ç»™æˆ‘ä½ ç›®æ ‡ GPU çš„å‡ ä¸ªç¡¬ä»¶å‚æ•°ï¼ˆS_sh_totalã€R_reg_totalã€BW_globã€FLOPS_peakã€L_glob_cyclesï¼‰ï¼Œæˆ‘æŠŠä¸Šé¢çš„æµç¨‹ç”¨ä¸€ä¸ªå°è¡¨æ ¼/è„šæœ¬è·‘å‡ºä¸€ç»„å€™é€‰ (BM,BN,BK) + æ¨è pipeline stages å¹¶ç»™å‡ºæ¯ä¸ªå€™é€‰çš„ Iã€shared usageã€reg usageã€S_minã€‚ï¼ˆæˆ‘ä¼šç»™å‡ºå…·ä½“æ•°å€¼å’Œæ¨èï¼‰
* æˆ–è€…æˆ‘ç›´æ¥å†™ä¸€æ®µ pseudo-code / CUDA kernel skeletonï¼ˆå¸¦ double/triple bufferingã€cp.async çš„ä¼ªå®ç°ç»“æ„ï¼‰ï¼ŒæŠŠ buffer å¸ƒå±€ã€warp-to-fragment mappingã€å¯„å­˜å™¨åˆ†é…å»ºè®®éƒ½å†™å‡ºæ¥ï¼Œä¾¿äºä½ ç§»æ¤åˆ° CUTLASS æˆ–è‡ªå·±å®ç°ã€‚

ä½ æƒ³å…ˆè¦å“ªä¸€ç§ï¼Ÿï¼ˆæˆ‘ä¸ä¼šç­‰å¾…ä½ å»åšé¢å¤–äº‹æƒ… â€”â€” å¦‚æœä½ æŠŠç¡¬ä»¶å‚æ•°è´´ä¸Šæ¥æˆ‘ç«‹åˆ»æŠŠæ•°å€¼ä»£å…¥å¹¶ç»™å‡ºå…·ä½“å€™é€‰ï¼›å¦‚æœä½ ä¸è´´ï¼Œæˆ‘å°±ç»™ä¸€ä¸ªé€šç”¨çš„ parametric è¡¨æ ¼ + kernel skeletonã€‚ï¼‰
