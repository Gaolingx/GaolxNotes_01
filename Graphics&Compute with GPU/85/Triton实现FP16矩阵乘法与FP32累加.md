# Tritonå®ç°FP16çŸ©é˜µä¹˜æ³•ä¸FP32ç´¯åŠ 

### ğŸ§‘â€ğŸ’» User

å¦‚ä½•ç”¨tritonå®ç°ä¸€ä¸ªfp16 matmul,fp32 acc gemm kernel?

---

### ğŸ¤– Assistant

# ä½¿ç”¨ Triton å®ç° FP16 çŸ©é˜µä¹˜æ³•ä¸ FP32 ç´¯åŠ çš„ GEMM Kernel

ä¸‹é¢æˆ‘å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨ Triton å®ç°ä¸€ä¸ªé«˜æ•ˆçš„ FP16 çŸ©é˜µä¹˜æ³• kernelï¼Œå¹¶åœ¨ç´¯åŠ é˜¶æ®µä½¿ç”¨ FP32 ç²¾åº¦ä»¥é˜²æ­¢ç²¾åº¦æŸå¤±ã€‚

## å®ç°æ€è·¯

1. ä½¿ç”¨ FP16 å­˜å‚¨è¾“å…¥çŸ©é˜µï¼Œå‡å°‘å†…å­˜å¸¦å®½éœ€æ±‚
2. åœ¨è®¡ç®—è¿‡ç¨‹ä¸­å°† FP16 å€¼è½¬æ¢ä¸º FP32 è¿›è¡Œç´¯åŠ ï¼Œç¡®ä¿æ•°å€¼ç¨³å®šæ€§
3. ä½¿ç”¨åˆ†å—(tiling)ç­–ç•¥ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼
4. åˆ©ç”¨å…±äº«å†…å­˜(shared memory)å‡å°‘å…¨å±€å†…å­˜è®¿é—®

## å®Œæ•´å®ç°ä»£ç 

```python
import torch
import triton
import triton.language as tl

@triton.autotune(
    configs=[
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),
        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),
    ],
    key=['M', 'N', 'K'],
)
@triton.jit
def matmul_kernel(
    # çŸ©é˜µæŒ‡é’ˆ
    a_ptr, b_ptr, c_ptr,
    # çŸ©é˜µç»´åº¦
    M, N, K,
    # æ­¥é•¿ä¿¡æ¯
    stride_am, stride_ak,
    stride_bk, stride_bn,
    stride_cm, stride_cn,
    # å…ƒå‚æ•°
    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # ç¡®å®šç¨‹åºID
    pid = tl.program_id(axis=0)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    num_pid_in_group = GROUP_SIZE_M * num_pid_n
    group_id = pid // num_pid_in_group
    first_pid_m = group_id * GROUP_SIZE_M
    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)
    pid_m = first_pid_m + (pid % group_size_m)
    pid_n = (pid % num_pid_in_group) // group_size_m

    # è®¡ç®—å†…å­˜åç§»
    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)
    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)

    # è¿­ä»£Kç»´åº¦è¿›è¡Œè®¡ç®—
    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        # åŠ è½½Aå’ŒBçš„å—
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        
        # å°†FP16è½¬æ¢ä¸ºFP32å¹¶è¿›è¡ŒçŸ©é˜µä¹˜æ³•ç´¯åŠ 
        a = a.to(tl.float32)
        b = b.to(tl.float32)
        accumulator += tl.dot(a, b)
        
        # ç§»åŠ¨æŒ‡é’ˆåˆ°ä¸‹ä¸€ä¸ªKå—
        a_ptrs += BLOCK_SIZE_K * stride_ak
        b_ptrs += BLOCK_SIZE_K * stride_bk

    # å°†FP32ç´¯åŠ ç»“æœè½¬æ¢ä¸ºFP16å­˜å‚¨
    c = accumulator.to(tl.float16)
    
    # è®¡ç®—CçŸ©é˜µçš„å†™å…¥ä½ç½®
    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
    
    # å­˜å‚¨ç»“æœ
    tl.store(c_ptrs, c, mask=c_mask)

def matmul(a, b):
    # æ£€æŸ¥è¾“å…¥
    assert a.shape[1] == b.shape[0], "çŸ©é˜µç»´åº¦ä¸åŒ¹é…"
    assert a.dtype == torch.float16 and b.dtype == torch.float16, "è¾“å…¥å¿…é¡»æ˜¯FP16"
    
    M, K = a.shape
    K, N = b.shape
    c = torch.empty((M, N), device=a.device, dtype=torch.float16)
    
    # å¯åŠ¨kernel
    grid = lambda META: (
        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),
    )
    
    matmul_kernel[grid](
        a, b, c,
        M, N, K,
        a.stride(0), a.stride(1),
        b.stride(0), b.stride(1),
        c.stride(0), c.stride(1),
    )
    return c

# æµ‹è¯•å‡½æ•°
def test_matmul():
    # åˆ›å»ºéšæœºFP16çŸ©é˜µ
    M, N, K = 1024, 1024, 1024
    a = torch.randn((M, K), device='cuda', dtype=torch.float16)
    b = torch.randn((K, N), device='cuda', dtype=torch.float16)
    
    # è®¡ç®—Tritonç»“æœ
    c_triton = matmul(a, b)
    
    # è®¡ç®—PyTorchç»“æœä½œä¸ºå‚è€ƒ
    c_ref = torch.matmul(a.float(), b.float()).half()
    
    # æ¯”è¾ƒç»“æœ
    print(f"æœ€å¤§è¯¯å·®: {torch.max(torch.abs(c_triton - c_ref))}")
    print(f"å¹³å‡è¯¯å·®: {torch.mean(torch.abs(c_triton - c_ref))}")
    
    # éªŒè¯æ­£ç¡®æ€§
    assert torch.allclose(c_triton, c_ref, atol=1e-2, rtol=0), "ç»“æœä¸åŒ¹é…"

if __name__ == "__main__":
    test_matmul()
```

## å…³é”®å®ç°ç»†èŠ‚

1. **æ•°æ®ç±»å‹å¤„ç†**ï¼š
   - è¾“å…¥çŸ©é˜µä½¿ç”¨FP16å­˜å‚¨
   - ç´¯åŠ å™¨ä½¿ç”¨FP32é˜²æ­¢ç²¾åº¦æŸå¤±
   - æœ€ç»ˆç»“æœè½¬æ¢å›FP16å­˜å‚¨

2. **å†…å­˜è®¿é—®ä¼˜åŒ–**ï¼š
   - ä½¿ç”¨åˆ†å—ç­–ç•¥å‡å°‘å†…å­˜è®¿é—®
   - åˆ©ç”¨TLè¯­è¨€å†…ç½®çš„`tl.dot`æ“ä½œè¿›è¡Œé«˜æ•ˆçŸ©é˜µä¹˜æ³•

3. **è‡ªåŠ¨è°ƒä¼˜**ï¼š
   - ä½¿ç”¨`@triton.autotune`è£…é¥°å™¨è‡ªåŠ¨é€‰æ‹©æœ€ä½³é…ç½®
   - æä¾›å¤šç§å—å¤§å°é…ç½®ä»¥é€‚åº”ä¸åŒçŸ©é˜µå°ºå¯¸

4. **è¾¹ç•Œå¤„ç†**ï¼š
   - ä½¿ç”¨æ©ç (mask)å¤„ç†éæ•´é™¤çš„çŸ©é˜µç»´åº¦
   - ç¡®ä¿ä¸ä¼šè¶Šç•Œè®¿é—®å†…å­˜

è¿™ä¸ªå®ç°æä¾›äº†é«˜æ•ˆçš„FP16çŸ©é˜µä¹˜æ³•ï¼ŒåŒæ—¶åœ¨ç´¯åŠ é˜¶æ®µä½¿ç”¨FP32ç²¾åº¦ä¿æŒæ•°å€¼ç¨³å®šæ€§ï¼Œé€‚åˆåœ¨æ”¯æŒTritonçš„GPUä¸Šè¿è¡Œã€‚