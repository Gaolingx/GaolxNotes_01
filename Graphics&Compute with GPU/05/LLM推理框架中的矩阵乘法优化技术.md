# LLM推理框架中的矩阵乘法优化技术

### 🧑‍💻 User

为什么llm推理框架中的matmul kernel要对weight block做分块和重排，具体是如何实现的？

---

### 🤖 Assistant

在LLM推理框架中（如vLLM、TensorRT-LLM、FlashAttention等），对权重矩阵（Weight Matrix）进行**分块（Tiling）和重排（Reordering）** 是优化大规模矩阵乘法（MatMul）Kernel性能的核心技术之一，尤其是在GPU上。其主要目的是解决**访存瓶颈（Memory Bound）** 问题，提升计算单元利用率。

**为什么需要分块和重排？**

1.  **GPU内存层次结构的限制：**
    *   **全局内存（HBM）：** 容量大（GB级别），但带宽高、延迟高（数百周期）。权重矩阵通常很大（GB级别），直接访问非常慢。
    *   **共享内存（Shared Memory / L1 Cache）：** 容量小（KB级别，如192KB/SM on A100），但带宽极高、延迟低（几十周期）。理想的计算数据源。
    *   **寄存器文件（Register File）：** 容量最小（KB级别/SM），速度最快。是计算单元（CUDA Core/Tensor Core）的直接操作对象。
    *   **问题：** 权重矩阵太大，无法一次性放入高速的共享内存或寄存器中供计算单元持续使用。

2.  **数据复用（Data Reuse）最大化：**
    *   在 `Y = X @ W` (其中 `X` 是激活矩阵，`W` 是权重矩阵) 中：
        *   `X` 的行（一个Batch的Token）通常会被复用于计算 `W` 的所有列（输出特征）。
        *   `W` 的列（一组输出神经元）通常会被复用于计算 `X` 的所有行（Batch内所有Token）。
    *   **目标：** 将计算划分成小块（Tiles），使得每个小块计算所需的数据（`X` 的一个Tile和 `W` 的一个Tile）能塞进共享内存甚至寄存器中。这样，`X` 的一个Tile可以复用多次（与 `W` 的不同Tiles相乘），`W` 的一个Tile也可以复用多次（与 `X` 的不同Tiles相乘）。

3.  **访存模式（Memory Access Pattern）优化：**
    *   **合并访问（Coalesced Access）：** GPU访问全局内存时，连续的线程（通常是相邻的32个线程，一个Warp）最好访问连续的内存地址。非合并访问会导致多次内存事务，严重降低带宽利用率。
    *   **Bank Conflict（共享内存冲突）：** GPU共享内存被分成多个Bank（通常是32个）。如果一个Warp内的多个线程同时访问同一个Bank的不同地址，就会发生冲突，导致串行访问。
    *   **问题：**
        *   权重矩阵 `W` 通常按行主序（Row-Major）存储：`W[out_dim][in_dim]`。计算 `X @ W` 时，每个输出元素需要 `W` 的一整行（连续的 `in_dim` 个元素）。当多个线程（负责输出矩阵的不同列）同时读取各自需要的 `W` 行时，它们访问的是全局内存中完全不连续的地址，导致**非合并访问**。
        *   即使将 `W` 的一个Tile加载到共享内存，如果保持原始布局，多个线程同时读取该Tile的不同行时，很容易发生**Bank Conflict**。

**如何实现：分块和重排**

核心思想是将大的 `W` 矩阵分解成多个小块（Blocks/Tiles），并在加载到共享内存或寄存器之前，对每个小块内的数据进行**布局转换（Layout Transformation）**，使其更适合后续计算单元的访问模式（特别是Tensor Core的要求）并避免Bank Conflict。

1.  **分块（Tiling）：**
    *   将大的 `W` 矩阵（维度 `[K, N]`，其中 `K` 是输入维度，`N` 是输出维度）在 `K` 和 `N` 维度上切割成更小的块（Tiles）。常见的Tile形状有 `128x32`, `128x64`, `256x64` 等（具体大小取决于硬件、数据类型、是否用Tensor Core）。
    *   计算过程被组织成嵌套循环：
        *   外层循环：遍历 `W` 在 `N` 维度（输出维度）上的Tiles。
        *   内层循环：遍历 `W` 在 `K` 维度（输入维度）上的Tiles（同时遍历 `X` 在 `K` 维度上的对应Tiles）。
        *   对于每个 `(K_tile, N_tile)` 对，将 `X` 的对应 `K_tile` 列（维度 `[B, K_tile]`）和 `W` 的 `(K_tile, N_tile)` 块（维度 `[K_tile, N_tile]`）加载到共享内存。
        *   在共享内存中，使用共享内存Tile进行局部矩阵乘累加（通常还会进一步分块利用寄存器）。

2.  **重排（Reordering / Layout Transformation）：**
    *   这是解决访存问题的关键一步。`W` 的分块数据在**从全局内存加载到共享内存的过程中**（或者加载后立即进行），其内部数据的排列顺序会被改变。
    *   **常见重排目标格式：**
        *   **用于CUDA Core的优化：** 将 `K_tile x N_tile` 的Tile从原始的Row-Major重排成**列主序（Column-Major）** 或特定的**共享内存布局**。目的是让后续计算时，一个Warp内的线程访问共享内存时能避免Bank Conflict（例如，让一个Warp访问的32个元素分布在32个不同的Bank上）。
        *   **用于Tensor Core的优化（更关键）：** Tensor Core要求输入矩阵满足特定的**Fragment**布局（如 `WMMA` 要求的 `row_major` 或 `col_major`，或 `mma.sync` 指令要求的 `.b16` / `.tf32` / `.bf16` 等特定布局）。最常见的重排是将 `W` 的Tile转换成 **`nvcuda::wmma::col_major`** 布局。
            *   具体操作：假设 `W` 原始布局是 `Row-Major [K, N]`。对于一个 `K_tile x N_tile` 的Tile，原始的 `(i, j)` 元素（第 `i` 行，第 `j` 列）在新的共享内存布局中会被放置到一个由 `(j % tile_stride_j, j / tile_stride_j, i)` 或类似公式计算出的位置。这相当于在 `N_tile` 维度（列）内部做了一个**转置（Transpose）** 或**交错（Interleaving）**。
    *   **如何实现重排：**
        *   **在加载线程中完成：** 负责从全局内存加载数据到共享内存的线程（通常是整个Block合作），在读取全局内存时，就有策略地按照目标布局写入共享内存。这通常需要精心设计线程索引的计算公式。
        *   **使用 `__ldg` 或 `__ldca` 指令：** 加载时利用常量缓存优化。
        *   **使用 `ldmatrix` 指令（Ampere+）：** 这是专门为加速矩阵乘法Tile加载和布局转换设计的PTX指令。一条 `ldmatrix` 指令可以让一个Warp（32线程）高效地从全局内存加载一个 `8x8`、`8x16`、`16x8` 或 `16x16` 的小Tile（具体大小取决于数据类型）到共享内存，并**自动完成布局转换**为目标格式（如Tensor Core要求的 `col_major` fragment）。这大大简化了代码并提升了加载效率。
        *   **向量化内存事务：** 使用 `float4` / `int4` / `uint4` 等类型进行加载/存储，最大化内存事务的吞吐量。

**具体流程示例（简化版，使用Tensor Core）：**

假设计算 `Y[B, N] = X[B, K] @ W[K, N]`，使用 `128x64` 的 `W` Tile。

1.  **Grid/Block 配置：** Grid覆盖输出 `Y` 的 `B x N` 空间。每个Block负责计算 `Y` 的一个 `B_tile x N_tile` 区域（例如 `64x128`）。每个Block有多个Warp（如256线程 = 8 Warps）。
2.  **分配共享内存：** 为 `X` 的Tile分配 `[B_tile, K_tile]` 空间，为 `W` 的Tile分配 `[K_tile, N_tile]` 空间（已考虑目标布局大小）。
3.  **外层循环（遍历 `N` 维度）：** 当前Block负责 `N` 维度的 `N_tile` 起始索引 `n_start`。
4.  **内层循环（遍历 `K` 维度）：**
    *   **加载 `X` Tile：** 将 `X[b_start: b_start+B_tile, k_start: k_start+K_tile]` 从全局内存加载到共享内存 `s_X`。加载过程通常设计成合并访问。
    *   **加载并重排 `W` Tile：** 将 `W[k_start: k_start+K_tile, n_start: n_start+N_tile]` 从全局内存加载到共享内存 `s_W`。
        *   关键：加载线程（例如一个Warp）读取全局内存中 `W` 的连续数据（可能是 `float4`），但根据目标布局（如 `col_major` for WMMA）计算其在 `s_W` 中的位置并写入。或者，更高效地使用 `ldmatrix.sync.aligned.m8n8.x4.shared.b16` 等指令，一次性加载一个 `16x16` 的 `bf16` Tile 并自动转换成 `col_major` 布局放入共享内存。
    *   **`__syncthreads()`：** 确保整个Block的线程都能看到完整的 `s_X` 和 `s_W` Tile。
    *   **局部计算：** 每个Warp（或线程组）从 `s_X` 和 `s_W` 中取出更小的子块（如 `16x16` for `bf16` Tensor Core），使用 `wmma::load_matrix_sync` 或 `mma.sync` 指令将其加载到Tensor Core的**寄存器级Fragment**，然后调用 `wmma::mma_sync` 或 `mma.sync` 执行矩阵乘累加操作。这些指令内部要求的布局由之前的重排保证。
    *   **`__syncthreads()`：** 确保所有线程完成对当前 `s_X` 和 `s_W` Tile 的使用，以便加载下一对Tile。
5.  **结束内层循环（`K`）：** 累加完成所有 `K` 维度上的Tiles。
6.  **写回结果：** 将寄存器Fragment中累加的最终结果（可能再做一次布局转换）写回全局内存 `Y[b_start: b_start+B_tile, n_start: n_start+N_tile]`。写回过程也需优化成合并访问。

**总结：**

| 步骤         | 目的                                                         | 关键技术/效果                                                 |
| :----------- | :----------------------------------------------------------- | :------------------------------------------------------------ |
| **分块 (Tiling)** | 将大矩阵分解成小块，适配GPU高速缓存（共享内存、寄存器）容量。 | 嵌套循环遍历K维度和N维度上的Tiles。                           |
| **重排 (Reordering)** | 改变权重Tile在共享内存中的布局，使其符合计算单元（特别是Tensor Core）的访问要求，并避免Bank Conflict。 | 在加载线程中使用精心设计的索引计算或 `ldmatrix` 指令完成自动转换（通常转成 `col_major`）。 |
| **结果**     | **1. 全局内存合并访问：** 加载 `W` Tile 更高效。<br>**2. 避免共享内存Bank Conflict：** 计算单元读取 `s_W` 更快。<br>**3. 适配Tensor Core：** 满足 `mma.sync` 指令的严格布局要求。<br>**4. 数据复用：** `X` Tile在共享内存中被复用 `N_tile` 次， `W` Tile被复用 `B_tile` 次。 | **显著提升MatMul Kernel的吞吐量，降低延迟，提高GPU利用率（接近理论算力）。** |

简而言之，对权重矩阵进行分块是为了让数据能放进高速内存，重排是为了让数据在高速内存中的摆放方式能让计算单元（特别是Tensor Core）以最高效、无冲突的方式访问它们。`ldmatrix` 等现代GPU指令的出现极大地简化并优化了这一过程。

---

### 🧑‍💻 User

关于这个数据重排，cpu的simd是不是也有硬性要求呢？

---

### 🤖 Assistant

是的，**CPU 的 SIMD（如 x86 的 SSE/AVX/AVX-512，ARM 的 NEON/SVE）在进行高效矩阵乘法或其它数据并行操作时，对数据的布局（即数据在内存中的排列顺序）也有严格的要求，虽然这种“硬性要求”的表现形式和 GPU 有所不同，但核心目标一致：最大化利用内存带宽和计算单元。**

以下是 CPU SIMD 在数据重排方面的关键点以及与 GPU 的对比：

1.  **对齐要求：**
    *   **硬性要求：** 大多数 SIMD 加载/存储指令 (**`_mm256_load_ps`, `vld1q_f32`** 等) 要求数据地址必须按 SIMD 寄存器宽度对齐（例如，AVX-512 需要 64 字节对齐）。未对齐的访问会导致性能显著下降（触发处理器内部的微操作处理）甚至引发硬件异常（取决于指令和设置）。
    *   **解决方案：** 在内存分配时就确保权重矩阵的起始地址和每个 Tile 的起始地址满足对齐要求。如果数据本身未对齐，需要使用专门的未对齐加载指令 (**`_mm256_loadu_ps`, `vld1q_f32_x4`** 等），但这通常比对齐加载慢。

2.  **连续访问与向量化：**
    *   **硬性要求（隐含）：** SIMD 指令最擅长处理**连续内存块**中的数据。一条 SIMD 加载指令可以一次性读取多个连续的浮点数到寄存器中（如 AVX2 一次读 8 个 `float`）。
    *   **问题：** 在计算 `Y = X @ W` 时：
        *   `X` 的行（一个 token 的特征向量）通常是连续的，适合 SIMD 加载。
        *   `W` 的列（一个输出神经元的所有权重）在 Row-Major 存储下也是连续的，适合 SIMD 加载。
        *   **但是，计算输出 `Y` 的一个元素 `Y[i, j]` 时，需要 `X[i, :]` (行向量) 和 `W[:, j]` (列向量) 的点积。**
    *   **关键冲突：** 直接计算点积时，`W[:, j]` 是一个**列**。在 Row-Major 存储的 `W` 矩阵中，**`W[:, j]` 的元素在内存中是不连续的！** 它们间隔了 `N` (输出维度) 个元素。这被称为**跨步访问（Strided Access）**。
    *   **性能灾难：** 使用 SIMD 计算这种点积极其低效：
        *   需要为 `W` 的每个权重元素执行一次单独的加载操作（无法利用 SIMD 的宽加载）。
        *   或者，使用 Gather 指令（如 AVX2 的 `_mm256_i32gather_ps`），但 Gather 在大多数 CPU 上性能开销很大（尤其是在步长较大时），远不如连续加载高效。

3.  **数据重排（核心优化）：**
    *   **目标：** 将权重矩阵 `W` 从 Row-Major (`[K, N]`) **重排（转置）为 Column-Major (`[N, K]`)** 或者以一种更适合计算 `X @ W` 的方式重新组织。
    *   **为什么 Column-Major 更好？**
        *   在 Column-Major 存储下，`W` 的一列 `W[:, j]` 在内存中是**连续**的！计算点积 `X[i, :] * W[:, j]` 时：
            *   加载 `X[i, :]`：仍然是连续加载（行）。
            *   加载 `W[:, j]`：现在也是连续加载（列在 Column-Major 中是连续存储的）。
        *   这样，计算点积的循环内核可以高效地使用 SIMD 进行乘加运算（如 `_mm256_fmadd_ps`），一次处理多个元素对（例如 8 个 `float`）。
    *   **如何实现重排：**
        *   **离线预处理：** 在模型加载时或首次推理前，将整个权重矩阵 `W` 从框架默认的 Row-Major 显式转置为 Column-Major (`W^T`)。这是最常见、最有效的方法。计算时实际执行 `Y = X @ W = (W^T @ X^T)^T`，但更高效的做法是直接计算 `Y = X * W^T`，其中 `*` 是 GEMM 操作（需要支持 Column-Major 输入的 GEMM 库），或者自己编写针对 `X (Row-Major)` 和 `W^T (Row-Major, 但内容等价于原 W 的 Column-Major)` 优化的 Kernel。
        *   **运行时分块转置：** 类似 GPU 的做法，在计算循环内部，将 `W` 的一个 Tile 从全局内存加载到 L1/L2 缓存后，在 CPU 缓存内进行转置（使用 SIMD 优化的小矩阵转置操作），然后再用于计算。这避免了存储整个转置矩阵的开销，但增加了每个 Tile 的计算开销（转置本身需要时间和指令）。适用于权重太大无法完整转储的场景，或者追求极致内存节省的场景。
        *   **使用专门的指令：** 某些 SIMD 指令集提供了有助于转置或数据重排的指令（如 Intel AVX-512 的 `vpermt2d` 或 ARM SVE 的灵活谓词和 gather/scatter），可以加速小块数据的重排过程。

4.  **缓存友好性：**
    *   **要求：** CPU 的缓存层次（L1, L2, L3）比 GPU 的更大但也更复杂。优化目标是让计算过程中访问的数据尽可能长时间地驻留在离核心最近的缓存中（L1）。
    *   **分块的作用：** 对 `W` 进行分块的核心目的之一就是使其单个 Tile 的大小能放入 CPU 的 L1 或 L2 缓存。这样，在计算 `X` 的多个行（Token）与该 `W` Tile 的乘积时，`W` Tile 只需要加载一次，然后在缓存中被反复复用。
    *   **重排的作用：** 在 Column-Major 布局下，`W` 的 Tile 在缓存中的布局与计算 `X` 的行和 `W` 的列的点积所需的访问模式完美匹配（都是连续访问），大大提高了缓存利用率和计算单元吞吐量。在 Row-Major 下访问 `W` 的列会导致缓存行被大量加载但只用到其中一小部分数据（缓存污染）。

**总结对比 (CPU SIMD vs. GPU Tensor Core/CUDA Core):**

| 特性             | CPU SIMD                                        | GPU (Tensor Core / CUDA Core)                      | 共同目标                            |
| :--------------- | :---------------------------------------------- | :------------------------------------------------ | :---------------------------------- |
| **硬性要求**     | **地址对齐** (多数加载/存储指令)                | **严格的数据布局** (Tensor Core Fragment)          | 确保指令能正确/高效执行             |
| **核心访问问题** | **跨步访问** (Row-Major W 的列访问不连续)       | **非合并访问** (Row-Major W 的行访问不连续)       | 最大化内存带宽利用率                |
| **主要重排方案** | **整体转置为 Column-Major (`W^T`)**             | **分块加载 + 块内布局转换 (-> `col_major`)**      | 使数据访问模式适合并行计算单元      |
| **重排驱动因素** | 使**列数据连续**，实现高效 SIMD 点积           | 使块内数据满足 Tensor Core 布局，避免 Bank Conflict | 提升计算单元利用率                  |
| **分块核心作用** | 使 Tile 放入 L1/L2 缓存，**复用权重**           | 使 Tile 放入共享内存/寄存器，**复用激活/权重**    | 解决内存墙，增加数据复用            |
| **关键优化指令** | `gather` (性能差)，专用重排指令 (有限)          | `ldmatrix` (Ampere+)，向量化加载 (`float4`)        | 加速数据搬运和布局转换              |
| **典型库/实现**  | oneDNN (MKL-DNN), BLIS, OpenBLAS (GEMM 参数)    | cuBLAS, cuDNN, CUTLASS, TensorRT-LLM, vLLM Kernels | 封装底层优化                        |

**结论：**

虽然 CPU SIMD 没有像 GPU Tensor Core 那样对数据块内部元素排列顺序有**极其硬性、指令级强制**的要求（除了地址对齐），但对数据进行**重排（尤其是整体转置为 Column-Major）和分块**是优化 CPU 上 LLM MatMul 性能的**绝对关键步骤**。这主要是为了解决**跨步访问**导致的 SIMD 加载效率低下和**缓存不友好**的问题，从而最大化利用宝贵的内存带宽和 SIMD 计算能力。其必要性和重要性不亚于 GPU 上的优化。高性能 CPU GEMM 库（如 oneDNN, OpenBLAS）内部都深度使用了这些技术。