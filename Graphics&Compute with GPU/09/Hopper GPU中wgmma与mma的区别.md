# Hopper GPU中wgmma与mma的区别

### 🧑‍💻 User

nvidia在hopper gpu中引入了wgmma，相较于mma有什么区别？

---

### 🤖 Assistant

NVIDIA 在 Hopper GPU 架构中引入的 **`wgmma` (Warp Group Matrix Multiply-Accumulate)** 指令是对之前架构（如 Ampere、Turing、Volta）中 **`mma` (Matrix Multiply-Accumulate)** 指令的重大演进和扩展。它们都服务于在 Tensor Cores 上执行高效的矩阵乘法累加操作（D = A * B + C），但设计目标、执行模型和适用场景有显著区别：

以下是主要区别：

1.  **执行单元范围：**
    *   **`mma`:** 在**单个 Warp**（通常是 32 个线程）内执行操作。Tensor Core 操作由 warp 内的线程协作发起和控制。
    *   **`wgmma`:** 在**一个 Warp Group**（在 Hopper 中通常包含 **4 个 Warps，共 128 个线程**）内执行操作。这是 `wgmma` 最核心的创新。Tensor Core 操作由 warp group 内的线程协作发起和控制。

2.  **线程协作模式：**
    *   **`mma`:** 需要 warp 内的所有 32 个线程**协同工作**（例如，通过 `ldmatrix` 指令协作加载数据到寄存器，然后共同发起 `mma` 调用）。线程直接参与数据的搬运和指令的同步执行。
    *   **`wgmma`:** 设计为**更异步和松散耦合**。发起 `wgmma` 操作的线程（可以是一个 warp group 中的少数线程，甚至理论上可以是单个线程，但通常还是需要协调）只需提供操作所需的**描述符**（包含操作数地址、形状、数据类型等信息）。实际的 Tensor Core 执行由硬件调度，发起线程不需要等待操作完成就可以继续执行其他指令（`wgmma` 通常与 `wgmma.wait_group` 或 `wgmma.commit_group` 配合使用进行显式同步）。数据加载（尤其是从共享内存）也由硬件更自动地处理，减轻了显式线程协作的负担。

3.  **目标操作规模：**
    *   **`mma`:** 主要针对相对较小的矩阵 Tile（例如 Ampere 上常见的 16x16x16 FP16, 或 8x32x16 INT8）。虽然可以通过多次 `mma` 调用来计算更大的矩阵，但协调开销相对较大。
    *   **`wgmma`:** 天然支持**更大规模**的矩阵乘法。一个 `wgmma` 操作可以直接处理更大的 Tile（例如 H100 上支持的 64x64x128 FP16/FP8/BF16, 128x128x64 INT4/INT8）。这显著减少了完成大型矩阵块计算所需的指令数量和同步点。

4.  **内存访问（特别是共享内存）：**
    *   **`mma`:** 加载输入矩阵（A, B）通常需要显式使用 `ldmatrix` 等指令，由 warp 内的线程协作完成，将数据从共享内存加载到寄存器。这需要仔细的共享内存布局（如 swizzling）来保证访问效率。
    *   **`wgmma`:** 对共享内存的访问模式进行了**硬件层面的深度优化**。通过 `wgmma.mma_async` 指令直接指定共享内存中的操作数地址和描述符，硬件能够自动高效地从共享内存中获取数据并送入 Tensor Cores，无需显式的 `ldmatrix` 加载到寄存器这一步。这极大地简化了编程，并允许硬件更灵活、更高效地利用共享内存带宽（例如，通过更宽的总线访问）。

5.  **编程模型与效率：**
    *   **`mma`:** 编程模型相对底层，需要开发者显式管理 warp 内线程的协作、数据加载和同步。对于大型计算，协调多个 warp 执行更大的 tile 需要额外的同步和通信开销。
    *   **`wgmma`:** 提供了**更高层次的抽象**。开发者关注的是描述需要执行的大型矩阵操作本身（通过描述符），硬件负责调度 warp group 内的资源和执行细节（包括数据加载和跨 warp 的协调）。这显著降低了编程复杂性，并减少了指令开销和同步开销，尤其是在计算大型矩阵块时。

6.  **吞吐量与延迟隐藏：**
    *   **`mma`:** 单个操作的吞吐量受限于 warp 级别。隐藏 Tensor Core 操作延迟主要依靠 warp 调度器在 warp 间切换。
    *   **`wgmma`:** 由于操作规模更大且异步执行，**单条指令就能启动更大的计算量**，提高了指令效率。更重要的是，发起 `wgmma` 的线程（或 warp）在操作执行期间**不需要阻塞等待**，可以继续执行后续不依赖该结果的指令（如加载下一批数据、执行其他计算）。这为**延迟隐藏**提供了巨大的潜力，允许更好地利用计算资源。显式的 `wgmma.wait_group` 用于确保结果在需要使用时已完成。

**总结：**

| 特性         | `mma` (Ampere, Turing, Volta)             | `wgmma` (Hopper)                                   | `wgmma` 的优势/变化                          |
| :----------- | :---------------------------------------- | :------------------------------------------------ | :------------------------------------------- |
| **执行范围** | **单个 Warp** (32 threads)                | **Warp Group** (4 warps, 128 threads)             | **更大执行单元**，天然支持大规模操作         |
| **协作模式** | **紧密耦合** (Warp 内所有线程显式协作)    | **异步/松散耦合** (描述符驱动，硬件自动执行)      | **简化编程**，**降低线程协调开销**           |
| **目标规模** | **较小 Tile** (e.g., 16x16x16)            | **大型 Tile** (e.g., 64x64x128, 128x128x64)       | **单条指令更大计算量**，**减少总指令数**     |
| **共享内存访问** | **显式** (需 `ldmatrix`, 需布局优化)    | **隐式/硬件优化** (直接访问, 自动高效加载)        | **简化编程**，**提高带宽利用率**             |
| **编程模型** | **相对底层** (管理协作/加载/同步)         | **更高抽象** (描述操作，硬件处理细节)             | **更易编程大型MM**，**关注算法而非协调**     |
| **执行模型** | **同步阻塞** (发起线程通常等待完成)       | **异步非阻塞** (发起后可继续执行，显式等待结果)   | **更好隐藏延迟**，**提高资源利用率**         |
| **主要目标** | **高效执行中小型MM**                      | **高效执行超大型MM，简化编程，最大化吞吐和延迟隐藏** | **面向下一代AI/HPC工作负载的扩展性**         |

**简单来说：**

*   **`mma`** 是 **“warp级”** 的矩阵乘累加，需要 warp 内紧密协作，适合中小规模计算。
*   **`wgmma`** 是 **“warp-group级”** 的、**异步描述符驱动**的矩阵乘累加，利用更大的执行单元和硬件自动化的数据加载/协调，**专为高效执行超大规模矩阵计算而设计**，显著简化编程并提升性能和资源利用率。

`wgmma` 是 Hopper 架构实现其巨大性能飞跃（尤其是在 Transformer 等大模型训练推理上）的关键创新之一，它解决了在大规模矩阵乘法中协调多个 warp 的复杂性和开销问题。