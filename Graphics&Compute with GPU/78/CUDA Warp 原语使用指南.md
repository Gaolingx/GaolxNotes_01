# CUDA Warp åŸè¯­ä½¿ç”¨æŒ‡å—

### ğŸ§‘â€ğŸ’» User

åˆ†ç±»æ¦‚è¿°cudaä¸­å¸¸è§çš„warpåŸè¯­æœ‰å“ªäº›ï¼Ÿç»“åˆä»£ç è¯´æ˜å¦‚ä½•æ­£ç¡®ä½¿ç”¨ä»–ä»¬ï¼Ÿ

---

### ğŸ¤– Assistant

å¥½çš„ï¼Œæˆ‘ä»¬æ¥åˆ†ç±»æ¦‚è¿°ä¸€ä¸‹ CUDA ä¸­å¸¸è§çš„ Warp åŸè¯­ (Warp Primitives)ï¼Œå¹¶ç»“åˆä»£ç è¯´æ˜å¦‚ä½•æ­£ç¡®ä½¿ç”¨å®ƒä»¬ã€‚

### ä»€ä¹ˆæ˜¯ Warp åŸè¯­ï¼Ÿ

åœ¨ CUDA ç¼–ç¨‹æ¨¡å‹ä¸­ï¼Œçº¿ç¨‹è¢«ç»„ç»‡æˆçº¿ç¨‹å— (Block)ï¼Œè€Œçº¿ç¨‹å—åˆè¢«åˆ’åˆ†ä¸ºä¸€ä¸ªä¸ªå¤§å°ä¸º 32 ä¸ªçº¿ç¨‹çš„å•å…ƒï¼Œè¿™ä¸ªå•å…ƒå°±å«åš **Warp**ã€‚åŒä¸€ä¸ª Warp ä¸­çš„ 32 ä¸ªçº¿ç¨‹åœ¨ç‰©ç†ä¸Šæ˜¯åŒæ—¶æ‰§è¡Œçš„ï¼ˆSIMT, Single Instruction, Multiple Threadsï¼‰ã€‚

**Warp åŸè¯­ (Warp Primitives)** æ˜¯ä¸€ç»„å†…å»ºå‡½æ•° (Intrinsics)ï¼Œå®ƒä»¬å…è®¸ä¸€ä¸ª Warp å†…çš„ 33 ä¸ªçº¿ç¨‹ä¹‹é—´è¿›è¡Œé«˜æ•ˆã€ä½å»¶è¿Ÿçš„é€šä¿¡å’Œåä½œï¼Œè€Œæ— éœ€é€šè¿‡å…±äº«å†…å­˜ (Shared Memory) å’Œæ˜¾å¼çš„ `__syncthreads()`ã€‚ä½¿ç”¨ Warp åŸè¯­é€šå¸¸èƒ½å¸¦æ¥æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå› ä¸ºå®ƒä»¬ç›´æ¥åœ¨ç¡¬ä»¶çº§åˆ«å®ç°ï¼Œé€Ÿåº¦è¿œå¿«äºåŸºäºå…±äº«å†…å­˜çš„çº¿ç¨‹é—´é€šä¿¡ã€‚

æ‰€æœ‰ Warp åŸè¯­å‡½æ•°çš„å‰ç¼€é€šå¸¸æ˜¯ `__` (åŒä¸‹åˆ’çº¿)ã€‚

-----

### Warp åŸè¯­çš„åˆ†ç±»ä¸è¯¦è§£

æˆ‘ä»¬å¯ä»¥å°†å¸¸è§çš„ Warp åŸè¯­å¤§è‡´åˆ†ä¸ºä»¥ä¸‹å‡ ç±»ï¼š

1.  **æŠ•ç¥¨ä¸é€‰ä¸¾ (Voting and Election)**
2.  **æ•°æ®å¹¿æ’­ (Broadcast)**
3.  **è§„çº¦ä¸èšåˆ (Reduction and Aggregation)**
4.  **Warp çº§åˆ«çš„çŸ©é˜µè¿ç®— (Matrix Operations)** - *ä¸»è¦ç”¨äº Tensor Core*

#### 1\. æŠ•ç¥¨ä¸é€‰ä¸¾ (Voting and Election)

è¿™ç±»åŸè¯­ç”¨äºæ£€æŸ¥ä¸€ä¸ª Warp å†…æ‰€æœ‰çº¿ç¨‹æˆ–éƒ¨åˆ†çº¿ç¨‹çš„æŸä¸ªå¸ƒå°”æ¡ä»¶æ˜¯å¦æˆç«‹ï¼Œå¹¶è¿”å›ä¸€ä¸ªç»Ÿä¸€çš„ç»“æœã€‚

  * **`__all_sync(mask, predicate)`**: åˆ¤æ–­ `mask` æŒ‡å®šçš„ Warp å†…æ‰€æœ‰çº¿ç¨‹çš„ `predicate` (ä¸€ä¸ªéé›¶æ•´æ•°) æ˜¯å¦éƒ½ä¸º `true`ã€‚åªæœ‰å½“æ‰€æœ‰ç›¸å…³çº¿ç¨‹çš„ `predicate` éƒ½ä¸ºçœŸæ—¶ï¼Œæ‰€æœ‰ç›¸å…³çº¿ç¨‹çš„è¿”å›å€¼æ‰ä¸º 1ï¼Œå¦åˆ™ä¸º 0ã€‚
  * **`__any_sync(mask, predicate)`**: åˆ¤æ–­ `mask` æŒ‡å®šçš„ Warp å†…æ˜¯å¦æœ‰**è‡³å°‘ä¸€ä¸ª**çº¿ç¨‹çš„ `predicate` ä¸º `true`ã€‚åªè¦æœ‰ä»»ä½•ä¸€ä¸ªç›¸å…³çº¿ç¨‹çš„ `predicate` ä¸ºçœŸï¼Œæ‰€æœ‰ç›¸å…³çº¿ç¨‹çš„è¿”å›å€¼å°±ä¸º 1ï¼Œå¦åˆ™ä¸º 0ã€‚
  * **`__ballot_sync(mask, predicate)`**: è¿™æ˜¯æœ€å¼ºå¤§çš„æŠ•ç¥¨åŸè¯­ã€‚å®ƒä¼šæ”¶é›† `mask` æŒ‡å®šçš„ Warp å†…æ¯ä¸ªçº¿ç¨‹çš„ `predicate` (çœŸæˆ–å‡)ï¼Œå¹¶ç”Ÿæˆä¸€ä¸ª 32 ä½çš„æ•´æ•°æ©ç  (bitmask) è¿”å›ç»™**æ‰€æœ‰**ç›¸å…³çº¿ç¨‹ã€‚å¦‚æœç¬¬ `i` ä¸ªçº¿ç¨‹çš„ `predicate` ä¸ºçœŸï¼Œé‚£ä¹ˆè¿”å›çš„æ•´æ•°ä¸­ç¬¬ `i` ä½å°±ä¸º 1ï¼Œå¦åˆ™ä¸º 0ã€‚
  * **`__activemask()`**: è¿”å›ä¸€ä¸ª 32 ä½æ•´æ•°ï¼Œè¡¨ç¤ºå½“å‰ Warp ä¸­æ‰€æœ‰**æ´»è·ƒ**çº¿ç¨‹çš„æ©ç ã€‚è¿™åœ¨å¤„ç†å¯èƒ½ç”±äºåˆ†æ”¯åˆ†åŒ– (branch divergence) å¯¼è‡´éƒ¨åˆ†çº¿ç¨‹ä¸æ´»è·ƒçš„æƒ…å†µæ—¶éå¸¸æœ‰ç”¨ã€‚

**å‚æ•°è¯´æ˜:**

  * `mask`: ä¸€ä¸ª 32 ä½æ•´æ•°ï¼Œç”¨äºæŒ‡å®š Warp ä¸­çš„å“ªäº›çº¿ç¨‹å‚ä¸æ­¤æ¬¡æ“ä½œã€‚é€šå¸¸ä½¿ç”¨ `0xffffffff` (å³ `(unsigned int)-1`) æ¥è¡¨ç¤º Warp å†…æ‰€æœ‰ 32 ä¸ªçº¿ç¨‹éƒ½å‚ä¸ã€‚`__activemask()` å¯ä»¥å¸®åŠ©ç”Ÿæˆä¸€ä¸ªåªåŒ…å«å½“å‰æ´»è·ƒçº¿ç¨‹çš„ `mask`ã€‚
  * `predicate`: æ¯ä¸ªçº¿ç¨‹æä¾›çš„å¸ƒå°”æ¡ä»¶ï¼Œé€šå¸¸æ˜¯ä¸€ä¸ª `int` æˆ– `bool` ç±»å‹çš„å€¼ã€‚

**ä»£ç ç¤ºä¾‹ï¼šæ£€æŸ¥æ•°æ®æ˜¯å¦å…¨éƒ¨æœ‰æ•ˆ**

å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªæ•°ç»„ï¼Œéœ€è¦æ£€æŸ¥ä¸€ä¸ª Warp å¤„ç†çš„ 32 ä¸ªæ•°æ®æ˜¯å¦éƒ½å¤§äº 0ã€‚

```cpp
#include <cuda_runtime.h>
#include <cooperative_groups.h> // ç°ä»£ CUDA ç¼–ç¨‹æ¨èåŒ…å«

__global__ void check_all_positive(const float* data, bool* results) {
    // è·å–çº¿ç¨‹åœ¨ Warp å†…çš„ç´¢å¼• (lane id)
    unsigned int lane_id = threadIdx.x % 32;
    // è·å– Warp åœ¨çº¿ç¨‹å—å†…çš„ç´¢å¼•
    unsigned int warp_id = threadIdx.x / 32;

    // è®¡ç®—å½“å‰çº¿ç¨‹åº”è¯¥åŠ è½½çš„æ•°æ®ç´¢å¼•
    int idx = threadIdx.x + blockIdx.x * blockDim.x;
    
    // æ¯ä¸ªçº¿ç¨‹åŠ è½½è‡ªå·±çš„æ•°æ®
    float my_data = data[idx];

    // åˆ¤æ–­å½“å‰çº¿ç¨‹çš„æ•°æ®æ˜¯å¦å¤§äº 0
    bool is_positive = (my_data > 0.0f);
    
    // ä½¿ç”¨ __all_sync æ£€æŸ¥ Warp å†…æ‰€æœ‰çº¿ç¨‹çš„æ•°æ®æ˜¯å¦éƒ½å¤§äº 0
    // 0xffffffff è¡¨ç¤º Warp å†…æ‰€æœ‰çº¿ç¨‹éƒ½å‚ä¸æŠ•ç¥¨
    unsigned int all_positive_mask = 0xffffffff;
    if (__all_sync(all_positive_mask, is_positive)) {
        // åªæœ‰å½“ Warp å†…æ‰€æœ‰çº¿ç¨‹çš„æ•°æ®éƒ½ > 0 æ—¶ï¼Œè¿™é‡Œçš„ä»£ç æ‰ä¼šè¢«æ‰§è¡Œ
        // è®© Warp çš„ç¬¬ä¸€ä¸ªçº¿ç¨‹ (lane 0) å†™å…¥ç»“æœ
        if (lane_id == 0) {
            results[warp_id + blockIdx.x * (blockDim.x / 32)] = true;
        }
    } else {
        if (lane_id == 0) {
            results[warp_id + blockIdx.x * (blockDim.x / 32)] = false;
        }
    }

    // ä½¿ç”¨ __ballot_sync çš„ä¾‹å­
    // ballot_result æ˜¯ä¸€ä¸ª 32 ä½çš„ bitmaskï¼Œè¿”å›ç»™ Warp å†…çš„æ‰€æœ‰çº¿ç¨‹
    unsigned int ballot_result = __ballot_sync(0xffffffff, is_positive);
    
    // è®© Warp çš„ç¬¬ä¸€ä¸ªçº¿ç¨‹æ‰“å°å‡ºæŠ•ç¥¨ç»“æœçš„äºŒè¿›åˆ¶è¡¨ç¤º
    if (lane_id == 0) {
        // ä½ å¯ä»¥åœ¨è¿™é‡Œåˆ†æ ballot_result æ¥äº†è§£å“ªäº›çº¿ç¨‹çš„æ•°æ®æ˜¯ positive çš„
        // ä¾‹å¦‚ï¼Œ(ballot_result & (1 << 5)) != 0 è¡¨ç¤ºç¬¬ 5 ä¸ªçº¿ç¨‹çš„æ•°æ®æ˜¯ positive çš„
    }
}
```

**æ­£ç¡®ä½¿ç”¨è¦ç‚¹:**

  * **åŒæ­¥ (Sync):** `_sync` åç¼€éå¸¸é‡è¦ã€‚å®ƒä¿è¯äº†å‚ä¸æ“ä½œçš„çº¿ç¨‹ (`mask` æŒ‡å®šçš„) åœ¨æ‰§è¡Œæ­¤åŸè¯­ä¹‹å‰ï¼Œéƒ½å·²ç»å®Œæˆäº†ä¹‹å‰çš„æŒ‡ä»¤ã€‚è¿™é¿å…äº†è¯»å†™å†²çª (RAW/WAR hazards)ã€‚**æ‰€æœ‰å‚ä¸çš„çº¿ç¨‹å¿…é¡»ä»¥ç›¸åŒçš„ `mask` è°ƒç”¨è¯¥åŸè¯­**ï¼Œå¦åˆ™è¡Œä¸ºæœªå®šä¹‰ã€‚
  * **åˆ†æ”¯åˆ†åŒ–:** å¦‚æœ `predicate` çš„è®¡ç®—åœ¨ä¸€ä¸ªæ¡ä»¶åˆ†æ”¯å†…ï¼Œå¯èƒ½ä¼šå¯¼è‡´ Warp åˆ†åŒ–ã€‚è¿™æ„å‘³ç€åªæœ‰éƒ¨åˆ†çº¿ç¨‹ä¼šæ‰§è¡Œè¯¥åŸè¯­ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½¿ç”¨ `__activemask()` æ¥ç”Ÿæˆ `mask` æ˜¯ä¸€ä¸ªå¥½ä¹ æƒ¯ï¼Œä»¥ç¡®ä¿åªæœ‰å½“å‰æ´»è·ƒçš„çº¿ç¨‹å‚ä¸æ“ä½œã€‚

-----

#### 2\. æ•°æ®å¹¿æ’­ (Broadcast)

è¿™ç±»åŸè¯­ç”¨äºå°†ä¸€ä¸ª Warp å†…æŸä¸ªç‰¹å®šçº¿ç¨‹çš„æ•°æ®ï¼Œé«˜æ•ˆåœ°å¹¿æ’­ç»™ `mask` æŒ‡å®šçš„å…¶ä»–æ‰€æœ‰çº¿ç¨‹ã€‚

  * **`__shfl_sync(mask, var, srcLane)`**: å°† `srcLane` (0-31) å·çº¿ç¨‹çš„ `var` å˜é‡çš„å€¼ï¼Œå¹¿æ’­ç»™ `mask` æŒ‡å®šçš„æ‰€æœ‰çº¿ç¨‹ã€‚
  * **`__shfl_up_sync(mask, var, delta)`**: å°†å½“å‰çº¿ç¨‹ `lane_id` å‡å» `delta` åçš„é‚£ä¸ªçº¿ç¨‹çš„ `var` å˜é‡å€¼ï¼Œè¿”å›ç»™å½“å‰çº¿ç¨‹ã€‚
  * **`__shfl_down_sync(mask, var, delta)`**: å°†å½“å‰çº¿ç¨‹ `lane_id` åŠ ä¸Š `delta` åçš„é‚£ä¸ªçº¿ç¨‹çš„ `var` å˜é‡å€¼ï¼Œè¿”å›ç»™å½“å‰çº¿ç¨‹ã€‚
  * **`__shfl_xor_sync(mask, var, laneMask)`**: å°†å½“å‰çº¿ç¨‹ `lane_id` ä¸ `laneMask` è¿›è¡Œå¼‚æˆ– (XOR) æ“ä½œï¼Œå¾—åˆ°æºçº¿ç¨‹çš„ IDï¼Œç„¶åå°†æºçº¿ç¨‹çš„ `var` å€¼è¿”å›ç»™å½“å‰çº¿ç¨‹ã€‚å¸¸ç”¨äºè¶å¼äº¤æ¢ (butterfly exchange) æ¨¡å¼ã€‚

**ä»£ç ç¤ºä¾‹ï¼šWarp å†…è§„çº¦æ±‚å’Œ**

Warp Shuffle æ˜¯å®ç°é«˜æ•ˆ Warp å†…è§„çº¦ (Reduction) çš„å…³é”®ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨ `__shfl_down_sync` è®¡ç®—ä¸€ä¸ª Warp å†… 32 ä¸ªæµ®ç‚¹æ•°ä¹‹å’Œçš„ä¾‹å­ã€‚

```cpp
#include <cuda_runtime.h>

__inline__ __device__ float warp_reduce_sum(float val) {
    // ä½¿ç”¨ __shfl_down_sync è¿›è¡Œå¯¹åŠè§„çº¦
    // æ¯æ¬¡å°† Warp åˆ†æˆä¸¤åŠï¼Œé«˜åŠåŒºçš„å€¼åŠ åˆ°ä½åŠåŒº
    // å‚ä¸æ©ç æ˜¯æ‰€æœ‰çº¿ç¨‹
    unsigned int mask = 0xffffffff;
    
    val += __shfl_down_sync(mask, val, 16);
    val += __shfl_down_sync(mask, val, 8);
    val += __shfl_down_sync(mask, val, 4);
    val += __shfl_down_sync(mask, val, 2);
    val += __shfl_down_sync(mask, val, 1);
    
    // æœ€ç»ˆç»“æœåœ¨ 0 å·çº¿ç¨‹ä¸­
    // å¯ä»¥é€šè¿‡ __shfl_sync å°†ç»“æœå¹¿æ’­å›æ‰€æœ‰çº¿ç¨‹
    return __shfl_sync(mask, val, 0); 
}

__global__ void sum_kernel(const float* input, float* output) {
    int idx = threadIdx.x + blockIdx.x * blockDim.x;
    float my_val = input[idx];
    
    // è®¡ç®— Warp å†…çš„å’Œ
    float warp_sum = warp_reduce_sum(my_val);
    
    // è®©æ¯ä¸ª Warp çš„ 0 å·çº¿ç¨‹å°†ç»“æœå†™å…¥å…¨å±€å†…å­˜
    if ((threadIdx.x % 32) == 0) {
        int warp_id = threadIdx.x / 32 + blockIdx.x * (blockDim.x / 32);
        output[warp_id] = warp_sum;
    }
}
```

**æ­£ç¡®ä½¿ç”¨è¦ç‚¹:**

  * **`_sync` çš„é‡è¦æ€§:** å’ŒæŠ•ç¥¨åŸè¯­ä¸€æ ·ï¼Œ`_sync` ä¿è¯äº†æ‰€æœ‰å‚ä¸çº¿ç¨‹çš„åŒæ­¥ã€‚
  * **æºçº¿ç¨‹æœ‰æ•ˆæ€§:** å½“ä½¿ç”¨ `__shfl_sync` æ—¶ï¼Œä½ å¿…é¡»ç¡®ä¿ `srcLane` æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ã€å‚ä¸æ“ä½œçš„çº¿ç¨‹ã€‚å¦‚æœ `srcLane` å¯¹åº”çš„çº¿ç¨‹ä¸åœ¨ `mask` ä¸­ï¼Œæˆ–è€…å·²ç»é€€å‡ºï¼Œé‚£ä¹ˆè¡Œä¸ºæ˜¯æœªå®šä¹‰çš„ã€‚å¯¹äº `up/down` æ“ä½œï¼Œå¦‚æœè®¡ç®—å‡ºçš„æºçº¿ç¨‹ ID è¶…å‡ºèŒƒå›´ (å¦‚ `lane_id < delta` for `__shfl_up_sync`)ï¼Œé‚£ä¹ˆæºæ•°æ®æ˜¯æœªå®šä¹‰çš„ã€‚
  * **è¿”å›è‡ªå·±çš„å€¼:** åœ¨ `up/down` æ“ä½œä¸­ï¼Œå¦‚æœè®¡ç®—å‡ºçš„æºçº¿ç¨‹ ID å°±æ˜¯å½“å‰çº¿ç¨‹è‡ªå·±ï¼Œé‚£ä¹ˆå®ƒä¼šè¿”å›è‡ªå·±çš„ `var` å€¼ã€‚

-----

#### 3\. è§„çº¦ä¸èšåˆ (Reduction and Aggregation)

ä» CUDA 9.0 å¼€å§‹ï¼Œå¼•å…¥äº†æ›´ç›´æ¥çš„ Warp çº§åˆ«è§„çº¦åŸè¯­ï¼Œå®ƒä»¬åœ¨è¯­ä¹‰ä¸Šæ›´æ¸…æ™°ï¼Œæœ‰æ—¶ä¹Ÿæ›´é«˜æ•ˆã€‚

  * **`__reduce_sync(mask, value, op)`**: å¯¹ `mask` æŒ‡å®šçš„çº¿ç¨‹ä¸­çš„ `value` æ‰§è¡ŒæŒ‡å®šçš„ `op` æ“ä½œã€‚`op` å¯ä»¥æ˜¯ `add`, `min`, `max`, `and`, `or`, `xor`ã€‚
  * ä¾‹å¦‚, `__reduce_add_sync(mask, value)` å°† `mask` å†…æ‰€æœ‰çº¿ç¨‹çš„ `value` ç›¸åŠ ï¼Œå¹¶å°†æœ€ç»ˆæ€»å’Œå¹¿æ’­ç»™æ‰€æœ‰å‚ä¸çº¿ç¨‹ã€‚

**ä»£ç ç¤ºä¾‹ï¼šä½¿ç”¨ `__reduce_add_sync` è¿›è¡Œ Warp å†…æ±‚å’Œ**

ä¸Šé¢çš„æ±‚å’Œä¾‹å­å¯ä»¥è¢«å¤§å¤§ç®€åŒ–ã€‚

```cpp
#include <cuda_runtime.h>

__inline__ __device__ float warp_reduce_sum_new(float val) {
    unsigned int mask = 0xffffffff;
    // ä¸€è¡Œä»£ç å®Œæˆ Warp å†…æ±‚å’Œï¼Œå¹¶æŠŠç»“æœå¹¿æ’­å›æ‰€æœ‰å‚ä¸çº¿ç¨‹
    return __reduce_add_sync(mask, val);
}

__global__ void sum_kernel_reduce(const float* input, float* output) {
    int idx = threadIdx.x + blockIdx.x * blockDim.x;
    float my_val = input[idx];
    
    // è®¡ç®— Warp å†…çš„å’Œ
    float warp_sum = warp_reduce_sum_new(my_val);
    
    // è®©æ¯ä¸ª Warp çš„ 0 å·çº¿ç¨‹å°†ç»“æœå†™å…¥å…¨å±€å†…å­˜
    if ((threadIdx.x % 32) == 0) {
        int warp_id = threadIdx.x / 32 + blockIdx.x * (blockDim.x / 32);
        output[warp_id] = warp_sum;
    }
}
```

**æ­£ç¡®ä½¿ç”¨è¦ç‚¹:**

  * **ç®€æ´æ€§ä¸å¯è¯»æ€§:** è¿™ç±»åŸè¯­å¤§å¤§å¢å¼ºäº†ä»£ç çš„å¯è¯»æ€§å’Œç®€æ´æ€§ã€‚
  * **æ€§èƒ½:** å®ƒä»¬é€šå¸¸ç”±ç¼–è¯‘å™¨è½¬æ¢ä¸ºä¸æ‰‹åŠ¨ Shuffle å®ç°åŒæ ·é«˜æ•ˆï¼ˆç”šè‡³æ›´ä¼˜ï¼‰çš„æŒ‡ä»¤åºåˆ—ã€‚
  * **åŒæ­¥è¯­ä¹‰:** `_sync` ä¾ç„¶è¡¨ç¤ºæ‰€æœ‰å‚ä¸çº¿ç¨‹çš„åŒæ­¥ç‚¹ã€‚

-----

#### 4\. Warp çº§åˆ«çš„çŸ©é˜µè¿ç®— (Matrix Operations)

è¿™ç±»åŸè¯­æ˜¯ä¸º **Tensor Core** è®¾è®¡çš„ï¼Œç”¨äºæ‰§è¡Œ `D = A * B + C` å½¢å¼çš„çŸ©é˜µèåˆä¹˜åŠ  (Matrix Multiply-Accumulate, MMA) è¿ç®—ã€‚å®ƒä»¬åœ¨æ·±åº¦å­¦ä¹ å’Œç§‘å­¦è®¡ç®—ä¸­è‡³å…³é‡è¦ã€‚

  * **`wmma::load_matrix_sync`**: ä»å†…å­˜åŠ è½½çŸ©é˜µåˆ†ç‰‡ (fragment) åˆ°æ¯ä¸ªçº¿ç¨‹çš„å¯„å­˜å™¨ä¸­ã€‚
  * **`wmma::mma_sync`**: åœ¨ Warp çº§åˆ«æ‰§è¡ŒçŸ©é˜µä¹˜åŠ æ“ä½œã€‚
  * **`wmma::store_matrix_sync`**: å°†è®¡ç®—ç»“æœçŸ©é˜µåˆ†ç‰‡å†™å›å†…å­˜ã€‚

è¿™äº›åŸè¯­çš„ä½¿ç”¨æ¯”è¾ƒå¤æ‚ï¼Œéœ€è¦åŒ…å« `mma.h` å¤´æ–‡ä»¶ï¼Œå¹¶å›´ç»• `wmma::fragment` æ¨¡æ¿ç±»è¿›è¡Œæ“ä½œã€‚

**ä»£ç ç¤ºä¾‹ï¼šä¸€ä¸ªç®€åŒ–çš„ WMMA ä½¿ç”¨æµç¨‹ (æ¦‚å¿µæ€§)**

```cpp
#include <mma.h>

using namespace nvcuda;

// å®šä¹‰çŸ©é˜µç»´åº¦å’Œç±»å‹
const int M = 16;
const int N = 16;
const int K = 16;

__global__ void wmma_gemm_kernel(half *a, half *b, float *c, float alpha, float beta) {
    // ... çœç•¥äº†ç´¢å¼•è®¡ç®— ...

    // 1. å£°æ˜ WMMA fragments
    wmma::fragment<wmma::matrix_a, M, N, K, half, wmma::col_major> a_frag;
    wmma::fragment<wmma::matrix_b, M, N, K, half, wmma::col_major> b_frag;
    wmma::fragment<wmma::accumulator, M, N, K, float> acc_frag;
    wmma::fragment<wmma::accumulator, M, N, K, float> c_frag;

    // 2. åŠ è½½æ•°æ®åˆ° fragments
    // `a` æ˜¯æŒ‡å‘ global memory çš„æŒ‡é’ˆ
    wmma::load_matrix_sync(a_frag, a, M);
    wmma::load_matrix_sync(b_frag, b, K);
    wmma::load_matrix_sync(c_frag, c, M, wmma::mem_col_major);

    // 3. æ‰§è¡Œ MMA è®¡ç®—: acc_frag = a_frag * b_frag + c_frag
    wmma::mma_sync(acc_frag, a_frag, b_frag, c_frag);

    // 4. å°†ç»“æœå†™å›
    // `c` æ˜¯æŒ‡å‘ global memory çš„æŒ‡é’ˆ
    wmma::store_matrix_sync(c, acc_frag, M, wmma::mem_col_major);
}
```

**æ­£ç¡®ä½¿ç”¨è¦ç‚¹:**

  * **ç¡¬ä»¶è¦æ±‚:** å¿…é¡»åœ¨æ”¯æŒ Tensor Core çš„ GPU æ¶æ„ä¸Šï¼ˆVolta, Turing, Ampere åŠä¹‹åï¼‰æ‰èƒ½ä½¿ç”¨ã€‚
  * **æ•°æ®ç±»å‹ä¸å¸ƒå±€:** å¯¹å‚ä¸è¿ç®—çš„çŸ©é˜µçš„æ•°æ®ç±»å‹ (å¦‚ `half`, `int8`), å¸ƒå±€ (è¡Œä¸»åº/åˆ—ä¸»åº) éƒ½æœ‰ä¸¥æ ¼è¦æ±‚ã€‚
  * **åä½œæ€§:** æ•´ä¸ª Warp å¿…é¡»ä½œä¸ºä¸€ä¸ªæ•´ä½“æ¥è°ƒç”¨ `wmma` çš„åŠ è½½ã€è®¡ç®—å’Œå­˜å‚¨å‡½æ•°ã€‚

### æ€»ç»“

| ç±»åˆ« | å¸¸ç”¨åŸè¯­ | ä¸»è¦ç”¨é€” |
| :--- | :--- | :--- |
| **æŠ•ç¥¨ä¸é€‰ä¸¾** | `__all_sync`, `__any_sync`, `__ballot_sync` | å¯¹ Warp å†…çº¿ç¨‹çš„æ¡ä»¶è¿›è¡Œå¿«é€Ÿåˆ¤æ–­ï¼Œç”¨äºæ§åˆ¶æµå’Œç®—æ³•ä¼˜åŒ–ã€‚ |
| **æ•°æ®å¹¿æ’­** | `__shfl_sync`, `__shfl_down_sync` ç­‰ | åœ¨ Warp å†…çº¿ç¨‹é—´é«˜æ•ˆäº¤æ¢æ•°æ®ï¼Œæ˜¯å®ç°è§„çº¦ã€æ‰«æç­‰å¹¶è¡Œç®—æ³•çš„åŸºç¡€ã€‚ |
| **è§„çº¦ä¸èšåˆ** | `__reduce_add_sync`, `__reduce_min_sync` ç­‰ | æ›´é«˜å±‚ã€æ›´ç®€æ´çš„ Warp å†…è§„çº¦å®ç°ã€‚ |
| **çŸ©é˜µè¿ç®—** | `wmma::mma_sync` ç­‰ (æ¥è‡ª `<mma.h>`) | åˆ©ç”¨ Tensor Core åŠ é€Ÿå°å‹çŸ©é˜µçš„ä¹˜åŠ è¿ç®—ã€‚ |

æ­£ç¡®ä½¿ç”¨ Warp åŸè¯­çš„å…³é”®åœ¨äºç†è§£å…¶ **Warp-Scope** çš„ç‰¹æ€§å’Œ **`_sync`** çš„åŒæ­¥è¯­ä¹‰ã€‚è¿™æ„å‘³ç€ä¸€ä¸ª Warp å†…çš„æ‰€æœ‰ç›¸å…³çº¿ç¨‹å¿…é¡»ååŒæ‰§è¡Œè¿™äº›æ“ä½œã€‚æŒæ¡å®ƒä»¬æ˜¯ç¼–å†™é«˜æ€§èƒ½ CUDA æ ¸å‡½æ•°çš„é‡è¦ä¸€æ­¥ã€‚