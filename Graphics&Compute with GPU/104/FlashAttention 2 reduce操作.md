# FlashAttention 2 reduceæ“ä½œ

### ğŸ§‘â€ğŸ’» User

transformer åŸç‰ˆå®ç°çš„flash attention 2 kernelä¸­æœ‰å“ªäº›reduceæ“ä½œï¼Ÿ

---

### ğŸ¤– Assistant

ä¸‹é¢æŒ‰ **Transformer åŸç‰ˆï¼ˆPyTorch å®˜æ–¹/Meta åŸç”Ÿï¼‰FlashAttention 2 kernel** çš„å®é™… kernel ç»“æ„ï¼ŒæŠŠ **æ‰€æœ‰å‡ºç°çš„ reduce æ“ä½œ** æŒ‰ç”¨é€”ã€ç»´åº¦ã€å‘ç”Ÿä½ç½®æ‹†è§£ç»™ä½ ã€‚å†…å®¹åŸºäº FA2 è®ºæ–‡ã€Tri Dao å®ç°ï¼ˆflash-attn2ï¼‰ã€PyTorch 2.2+ å®˜æ–¹ kernelã€ä»¥åŠ NVIDIA CUTLASS/CuTe æ”¯æ’‘ä»£ç çš„åˆ†ææ€»ç»“ã€‚

---

# âœ… FlashAttention 2 ä¸­çš„æ‰€æœ‰ Reduce æ“ä½œï¼ˆæŒ‰æ‰§è¡Œé˜¶æ®µåˆ†ç±»ï¼‰

FlashAttention 2 çš„æ ¸å¿ƒä¼˜åŒ–æ˜¯ **æµå¼ attentionï¼ˆstreaming softmaxï¼‰+ tile å¹¶è¡Œ + å¯„å­˜å™¨å±€éƒ¨æ€§**ã€‚
è¦å®ç°è¿™ä¸ªè¿‡ç¨‹ï¼Œå¿…é¡»è¿›è¡Œå¤šä¸ª **åˆ†å¸ƒå¼å½’çº¦ï¼ˆreduceï¼‰æ“ä½œ**ã€‚

ä¸‹é¢æŒ‰ Q/K/V tile å¤„ç†çš„ä¸‰ä¸ªé˜¶æ®µè¯´æ˜ã€‚

---

# 1ï¼‰QK^T é˜¶æ®µï¼šMatmul + æ¯è¡Œæœ€å¤§å€¼ reduceï¼ˆmax-reductionï¼‰

åœ¨å¤„ç†ä¸€ä¸ª Q tile Ã— K tile çš„å±€éƒ¨ attention score æ—¶ï¼Œä¼šå¾—åˆ°ä¸€å— **partial scores tile**ï¼ˆå¤§å°ï¼š`BLOCK_M Ã— BLOCK_N`ï¼‰ã€‚

### â—Reduce #1ï¼šè¡Œå†…æœ€å¤§å€¼ï¼ˆrow-wise max reductionï¼‰

ä¸ºäº†å®ç° **ç¨³å®š softmax**ï¼Œéœ€è¦åœ¨æ¯å¤„ç†ä¸€ä¸ª K tile åï¼š

```
m_i = max_j(score_i_j)
```

å³ï¼š
å¯¹ **æ¯ä¸ª query è¡Œ** åšä¸€æ¬¡ **reduce_max**ï¼ˆå¤§å°ä¸º BLOCK_Nï¼‰ã€‚

FlashAttention 2 åšçš„æ˜¯ **tile streaming softmax**ï¼Œæ‰€ä»¥è¿˜è¦æŠŠè¿™ä¸ªæœ€å¤§å€¼ä¸ä»¥å‰ tile çš„æœ€å¤§å€¼åˆå¹¶ï¼š

```
m_new = max(m_old, m_tile)
```

â¡ **reduce ç±»å‹ï¼šè¡Œæ–¹å‘ max reduce**
â¡ **ç›®çš„ï¼šsoftmax ç¨³å®šåŒ–ï¼ˆé¿å… exp overflowï¼‰**

---

# 2ï¼‰Softmax å½’ä¸€é˜¶æ®µï¼šexp + reduce_sumï¼ˆsum-reductionï¼‰

å¾—åˆ°æœ¬ tile çš„æœ€å¤§å€¼åï¼Œä¼šè®¡ç®—ï¼š

```
p_ij = exp(score_ij - m_i)
```

æ¥ä¸‹æ¥å¯¹æ¯ä¸ª query è¡Œè¿›è¡Œï¼š

### â—Reduce #2ï¼šè¡Œå†…æŒ‡æ•°å’Œï¼ˆrow-wise sum reductionï¼‰

```
s_i = Î£_j p_i_j
```

ä¸å†å²ç»Ÿè®¡çš„ softmax denominator åˆå¹¶ï¼š

```
l_new = l_old * exp(m_old - m_new) + s_tile
```

â¡ **reduce ç±»å‹ï¼šè¡Œæ–¹å‘ sum reduce**
â¡ **ç›®çš„ï¼šsoftmax è¡Œå½’ä¸€åŒ–**

è¿™ä¸ª reduce ä¹Ÿæ˜¯ FlashAttention 2 ä¸­æœ€æ˜‚è´µã€ä¼˜åŒ–æœ€å…³é”®çš„æ­¥éª¤ä¹‹ä¸€ï¼Œå› ä¸ºå®ƒåœ¨æ¯ä¸ª tile éƒ½è¦æ‰§è¡Œã€‚

---

# 3ï¼‰PV é˜¶æ®µï¼šåŠ æƒæ±‚å’Œ reduceï¼ˆweighted sum reductionï¼‰

å½“ softmax æƒé‡ç®—å‡ºæ¥åï¼Œéœ€è¦å°†å…¶ä¸ V tile ç›¸ä¹˜ï¼š

```
out_i += Î£_j  p_ij * V_j
```

è¿™æ˜¯ä¸€ä¸ªå…¸å‹çš„ **çŸ©é˜µ-å‘é‡åŠ æƒ reduce**ã€‚

### â—Reduce #3ï¼šæƒé‡ Ã— V çš„ reduce-sumï¼ˆrow Ã— V-dimï¼‰

å¯¹æ¯ä¸ª query è¡Œå’Œæ¯ä¸ª head dimensionï¼š

```
output[i, d] += Î£_j  p_ij * V[j, d]
```

â¡ å±äº **reduce_sum across K tile**
â¡ å®é™…åœ¨ kernel ä¸­æ˜¯ï¼š

* register å†…éƒ¨æ‰§è¡Œ FMA
* å¯¹çº¿ç¨‹/warp å†…éƒ¨ partial sums åš reduceï¼ˆwarp reduce + block reduceï¼‰

---

# 4ï¼‰K/V tile ç»´æŒä¸­é—´çŠ¶æ€çš„ reduceï¼ˆcross-thread reductionï¼‰

ä¸ºäº†åˆå¹¶æ¥è‡ªä¸åŒçº¿ç¨‹/warp çš„ partial sumï¼ŒFlashAttention 2 åœ¨ï¼š

* è¡Œæœ€å¤§å€¼è®¡ç®—
* è¡Œ sum è®¡ç®—
* PV åŠ æƒæ±‚å’Œ

å†…éƒ¨éƒ½éœ€è¦ **warp-level** / **block-level reduce**ï¼š

### â—Reduce #4ï¼šè·¨çº¿ç¨‹ reduceï¼ˆwarp reduceï¼‰

åŒ…æ‹¬ï¼š

* warp-level reduce_max
* warp-level reduce_sum
* block sync åçš„ block-level partial reduce

è¿™äº› reduce å¹¶ä¸ç›´æ¥å¯¹åº”ç®—æ³•æ­¥éª¤ï¼Œä½†å®ƒä»¬æ˜¯å®ç° FA2 kernel æ€§èƒ½çš„å…³é”®ã€‚

---

# 5ï¼‰Backwardï¼ˆåå‘ï¼‰ä¸­çš„ reduceï¼ˆå¦‚æœä½ å…³å¿ƒ backward kernelï¼‰

FlashAttention 2 backward åŒ…å«æ›´å¤š reduceï¼š

### â—Reduce #5ï¼šdP å¯¹è¡Œæ±‚å’Œï¼ˆsum reduceï¼‰

åœ¨ backwardï¼Œsoftmax çš„å¯¼æ•°éœ€è¦ï¼š

```
dp_i_j = dS_i_j - p_i_j * Î£_j(dS_i_j * p_i_j)
```

è¿™é‡Œï¼š

```
Î£_j(dS_i_j * p_i_j)
```

æ˜¯ä¸€ä¸ª **row-wise reduce_sum**ã€‚

### â—Reduce #6ï¼šdQ/dK/dV å›ä¼ ä¸­çš„ weighted reduce

ä¾‹å¦‚ï¼š

dQ = dP Ã— K
dK = dP^T Ã— Q
dV = P^T Ã— dO

è¿™äº›éƒ½æ˜¯ **è·¨ tile åŠ æƒæ±‚å’Œ reduce**ã€‚

---

# ğŸ”¥ æ€»ç»“ï¼šFlashAttention 2 çš„ Reduce æ“ä½œåˆ—è¡¨ï¼ˆå®Œæ•´ç‰ˆï¼‰

| é˜¶æ®µ               | Reduce ç±»å‹                  | ç›®çš„                    |
| ---------------- | -------------------------- | --------------------- |
| QK^T             | row-wise **max**           | streaming softmax ç¨³å®šåŒ– |
| Softmax          | row-wise **sum**           | softmax å½’ä¸€åŒ–           |
| PV               | row-wise **weighted sum**  | è¾“å‡ºçš„ç´¯ç§¯                 |
| Cross-thread     | warp reduce / block reduce | åˆå¹¶çº¿ç¨‹ partial sums     |
| Backward softmax | row-wise sum               | è®¡ç®— softmax å¯¼æ•°         |
| Backward matmul  | weighted sum               | dQ/dK/dV çš„ reduce     |

---

# ğŸ“Œ æœ€æ ¸å¿ƒçš„ 3 ä¸ª reduceï¼ˆforward pathï¼‰

å¦‚æœä½ åªå…³å¿ƒ forward FA2ï¼Œåˆ™ä¸»è¦ reduce å°±æ˜¯ 3 ä¸ªï¼š

1. **reduce_max**ï¼šsoftmax çš„è¡Œæœ€å¤§å€¼
2. **reduce_sum**ï¼šexp åçš„ softmax è¡Œå’Œ
3. **reduce_fma_sum**ï¼šp Ã— V çš„åŠ æƒå’Œ

è¿™ä¸‰ä¸ª reduce åœ¨ FA2 çš„æ‰€æœ‰ tile ä¸­æ¯æ¬¡éƒ½ä¼šæ‰§è¡Œï¼Œæ˜¯æ€§èƒ½ä¼˜åŒ–é‡ç‚¹ã€‚

---

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥ç»§ç»­æ·±å…¥ï¼š

* ç”»å‡º FA2 çš„é€ tile æ•°æ®æµå…¨å›¾
* ç»™ä½ çœ‹åŸç‰ˆ CUDA kernel ä¸­ reduce å…·ä½“ç”¨åˆ°çš„ warp primitives
* å¯¹æ¯” FA1 å’Œ FA2 åœ¨ reduce ç»„ç»‡æ–¹å¼ä¸Šçš„å·®åˆ«
* åˆ†æä¸ºä»€ä¹ˆ FA2 reduce æ¯” FA1 å¿« 2ï½4Ã—ï¼ˆblock æ‰å¹³åŒ– + register tiling + warp specializationï¼‰

ä½ éœ€è¦å“ªä¸€éƒ¨åˆ†ï¼Ÿ
