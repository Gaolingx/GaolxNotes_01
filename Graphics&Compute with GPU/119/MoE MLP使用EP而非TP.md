# MoE MLPä½¿ç”¨EPè€ŒéTP

### ğŸ§‘â€ğŸ’» User

ä¸ºä»€ä¹ˆå‚æ•°é‡å·¨å¤§çš„moeæ¨¡å‹mlp layersè¦ç”¨å¤§è§„æ¨¡epè€Œä¸æ˜¯tpï¼Ÿ

---

### ğŸ¤– Assistant

è¿™æ˜¯ä¸€ä¸ªéå¸¸æ·±åˆ»ä¸”è§¦åŠå¤§æ¨¡å‹ç³»ç»Ÿè®¾è®¡æ ¸å¿ƒçš„é—®é¢˜ã€‚ç®€å•æ¥è¯´ï¼Œåœ¨å‚æ•°é‡å·¨å¤§çš„ MoEï¼ˆMixture-of-Expertsï¼‰æ¨¡å‹çš„ MLP å±‚ä¸­ï¼Œä¼˜å…ˆé€‰æ‹© **ä¸“å®¶å¹¶è¡Œï¼ˆExpert Parallelism, EPï¼‰** è€Œä¸æ˜¯çº¯ç²¹çš„ **å¼ é‡å¹¶è¡Œï¼ˆTensor Parallelism, TPï¼‰**ï¼Œä¸»è¦æºäºä»¥ä¸‹ä¸‰ä¸ªæ ¸å¿ƒæƒè¡¡ï¼š**é€šä¿¡æ•ˆç‡**ã€**è®¡ç®—ç²’åº¦ï¼ˆçŸ©é˜µä¹˜æ³•æ•ˆç‡ï¼‰** ä»¥åŠ **æ‰©å±•æ€§**ã€‚

ä»¥ä¸‹æ˜¯è¯¦ç»†çš„æŠ€æœ¯åˆ†æï¼š

### 1. é€šä¿¡æ¨¡å¼ä¸å¼€é”€ (Communication Overhead)

è¿™æ˜¯æœ€ä¸»è¦çš„åŸå› ã€‚

*   **TP çš„é€šä¿¡æ¨¡å¼ (All-Reduce):**
    *   TP å°†ä¸€ä¸ªå¤§çš„çŸ©é˜µä¹˜æ³•åˆ‡åˆ†åˆ°å¤šä¸ª GPU ä¸Šã€‚åœ¨æ¯ä¸€å±‚ MLP çš„å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ä¸­ï¼Œéƒ½éœ€è¦è¿›è¡Œ **All-Reduce** æ“ä½œæ¥åŒæ­¥ç»“æœã€‚
    *   è¿™æ„å‘³ç€ï¼Œä¸ç®¡ä½ çš„æ¨¡å‹æœ‰å¤šç¨€ç–ï¼Œåªè¦ä½ ç”¨äº† TPï¼Œå°±éœ€è¦é¢‘ç¹çš„ã€é«˜å¸¦å®½çš„åŒæ­¥é€šä¿¡ã€‚
    *   å¯¹äº MoE æ¥è¯´ï¼Œè™½ç„¶æ¿€æ´»çš„å‚æ•°å°‘ï¼Œä½†å¦‚æœç”¨ TP åˆ‡åˆ†ä¸“å®¶ï¼Œæ‰€æœ‰ GPU å¿…é¡»åŒæ­¥ç­‰å¾…æ‰€æœ‰åˆ‡ç‰‡è®¡ç®—å®Œæˆã€‚

*   **EP çš„é€šä¿¡æ¨¡å¼ (All-to-All):**
    *   EP å°†ä¸åŒçš„â€œä¸“å®¶ï¼ˆExpertsï¼‰â€å®Œæ•´çš„æ”¾ç½®åœ¨ä¸åŒçš„ GPU ä¸Šã€‚
    *   é€šä¿¡å‘ç”Ÿåœ¨è®¡ç®—ä¹‹å‰ï¼ˆDispatchï¼‰å’Œè®¡ç®—ä¹‹åï¼ˆCombineï¼‰ã€‚è¿™æ˜¯ä¸€ä¸ª **All-to-All** çš„è¿‡ç¨‹ï¼šGPU å°† token å‘é€åˆ°æŒæœ‰å¯¹åº”ä¸“å®¶çš„ GPU ä¸Šã€‚
    *   **å…³é”®ç‚¹ï¼š** åœ¨ MoE ä¸­ï¼ŒToken æ˜¯ç¨€ç–è·¯ç”±çš„ã€‚å¯¹äºä¸€ä¸ª batchï¼Œåªæœ‰ä¸€éƒ¨åˆ† token ä¼šè¢«è·¯ç”±åˆ°ç‰¹å®šçš„ä¸“å®¶ã€‚EP åªéœ€è¦ç§»åŠ¨æ•°æ®ï¼ˆTokenï¼‰ï¼Œè€Œä¸€æ—¦ Token åˆ°è¾¾ç›®æ ‡ GPUï¼Œæ‰€æœ‰çš„çŸ©é˜µè®¡ç®—éƒ½æ˜¯**æœ¬åœ°çš„**ï¼Œæ— éœ€ä¸­é—´åŒæ­¥ã€‚
    *   åœ¨å¤§è§„æ¨¡é›†ç¾¤ä¸‹ï¼ŒAll-to-Allï¼ˆé‡æ’æ•°æ®ï¼‰çš„å¼€é”€é€šå¸¸ä½äºåœ¨æ¯ä¸€å±‚å†…éƒ¨åšé«˜é¢‘ All-Reduceï¼ˆåŒæ­¥æ¢¯åº¦/æ¿€æ´»å€¼ï¼‰çš„å¼€é”€ã€‚

### 2. è®¡ç®—ç²’åº¦ä¸ GEMM æ•ˆç‡ (Computational Granularity)

GPUæå…¶æ“…é•¿åšå¤§çš„çŸ©é˜µä¹˜æ³•ï¼ˆGEMMï¼‰ï¼Œéå¸¸è®¨åŒåšç¢å°çš„çŸ©é˜µä¹˜æ³•ã€‚

*   **å¦‚æœç”¨ TP åˆ‡åˆ† MoE:**
    *   MoE çš„ç‰¹ç‚¹æ˜¯ä¸“å®¶æ•°é‡å¤šï¼ˆä¾‹å¦‚ 64 ä¸ªï¼‰ï¼Œä½†æ¯ä¸ªä¸“å®¶çš„ä¸ªå¤´é€šå¸¸æ¯”åŒçº§åˆ«çš„ Dense æ¨¡å‹è¦å°ï¼ˆæˆ–è€…æŒå¹³ï¼‰ã€‚
    *   å¦‚æœå¯¹ä¸€ä¸ªæœ¬æ¥å°±ä¸ç®—å·¨å¤§çš„ä¸“å®¶çŸ©é˜µå†è¿›è¡Œ TP åˆ‡åˆ†ï¼ˆä¾‹å¦‚åˆ‡æˆ 8 ä»½ï¼‰ï¼Œæ¯ä¸ª GPU ä¸Šåˆ†åˆ°çš„çŸ©é˜µå—ä¼šéå¸¸å°ã€‚
    *   **åæœï¼š** è¿™ç§â€œç¢ç‰‡åŒ–â€çš„è®¡ç®—ä¼šå¯¼è‡´ GPU çš„ **Kernel Launch å¼€é”€å æ¯”è¿‡é«˜**ï¼Œä¸”æ— æ³•å¡«æ»¡ GPU çš„ Tensor Coreï¼Œå¯¼è‡´è®¡ç®—æ•ˆç‡ï¼ˆMFUï¼‰æ€¥å‰§ä¸‹é™ã€‚

*   **å¦‚æœç”¨ EP:**
    *   æ¯ä¸ª GPU è´Ÿè´£ä¸€ä¸ªæˆ–å¤šä¸ªå®Œæ•´çš„ä¸“å®¶ã€‚
    *   è¢«è·¯ç”±è¿‡æ¥çš„ Token ä¼šç»„æˆä¸€ä¸ª Batchã€‚
    *   GPU æ‰§è¡Œçš„æ˜¯ä¸€ä¸ªå®Œæ•´çš„ã€è¾ƒå¤§ Batch çš„çŸ©é˜µä¹˜æ³•ã€‚è¿™èƒ½æå¤§åœ°æé«˜ GPU çš„è®¡ç®—åˆ©ç”¨ç‡ã€‚

### 3. æ¨¡å‹çš„ç¨€ç–æ€§æœ¬è´¨ (Sparsity)

MoE çš„æ ¸å¿ƒè®¾è®¡ç†å¿µæ˜¯ï¼š**å‚æ•°é‡å·¨å¤§ï¼Œä½†è®¡ç®—é‡æ’å®šï¼ˆSparse activationï¼‰**ã€‚

*   **TP æ˜¯ä¸º Dense æ¨¡å‹è®¾è®¡çš„ï¼š** TP çš„ç›®çš„æ˜¯ä¸ºäº†è§£å†³å•å¡æ˜¾å­˜æ”¾ä¸ä¸‹ã€æˆ–è€…å•å¡è®¡ç®—å¤ªæ…¢çš„é—®é¢˜ã€‚å®ƒå‡è®¾æ‰€æœ‰çš„å‚æ•°éƒ½è¦å‚ä¸æ¯ä¸€æ¬¡è®¡ç®—ã€‚
*   **EP æ˜¯ä¸º Sparse æ¨¡å‹è®¾è®¡çš„ï¼š** MoE ä¸­ï¼Œæ¯æ¬¡åªæœ‰ Top-K ä¸ªä¸“å®¶è¢«æ¿€æ´»ã€‚
    *   ä½¿ç”¨ EPï¼Œæˆ‘ä»¬å®é™…ä¸Šæ˜¯å°†â€œé—²ç½®çš„ä¸“å®¶â€æ”¾åœ¨äº†åˆ«çš„ GPU ä¸Šï¼Œäº’ä¸å¹²æ‰°ã€‚
    *   å¦‚æœä½¿ç”¨ TPï¼Œæ‰€æœ‰ GPU éƒ½è¦å‚ä¸è®¡ç®—é‚£ä¸ªâ€œè¢«æ¿€æ´»çš„ä¸“å®¶â€çš„ä¸€å°éƒ¨åˆ†åˆ‡ç‰‡ï¼Œè¿™è¿èƒŒäº† MoE è§£è€¦è®¡ç®—çš„è®¾è®¡åˆè¡·ã€‚

### 4. æ‰©å±•æ€§ (Scalability)

*   **EP çš„çº¿æ€§æ‰©å±•ï¼š** å¦‚æœä½ æƒ³æŠŠ MoE æ¨¡å‹çš„å‚æ•°é‡æ‰©å¤§ 10 å€ï¼ˆå¢åŠ ä¸“å®¶æ•°é‡ï¼‰ï¼Œä½ åªéœ€è¦å¢åŠ  10 å€çš„ GPUï¼Œä½¿ç”¨ EP å³å¯ã€‚æ¯ä¸ª GPU ä¸Šçš„è®¡ç®—è´Ÿè½½å’Œé€šä¿¡æ¨¡å¼åŸºæœ¬ä¿æŒä¸å˜ã€‚
*   **TP çš„æ‰©å±•ç“¶é¢ˆï¼š** TP ä¸¥é‡å—é™äºèŠ‚ç‚¹å†…çš„äº’è”å¸¦å®½ï¼ˆå¦‚ NVLinkï¼‰ã€‚è·¨èŠ‚ç‚¹è¿›è¡Œ TP é€šä¿¡å»¶è¿Ÿå¤ªé«˜ï¼Œé€šå¸¸ä¸å¯æ¥å—ã€‚å› æ­¤ TP çš„è§„æ¨¡é€šå¸¸é™åˆ¶åœ¨å•æœºï¼ˆ8å¡ï¼‰æˆ–è€…å°è§„æ¨¡é›†ç¾¤å†…ã€‚è€Œå·¨å‹ MoE éœ€è¦æˆç™¾ä¸Šåƒå¼ å¡ï¼ŒEP å¤©ç„¶é€‚åˆè·¨èŠ‚ç‚¹éƒ¨ç½²ã€‚

### æ€»ç»“ä¸ç‰¹ä¾‹

| ç‰¹æ€§ | å¼ é‡å¹¶è¡Œ (TP) | ä¸“å®¶å¹¶è¡Œ (EP) |
| :--- | :--- | :--- |
| **é€šä¿¡æ“ä½œ** | All-Reduce (åŒæ­¥æ±‚å’Œ) | All-to-All (æ•°æ®è·¯ç”±) |
| **è®¡ç®—ä½ç½®** | ååŒè®¡ç®—ï¼Œåˆ‡åˆ†çŸ©é˜µ | æœ¬åœ°è®¡ç®—ï¼Œå®Œæ•´çŸ©é˜µ |
| **é€‚ç”¨åœºæ™¯** | å•ä¸ªçŸ©é˜µå·¨å¤§ï¼Œå¿…é¡»åˆ‡åˆ† | å¤šä¸ªç‹¬ç«‹çŸ©é˜µï¼ˆä¸“å®¶ï¼‰ï¼Œæ— éœ€åˆ‡åˆ† |
| **GPU æ•ˆç‡** | çŸ©é˜µåˆ‡åˆ†è¿‡å°ä¼šå¯¼è‡´åˆ©ç”¨ç‡ä½ | ä¿æŒçŸ©é˜µå®Œæ•´ï¼Œåˆ©ç”¨ç‡é«˜ |

**ç‰¹ä¾‹ï¼ˆEP+TPï¼‰ï¼š**
è™½ç„¶ EP æ˜¯ä¸»æµï¼Œä½†åœ¨æŸäº›æç«¯æƒ…å†µä¸‹ï¼ˆä¾‹å¦‚ DeepSeek-V3 æˆ–æŸäº›è¶…å¤§ä¸“å®¶æ¨¡å‹ï¼‰ï¼Œå¦‚æœ**å•ä¸ªä¸“å®¶çš„å¤§å°**å·²ç»å¤§åˆ°å•å¼  GPU æ”¾ä¸ä¸‹ï¼Œæˆ–è€…ä¸ºäº†å¹³è¡¡æ¨ç†æ—¶çš„æ˜¾å­˜å¸¦å®½å‹åŠ›ï¼Œä¹Ÿä¼šé‡‡ç”¨ **EP + TP** çš„æ··åˆæ¨¡å¼ã€‚å³ï¼šä¸åŒ GPU ç»„è´Ÿè´£ä¸åŒä¸“å®¶ï¼ˆEPï¼‰ï¼Œä½†ç»„å†…çš„å¤šä¸ª GPU å…±åŒé€šè¿‡ TP åˆ‡åˆ†è®¡ç®—åŒä¸€ä¸ªä¸“å®¶ã€‚

åœ¨æ‚¨æä¾›çš„ä»£ç ç‰‡æ®µä¸­ï¼ˆä¾‹å¦‚ `ParallelLlamaMLP`ï¼‰ï¼Œå±•ç¤ºçš„æ˜¯å…¸å‹çš„ Dense æ¨¡å‹çš„ TP å®ç°ã€‚å¦‚æœæ˜¯ MoEï¼Œä»£ç é€»è¾‘ä¼šå®Œå…¨ä¸åŒï¼Œé€šå¸¸æ¶‰åŠ `Gate -> Dispatch (All-to-All) -> Local Computation -> Combine (All-to-All)` çš„æµç¨‹ã€‚