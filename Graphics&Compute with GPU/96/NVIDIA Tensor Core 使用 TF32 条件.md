# NVIDIA Tensor Core 使用 TF32 条件

### 🧑‍💻 User

机器学习场景，nvidia的tensorcore什么情况下会用到tf32精度？

---

### 🤖 Assistant

## 机器学习场景中，NVIDIA Tensor Core 何时会使用 TF32 精度？

在机器学习场景中，NVIDIA 的 Tensor Core 会在满足以下几个关键条件时自动使用 TF32（TensorFloat-32）精度进行计算，从而在几乎不牺牲模型精度的前提下，显著提升训练和推理的性能：

**核心条件概览：**

1.  **硬件支持：** GPU 必须是 NVIDIA Ampere 架构或更新的架构。这包括 A100、H100、RTX 30 系列、RTX 40 系列等。更早的架构（如 Volta 或 Turing）不支持 TF32。

2.  **操作类型：** TF32 主要加速大规模的矩阵乘法（GEMMs）和卷积运算。这是深度学习模型中最常见的计算密集型操作，例如全连接层、卷积层以及 Transformer 中的注意力机制等。

3.  **输入数据类型：** 参与运算的张量（Tensor）必须是 32 位浮点数（FP32 或 `float32`）。TF32 是一种内部计算格式，它会接收 FP32 的输入，在 Tensor Core 内部以 TF32 的精度执行矩阵乘加运算，然后输出 FP32 的结果。因此，用户的模型和数据无需进行任何代码层面的修改，继续使用标准的 FP32 即可。

4.  **软件和框架支持：** 主流的深度学习框架需要启用对 TF32 的支持。幸运的是，从较新的版本开始，这通常是默认行为：

      * **PyTorch:** 从 1.7 版本开始，默认在支持的硬件上为卷积和矩阵乘法启用 TF32。
      * **TensorFlow:** 从 2.4 版本开始，默认在支持的硬件上启用 TF32。

**TF32 的工作机制：**

TF32 是一种巧妙的混合精度格式。它拥有与 FP32 相同的 8 位指数位，这意味着它可以表示的数值范围与 FP32 完全相同，从而避免了 FP16 中常见的梯度消失或溢出问题。然而，它的尾数位（精度）从 FP32 的 23 位减少到了 10 位，与 FP16 的尾数位相同。

在进行矩阵运算时，Ampere 及后续架构的 Tensor Core 会将输入的 FP32 数据“看作”TF32 进行处理，利用硬件加速的优势高速完成计算，最后将计算结果以标准的 FP32 格式写回内存。整个过程对用户是透明的，无需更改代码。

**如何控制 TF32 的使用？**

尽管 TF32 是默认启用的，但在某些对精度要求极高的科学计算或特定模型中，用户可能希望禁用它以确保完全的 FP32 精度。可以通过以下方式进行控制：

  * **PyTorch:**

    ```python
    import torch

    # 禁用 cuDNN 卷积的 TF32
    torch.backends.cudnn.allow_tf32 = False

    # 禁用 CUDA 矩阵乘法的 TF32
    torch.backends.cuda.matmul.allow_tf32 = False
    ```

  * **TensorFlow:**
    虽然 TensorFlow 默认启用，但可以通过设置环境变量来全局禁用。

  * **全局环境变量:**
    NVIDIA 提供了一个环境变量 `NVIDIA_TF32_OVERRIDE`，可以强制禁用 TF32。在执行脚本前设置该变量即可：

    ```bash
    export NVIDIA_TF32_OVERRIDE=0
    python your_training_script.py
    ```

**总结**

| 条件 | 描述 |
| :--- | :--- |
| **GPU 架构** | NVIDIA Ampere 或更新架构 (A100, H100, RTX 30/40系列等) |
| **操作类型** | 矩阵乘法 (GEMMs) 和卷积运算 |
| **输入数据** | 32位浮点数 (FP32) |
| **深度学习框架** | PyTorch \>= 1.7, TensorFlow \>= 2.4 (默认开启) |

因此，在现代的机器学习工作流程中，只要您使用的是受支持的 NVIDIA GPU 和较新版本的深度学习框架，并且您的模型正在执行标准的 FP32 训练或推理，那么 Tensor Core 就会在后台自动利用 TF32 精度来加速您的计算密集型任务。这为开发者提供了一个“免费”的性能提升，是 Ampere 及以后架构的一大核心优势。

---

### 🧑‍💻 User

为什么训练时的前向/反向传播经常会用到tf32精度？而优化器数值更新仍旧在fp32精度？

---

### 🤖 Assistant

您提出的这个问题非常精准，直击了现代深度学习训练中性能与精度权衡的核心。您的观察是完全正确的：**训练过程中的前向/反向传播大量使用 TF32，而优化器（Optimizer）的权重更新和状态累加则坚守 FP32。**

这背后的根本原因是这两部分任务在**计算特性**和**对数值精度的敏感度**上有着本质的区别。

---

### 1. 前向/反向传播：追求极致的计算吞吐量

在前向传播（计算预测值和损失）和反向传播（计算梯度）的过程中，绝大部分的计算都集中在两类操作上：
* **矩阵乘法 (GEMMs):** 在全连接层、RNN、Transformer的自注意力机制中。
* **卷积 (Convolutions):** 在卷积神经网络（CNN）中。

这些操作具有以下特点：

* **计算密集型:** 它们占据了训练时钟周期的绝大部分（通常超过80%）。
* **大规模并行:** 涉及到的都是大型张量之间独立的、并行的运算。
* **对微小误差不敏感:**
    * 深度学习训练本身就是一个**带有噪声的、随机的优化过程**（例如使用随机梯度下降SGD及其变体）。
    * 单次前向/反向传播中由 TF32 引入的微小计算误差，相对于数据本身的噪声、随机初始化、dropout 等正则化手段来说，影响微乎其微。
    * 这些微小的误差在数百万次的迭代中，其影响会被整体的梯度下降方向和统计规律所“平均掉”，通常不会影响模型的最终收敛效果。

**因此，对于前向/反向传播，首要目标是“快”。** TF32 正是为此而生，它利用 Ampere 及更新架构上的 Tensor Core，在几乎不改变代码的情况下，将这些核心运算的速度提升数倍，而其精度（10位尾数）对于计算梯度来说已经完全足够。TF32 拥有和 FP32 相同的数值范围（8位指数），也避免了 FP16 容易出现的溢出或下溢问题，是一个非常平衡的“免费午餐”。

---

### 2. 优化器更新：保障数值累加的稳定性

优化器（如 Adam, SGD, RMSprop）执行的任务则完全不同。它的核心是根据计算出的梯度来更新模型的权重，公式通常类似：

`new_weight = old_weight - learning_rate * gradient`

对于带有动量（Momentum）或自适应学习率的优化器（如 Adam），还需要更新其内部状态变量（如一阶矩 `m` 和二阶矩 `v`）。

这些操作的特点是：

* **累加的本质:** 权重更新是一个**长期、持续的累加过程**。在数百万步的训练中，每一次微小的更新都会累积到原始权重上。
* **对精度高度敏感:**
    * **梯度可能非常小:** 特别是在训练后期，梯度值可能变得非常微小。如果使用较低的精度，`learning_rate * gradient` 的结果可能会因为小于该精度格式所能表示的最小数值而被**舍入为零**，导致权重不再更新，即“权重停滞”（Stale Weights）。
    * **权重本身是“高精度资产”:** 模型的权重是整个训练过程的最终沉淀。如果主权重副本的精度不足，会丢失训练过程中学到的精细信息。
    * **优化器状态需要精确:** Adam 等优化器的状态变量（如动量）是对历史梯度的累积或滑动平均。如果这些累加器使用低精度，会很快丢失历史信息，破坏优化器算法的有效性。

**因此，对于优化器更新，首要目标是“准”。** 在这一步，计算量本身并不大（只是简单的元素级加减乘除），性能不是瓶颈。关键在于必须确保每一次微小的更新都能被精确地应用和累积。使用 FP32 可以提供足够的精度和动态范围，来捕捉和累积这些细微的变化，确保模型能够稳定地收敛到最优状态。

---

### 总结与类比

| 特性 | 前向/反向传播 | 优化器更新 |
| :--- | :--- | :--- |
| **核心任务** | 计算梯度 | 应用梯度，更新权重 |
| **计算类型** | 大规模矩阵乘法、卷积 | 元素级的加、减、乘、除 |
| **主要瓶颈** | **计算吞吐量** (GPU算力) | **数值稳定性** |
| **精度要求** | **较低** (对微小误差容忍度高) | **极高** (需要累积微小变化) |
| **理想格式** | **TF32** (速度快，范围广，精度够用) | **FP32** (保证累加的精确性) |

**一个形象的比喻：**

* **优化器和权重** 就像是银行账户的**总余额**。你希望这个数字是绝对精确的（FP32）。
* **前向/反向传播中的梯度** 就像是每天成千上万笔**交易的计算过程**。在计算每笔交易的手续费或利息时，稍微简化一下计算（TF32），只要误差在可控范围内，对最终的总余额影响是可以忽略不计的，但处理速度却能大大加快。但最终结算到总余额时，你必须使用最精确的方式入账。

这就是为什么混合精度训练（Mixed Precision Training）中，通常会将模型权重保留一份 FP32 的主副本（Master Weights），在前向/反向传播时使用 FP16/BF16 进行计算，最后将计算出的梯度转换回 FP32，再由优化器更新到 FP32 的主权重上。TF32 则更进一步，让这个过程对用户几乎透明化了。