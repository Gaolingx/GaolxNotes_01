# Grouped GEMM æ€§èƒ½ä¼˜åŠ¿

### ğŸ§‘â€ğŸ’» User

GroupedGEMM for MoEä¸ºä»€ä¹ˆç›¸è¾ƒäºå¯åŠ¨topkä¸“å®¶ä¸ªç‹¬ç«‹çš„batch gemmï¼Œæ€§èƒ½æ›´é«˜ï¼Ÿ

---

### ğŸ¤– Assistant

ä¸‹é¢ç»™ä½ ä¸€ä¸ª**ä»æ¶æ„å±‚é¢ã€kernel çº§è®¿å­˜ã€è°ƒåº¦ã€å¹¶è¡Œåº¦**åˆ° **æ··åˆé‡åŒ–** çš„å®Œæ•´è§£é‡Šï¼Œä¸ºä»€ä¹ˆ **Grouped GEMM åœ¨ MoE ä¸­æ¯”ä¾æ¬¡æ‰§è¡Œ Top-k ä¸“å®¶ GEMM è¦å¿«å¾—å¤š**ã€‚

---

# âœ… TL;DRï¼ˆæ ¸å¿ƒåŸå› ï¼‰

Grouped GEMM æŠŠ â€œå¤šä¸ªä¸“å®¶çš„å°çŸ©é˜µâ€ **åˆå¹¶æˆä¸€ä¸ªå¤§æ‰¹æ¬¡ï¼ˆbatchedï¼‰çš„å¤§å‹ GEMM** æ¥æ‰§è¡Œï¼Œä½¿å¾—ï¼š

* **æ›´é«˜çš„ GPU åˆ©ç”¨ç‡ï¼ˆSM occupancyï¼‰**
* **æ›´å¥½çš„ Tensor Core tile åˆ©ç”¨ç‡ï¼ˆæ›´å¤§ M/N/K â†’ æ›´æ¥è¿‘å³°å€¼ï¼‰**
* **æ›´æœ‰æ•ˆçš„å†…å­˜è®¿é—®ï¼ˆcoalesced, L2 reuseï¼‰**
* **å‡å°‘ kernel launch å¼€é”€**
* **æ›´å°‘çš„ DRAM å¾€è¿”**
* **å‡å°‘å° GEMM çš„è°ƒåº¦ç¢ç‰‡**

ç®€å•è¯´ï¼š

> **GPU ä¸æ“…é•¿è·‘å¾ˆå¤šâ€œå°çŸ©é˜µâ€ï¼Œæ“…é•¿è·‘å°‘é‡â€œå¤§çŸ©é˜µâ€ã€‚Grouped GEMM æŠŠæ‰€æœ‰å°çŸ©é˜µè‡ªåŠ¨æ‰“åŒ…æˆä¸€ä¸ªâ€œå¤§çŸ©é˜µâ€å»è·‘ã€‚**

---

# 1. ğŸŸ¥ æ ¹æœ¬åŸå› ï¼šå° GEMM æ€§èƒ½æå·®ï¼ˆTensor Core tile åˆ©ç”¨ç‡ä½ï¼‰

MoE æ¯ä¸ªä¸“å®¶ FFN é€šå¸¸æ˜¯ï¼š

* hidden_dim = 4096
* ffn_dim = 11008ï¼ˆæˆ–ç±»ä¼¼ï¼‰
* æ¯ä¸ª token åˆ†é…åˆ°æå°‘æ•°ï¼ˆtop-1 / top-2ï¼‰ä¸“å®¶ â†’ **åˆ†æ‘Šåçš„ M å¾ˆå°**

å¦‚æœæ¯ä¸ªä¸“å®¶åªå¤„ç†å°‘é‡ tokenï¼Œæ¯”å¦‚ 2ï½32 ä¸ªï¼Œé‚£ä¹ˆ**ç‹¬ç«‹ GEMM çš„ MÃ—Kã€NÃ—K éƒ½å¾ˆå°**ã€‚

Tensor Core çš„å›ºå®š tile æ˜¯ï¼š

* Ampere/Hopperï¼š16Ã—16Ã—16
* Blackwellï¼šä¾ç„¶ 16Ã—16ï¼Œä½†æ›´å¯†é›†

å¦‚æœçŸ©é˜µç»´åº¦ä¸å¤§äº 16/32ï¼Œå°±ä¼šæœ‰å¤§é‡ç¢ç‰‡ â†’ **æ€§èƒ½æ‰åˆ° 10â€“30% ç†è®ºå³°å€¼**

GroupedGEMM æŠŠæ¥è‡ªå¤šä¸ªä¸“å®¶çš„ **M å †ä¸€èµ·** ç»„æˆä¸€ä¸ª large-batchï¼Œè®©çŸ©é˜µæ›´æ¥è¿‘å¤§ tileï¼Œå› æ­¤æ€§èƒ½ç«‹å³æš´æ¶¨åˆ° 70â€“95% å³°å€¼ã€‚

---

# 2. ğŸŸ§ SM occupancyï¼šç‹¬ç«‹ tiny GEMM æ— æ³•å–‚é¥± GPU

å‡è®¾æœ‰ 160 ä¸“å®¶ï¼Œtop-2 è·¯ç”± â†’ æ¯æ¬¡è¦å¯åŠ¨ 320 ä¸ªå° GEMMã€‚
å…¶ä¸­å¤§éƒ¨åˆ†ä¸“å®¶æ¯æ¬¡åªæœ‰æå°‘ tokenï¼ˆè´Ÿè½½ä¸å‡ï¼‰ã€‚

è¿™å¯¼è‡´ï¼š

| æ–¹æ¡ˆ           | æ¯æ¬¡ GEMM å¤§å°            | å¹¶è¡Œåº¦                | SM åˆ©ç”¨ç‡         |
| ------------ | --------------------- | ------------------ | -------------- |
| ç‹¬ç«‹ GEMM      | å¾ˆå¤šå° GEMM              | æ¯ä¸ª GEMM å¯åŠ¨å¾ˆå°‘ block | **ä½ï¼ˆ20â€“40%ï¼‰**  |
| Grouped GEMM | åˆå¹¶æˆä¸€ä¸ªå·¨å¤§çš„ batched GEMM | block æ•°å·¨å¤§          | **é«˜ï¼ˆ80â€“100%ï¼‰** |

Grouped GEMM å¯ä»¥å°†æ•´ä¸ª MoE å±‚å½“æˆä¸€ä¸ªå·¨å‹ GEMM æ¥è·‘ï¼ŒSM å®Œå…¨åƒæ»¡ã€‚

---

# 3. ğŸŸ¨ Kernel launch æ•°é‡å‡å°‘ï¼ˆéå¸¸å¤§å½±å“ï¼‰

ç‹¬ç«‹ GEMM:

* top-2 experts
* æ¯å±‚ ~ 64 ä¸ª MoE å±‚
* æ¯å±‚ ~ 20â€“40 ä¸ª token per expertï¼ˆä½†ç¨€ç–ï¼‰

ä½ å¯èƒ½éœ€è¦ï¼š

> **å‡ ç™¾åˆ°å‡ åƒä¸ª GEMM launch**

GPU kernel-launch overhead å¯¹äºå° GEMM å æ¯”æé«˜ï¼ˆç”šè‡³ >30% æ—¶é—´ï¼‰ã€‚

Grouped GEMMï¼š

> åª launch ä¸€æ¬¡ â†’ kernel å†…éƒ¨è‡ªåŠ¨è°ƒåº¦æ‰€æœ‰ä¸“å®¶çš„ tileã€‚

launch overhead è¢«å®Œå…¨æ‘Šè–„ã€‚

---

# 4. ğŸŸ© L2 / registers / shared memory reuse æ›´é«˜

ç‹¬ç«‹å° GEMMï¼š

* æ¯ä¸ªä¸“å®¶éƒ½è¦ç‹¬ç«‹åŠ è½½æƒé‡åˆ° L2
* é‡å¤å¸¦å®½æµªè´¹å·¨å¤§
* ç”±äº token å¾ˆå°‘ï¼Œæƒé‡ç¼“å­˜å‘½ä¸­ç‡ä½

Grouped GEMMï¼š

* æƒé‡è¿ç»­å¸ƒå±€ï¼Œkernel å†…éƒ¨æŒ‰ expert id æ‹‰å–
* ç›¸é‚»ä¸“å®¶çŸ©é˜µå¸¸å¸¸èƒ½è¢« L2 å‘½ä¸­
* åŒä¸€ kernel å†… reuse æ›´é«˜

å®é™…æµ‹é‡ï¼šGrouped GEMM å¯ä»¥ä½¿ L2 hit rate æå‡ 20â€“50%ã€‚

---

# 5. ğŸŸ¦ dispatchï¼ˆtoken â†’ expertï¼‰çš„ç»“æœå¤©ç„¶é€‚åˆ batched layout

MoE å¿…é¡»æŠŠ token åˆ†é…åˆ° expertï¼š

* åš scatter â†’ buffer[M_expert][dim]
* ç„¶åæŠŠ buffer feed åˆ°ä¸“å®¶ FFN GEMM

Grouped GEMM ç›´æ¥è¯» â€œscatter åçš„ bufferâ€ çš„æŒ‡é’ˆæ•°ç»„ï¼Œå½¢æˆ **pointer-based batched GEMM**ã€‚

ç‹¬ç«‹ GEMM åˆ™éœ€è¦ï¼š

* åˆ†é…å¾ˆå¤šå° buffer
* å¯åŠ¨å¤§é‡å° GEMM
* æ¯ä¸ª GEMM çš„å¸¦å®½/è°ƒåº¦æµªè´¹æ›´æ˜æ˜¾

---

# 6. ğŸŸª æ··åˆé‡åŒ–ï¼ˆQ4_Kã€IQ4_NL ç­‰ï¼‰ä¸‹ Grouped GEMM æ›´å®¹æ˜“ fuse

ä½ åœ¨è°ƒ `iqk_moe_fused_up_gate`ã€`mul_mat_up_gate_NxM` æ—¶åº”è¯¥å·²ç»çœ‹åˆ°ï¼š
Grouped GEMM èƒ½æŠŠï¼š

* dequant
* matmul
* bias
* gate activation

èåˆæˆä¸€ä¸ª kernelã€‚

**å¤šä¸ªä¸“å®¶åˆ†åˆ«åšåˆ™æ— æ³•èåˆã€‚**

èåˆè¶Šæ·± â†’ å¸¦å®½è¶Šçœï¼ˆèŠ‚çœ 2â€“3Ã— DRAM æµé‡ï¼‰ â†’ æ€§èƒ½æ›´é«˜ã€‚

---

# 7. ğŸŸ« GPU è°ƒåº¦å‡å°‘ç¢ç‰‡åŒ–ï¼ˆé¿å… context switching + pipeline stallï¼‰

ç‹¬ç«‹ GEMM æƒ…å†µï¼š

* æ¯ä¸ªå° GEMM è¿è¡Œæ—¶é—´æçŸ­ï¼ˆå‡ å¾®ç§’ï¼‰
* GPU pipeline ç»å¸¸ IDLE
* è°ƒåº¦å™¨æ— æ³• overlap

Grouped GEMMï¼š

* ä¸€ä¸ª kernel è·‘å‡ ååˆ°å‡ ç™¾å¾®ç§’
* è°ƒåº¦å™¨å¯ä»¥å……åˆ† overlap MEM / compute
* æŒ‡ä»¤æµè¿ç»­æ— ä¸­æ–­

GPU æœ€æ€•å° kernelï¼š**Grouped å°±æ˜¯è®© GPU è·‘â€œå¤§æ´»å„¿â€ã€‚**

---

# 8. ä¾‹å­ï¼šä¸ºä»€ä¹ˆ tiny GEMM æ˜“è¾¾ä¸åˆ° tensor core å³°å€¼ï¼Ÿ

æ¯”å¦‚ä¸“å®¶ M=16ã€K=4096ã€N=11008
ç‹¬ç«‹ GEMMï¼š

* M=16 â†’ éå¸¸å° â†’ Tensor Core åªèƒ½ç”¨å°‘æ•° tile
* warp-level ä¸²è¡ŒåŒ–ä¸¥é‡
* occupancy åªæœ‰ 30%

Grouped GEMMï¼š

* æ¥è‡ª 40 ä¸ªä¸“å®¶ token å †å  â†’ M=16Ã—40=640
* è¶³å¤Ÿå¤§ â†’ å®Œå…¨åƒæ»¡ Tensor Core

é€šå¸¸æ€§èƒ½å·®è·ï¼š**3Ã—â€“10Ã—**ã€‚

---

# æ€»ç»“ï¼šGrouped GEMM ä¸ºä»€ä¹ˆæ›´å¿«ï¼Ÿ

| åŸå›                               | æ•ˆæœ                |
| ------------------------------- | ----------------- |
| å°†å¤šä¸ªä¸“å®¶çš„ tiny GEMM åˆå¹¶ â†’ å¤§ GEMM    | tensor core åˆ©ç”¨ç‡æš´æ¶¨ |
| SM occupancy ä» 20â€“40% â†’ 80â€“100% | æ›´é«˜å¹¶è¡Œåº¦             |
| kernel launch å‡å°‘æ•°ç™¾å€             | æ˜¾è‘—å‡å°‘ overhead     |
| L2 / cache reuse æ›´å¥½             | æƒé‡å¸¦å®½é™ä½            |
| èåˆ dequant + gemm + gate        | å†…å­˜æµé‡å‡å°‘            |
| å‡å°‘ GPU è°ƒåº¦ç¢ç‰‡                     | pipeline æ›´å¹³æ»‘      |

ä¸€å¥è¯æ€»ç»“ï¼š

> **Grouped GEMM è®© GPU ä»â€œè·‘ä¸€å †å°æ´»â€å˜æˆâ€œè·‘ä¸€ä»½å¤§æ´»â€ï¼Œä»è€Œæ— é™æ¥è¿‘ç†è®ºå³°å€¼ã€‚**

---

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥åŸºäºä½ æ­£åœ¨è°ƒçš„ï¼š

* `iqk_moe_fused_up_gate`
* `mul_mat_up_gate_NxM`

ç»™ä½ ç”»ä¸€ä¸ª **Grouped GEMM vs ç‹¬ç«‹ä¸“å®¶ GEMM çš„ kernel-level å¯¹æ¯”å›¾**ï¼ˆåŒ…æ‹¬ tile mappingã€è®¿å­˜ã€æŒ‡ä»¤æµï¼‰ã€‚


---

### ğŸ§‘â€ğŸ’» User

åœ¨deepseek-aiçš„deepgemmï¼ˆhttps://github.com/deepseek-ai/DeepGEMMï¼‰ä¸­ï¼Œè¿™å…·ä½“æ˜¯æ€ä¹ˆå®ç°top-kä¸ªä¸“å®¶çš„fused gemmçš„ï¼Ÿ

---

### ğŸ¤– Assistant

å¥½çš„ â€” è¿™æ˜¯æˆ‘æ ¹æ®å…¬å¼€èµ„æ–™ + æºç ï¼æ–‡æ¡£ï¼Œå¯¹ DeepGEMMï¼ˆæ¥è‡ª DeepSeek AIï¼‰é‡Œï¼Œ**top-k ä¸“å®¶ + fused / grouped GEMM** æ˜¯**å¦‚ä½•å®ç°**çš„æ¯”è¾ƒæ¸…æ™°è¯´æ˜ï¼æ¨æ–­ã€‚æ€»ä½“æ¥è¯´ï¼ŒDeepGEMM æä¾›äº†ä¸“é—¨ä¸º MoE åœºæ™¯è®¾è®¡çš„ â€œGrouped GEMMâ€ kernelï¼Œé€šè¿‡æŠŠå¤šä¸ªä¸“å®¶ (experts) çš„ token æ‰¹ (batch) åˆå¹¶ (concatenate) æˆ– ç”¨ maskï¼Œç»Ÿä¸€åšä¸€æ¬¡å¤§ GEMMï¼Œä»è€Œå®ç°å‰é¢æˆ‘ä»¬è®¨è®ºè¿‡çš„é«˜æ€§èƒ½ã€‚ä»¥ä¸‹æ˜¯å…³é”®æœºåˆ¶ & å®ç°ç»†èŠ‚ã€‚

---

## âœ… DeepGEMM æ”¯æŒ MoEï¼šGrouped GEMM çš„æ¥å£ä¸è®¾è®¡

* DeepGEMM æ˜ç¡®åœ¨ README ä¸­æŒ‡å‡ºï¼Œå®ƒé™¤äº†æ™®é€š dense GEMMï¼Œä¹Ÿæ”¯æŒ MoE çš„ â€œgroupedâ€ GEMMã€‚([GitHub][1])
* å¯¹äº MoE forward (token â†’ expert â†’ FFN) çš„åœºæ™¯ï¼Œå®ƒæä¾› **M-grouped GEMM**ï¼šå³ N, K ç»´å¯¹æ‰€æœ‰ä¸“å®¶å…±äº«ã€åªæœ‰ M ç»´ (ä»£è¡¨ token æ•° / batch size) éšä¸“å®¶å˜åŒ–ã€‚å…·ä½“å‡½æ•°åŒ…æ‹¬ï¼š `m_grouped_fp8_gemm_nt_contiguous`, å¯¹äº SM100 è¿˜æœ‰ `m_grouped_fp8_gemm_nn_contiguous`ã€‚([DeepWiki][2])
* å¯¹äº MoE æ¨ç† (inference) ä¸­ decode é˜¶æ®µ (token æ•° / expert assignment å¯èƒ½åŠ¨æ€ã€éš¾é¢„æµ‹)ï¼Œè¿˜æ”¯æŒ **masked grouped GEMM**ï¼š`m_grouped_fp8_gemm_nt_masked`ï¼Œç”¨ mask tensor æ¥è·³è¿‡é‚£äº›æ²¡æœ‰è¢«è·¯ç”± (no token assigned) çš„ä¸“å®¶ / slotsï¼Œä»è€Œé¿å…æ— æ„ä¹‰è®¡ç®—ã€‚([GitHub][1])
* å¦å¤–ï¼Œå¯¹äº MoE çš„æƒé‡æ¢¯åº¦ (weight-gradient) è®¡ç®— (é€šå¸¸ backward / training)ï¼Œå®ƒè¿˜æä¾› **K-grouped GEMM**ï¼š`k_grouped_fp8_gemm_tn_contiguous` (æˆ– nt depending on layout)ï¼Œé€‚åˆ K ç»´ (ç‰¹å¾ç»´) éšä¸“å®¶æˆ– shard å˜åŒ–çš„æƒ…å†µã€‚([DeepWiki][3])

æ‰€ä»¥ï¼š**æ¥å£å±‚é¢**ï¼ŒDeepGEMM å°±æ”¯æŒ â€œå°†å¤šä¸ªä¸“å®¶çš„å°çŸ©é˜µ (å° batch, small M) åˆå¹¶ (contiguous) æˆ–æŒ‰ mask è·³è¿‡ (masked)ï¼Œä¸€æ¬¡æ€§åšå¤§æ‰¹é‡ GEMM**ã€‚

---

## ğŸ”§ èƒŒåçš„å®ç°æœºåˆ¶ (kernel-level + memory/compute-layout) â€” å¦‚ä½• â€œfuse + grouped + high æ€§èƒ½â€

DeepGEMM å¹¶ä¸æ˜¯ç®€å•å¾ªç¯è°ƒç”¨å¤šä¸ªå° GEMMï¼Œè€Œæ˜¯é€šè¿‡ GPU kernel + layout design + JITä¼˜åŒ– + FP8 + hardware ç‰¹æ€§ç»„åˆï¼Œå®ç°é«˜æ€§èƒ½ fused grouped GEMMï¼š

* **Contiguous layout**: å¯¹äºå‰å‘ / prefill é˜¶æ®µ (è®­ç»ƒæˆ–æ¨ç†å‰ fill)ï¼Œå¤šä¸ªä¸“å®¶å¯¹ token çš„è¾“å‡º /ä¸­é—´ activation è¢« **æ‹¼æ¥ (concatenate)** åœ¨ä¸€èµ· (æ²¿ M è½´) æˆä¸ºä¸€ä¸ªå¤§çŸ©é˜µ A (shape: total_tokens Ã— K)ï¼Œç„¶åä¸ä¸“å®¶æƒé‡ B (K Ã— N) åšä¸€æ¬¡ GEMMã€‚è¿™æ · N, K å¯¹æ‰€æœ‰ä¸“å®¶ä¸€è‡´ï¼ŒM åªæ˜¯ â€œæ‰€æœ‰è¢«æ¿€æ´» token æ€»å’Œâ€ã€‚ DeepGEMM è¦æ±‚ â€œæ¯ä¸ªä¸“å®¶çš„ segment (group) åœ¨ M æ–¹å‘ä¸Šå¯¹é½åˆ° GEMM block sizeâ€ (é€šè¿‡ `get_mk_alignment_for_contiguous_layout()`)ã€‚([artivio.eu - Art Through Technology][4])
* **Masked layout**: å¯¹äº decode æ—¶ä¸“å®¶ assignment åŠ¨æ€ + batch å·®å¼‚å¤§ + æƒ³ç”¨ CUDA Graph çš„æƒ…å†µ (å¸Œæœ› kernel launch å›ºå®š)ï¼ŒDeepGEMM æä¾› masked grouped GEMMã€‚å³ä¼ å…¥ä¸€ä¸ª mask tensorï¼Œkernel å†…éƒ¨åªå¯¹ â€œvalid (è¢«è·¯ç”± / æœ‰ token)â€ expert-slot åšè®¡ç®—ï¼Œå…¶å®ƒè·³è¿‡ã€‚è¿™æ ·é¿å…äº†ä¸ºæ¯ä¸ªä¸“å®¶åˆ†åˆ« launch kernelï¼Œä¹Ÿé¿å…äº†å¯¹æ—  token çš„ä¸“å®¶åšæ— ç”¨ä¹˜æ³•ã€‚([GitHub][1])
* **FP8 + fine-grained scaling (quantization + dequantization)**: DeepGEMM ä½¿ç”¨ FP8 å¯¹è¾“å…¥ (activation + weight) åšä½ç²¾åº¦å­˜å‚¨ï¼è®¡ç®—ï¼Œä»¥å‡å°å†…å­˜å ç”¨å’Œå¸¦å®½å‹åŠ›ã€‚ä¸ºäº†å…¼é¡¾ç²¾åº¦ï¼Œå®ƒå®ç°äº† â€œç»†ç²’åº¦ scaling + ä¸¤çº§ç´¯åŠ  (promotion / accumulate in higher precision)â€ â€” ä¹Ÿå°±æ˜¯å³ä½¿ç”¨ FP8 åšä¹˜æ³•ï¼Œä¹Ÿç”¨æ›´é«˜ç²¾åº¦ (e.g. FP32 æˆ– BF16) åšç´¯åŠ /è¾“å‡ºï¼Œä»¥ä¿éšœæ•°å€¼ç¨³å®šæ€§ã€‚([æœç‹][5])
* **JIT å†…æ ¸ç”Ÿæˆ**: DeepGEMM ä¸æ˜¯ä¸€ä¸ªå†™æ­»æ‰€æœ‰å¯èƒ½ shape çš„æ¨¡æ¿åº“ã€‚å®ƒé€šè¿‡è½»é‡çº§ JIT (Just-In-Time) ç³»ç»Ÿï¼Œåœ¨ runtime æ ¹æ®å®é™…è¾“å…¥ (M, N, K, expert group counts, layout type) ç”Ÿæˆ kernelã€‚è¿™å…è®¸ kernel compile-time å¸¸é‡ (block size, thread config, TMA å‚æ•°ç­‰) ä¸å®é™… shape å¯¹é½ï¼Œä»è€Œæœ€å¤§åŒ–æ€§èƒ½ (å‡å°‘å¯„å­˜å™¨æµªè´¹ã€é™æ€ä¼˜åŒ–ã€ç¡¬ä»¶ç‰¹æ€§åˆ©ç”¨)ã€‚([DeepWiki][3])
* **åˆ©ç”¨ç¡¬ä»¶ / GPU ç‰¹æ€§ (å¦‚ TMA / Tensor Core / Hopper-specific optimizations)**: DeepGEMM é’ˆå¯¹ NVIDIA Hopper (SM90) / Blackwell (SM100) æ¶æ„åšä¼˜åŒ– â€” åˆ©ç”¨ TMA (Tensor Memory Accelerator) æ¥åŠ é€Ÿæ•°æ®åŠ è½½ï¼å­˜å‚¨ã€prefetchã€broadcast ç­‰ï¼›åˆ©ç”¨ tensor cores åšé«˜ååé‡ FP8 GEMMï¼›åˆ©ç”¨ layout + alignment + swizzling + L2 / cache reuseï¼Œå‡å°‘å†…å­˜å¸¦å®½å‹åŠ›ã€‚([DeepEP][6])

ç»¼åˆä»¥ä¸Šï¼ŒDeepGEMM çš„ â€œgrouped + fused MoE GEMMâ€ å¹¶éæ‹¼å‡‘å¤šä¸ªæ™®é€š GEMMï¼Œè€Œæ˜¯ä»æ•°æ® layout (contiguous / masked)ã€é‡åŒ– (FP8 + scaling / accumulate)ã€kernel generation (JIT)ã€ç¡¬ä»¶ç‰¹æ€§ (TMA / Tensor Core) å†åˆ° runtime è°ƒåº¦ï¼Œå¤šå±‚ä¼˜åŒ– â€” ä»æ ¹æœ¬ä¸ŠæŠŠ â€œmany small expert GEMMsâ€ è½¬åŒ–æˆ â€œone (or few) big, efficient GEMM(s) with minimal overhead + maximal throughputâ€ã€‚

---

## ğŸ¯ åœ¨ top-k + MoE åœºæ™¯ä¸­æ˜¯æ€æ · â€œç”¨èµ·æ¥â€çš„

ç»“åˆå¸¸è§ MoE æ¨ç† /è®­ç»ƒæµç¨‹ (å³ top-k è·¯ç”± + å¤šä¸“å®¶ forward):

1. å¯¹æ¯ä¸ª tokenï¼Œé€šè¿‡ router å¾—åˆ°å®ƒå¯¹åº”çš„ top-k experts (topk_ids) + æƒé‡ (topk_weights)
2. å°†å„ token çš„ embedding / hidden_state æŒ‰ expert grouping + token assignment åš scatter / bucket â†’ å¾—åˆ° per-expert â€œmini-batchâ€ of tokens.
3. **Contiguous layout path** (prefill /è®­ç»ƒ / é decode): æŠŠæ‰€æœ‰ä¸“å®¶ token åˆå¹¶ (concatenate) â†’ large tensor A; åŒæ—¶å°†å¯¹åº”ä¸“å®¶æƒé‡ (å¯èƒ½æ˜¯ per-expert weight matrices) å¯¹åº” B çŸ©é˜µæ•´ç† (å¯¹äº experts share same shape, B shape ç›¸åŒ) â†’ è°ƒç”¨ `m_grouped_fp8_gemm_nt_contiguous(A, B, ...)`ã€‚
4. **Masked layout path** (decode / dynamic assignment): ä¸åš concatï¼Œè€Œæ˜¯ä¿ç•™ä¸€ä¸ª buffer + maskï¼Œè°ƒç”¨ `m_grouped_fp8_gemm_nt_masked(...)`ï¼Œkernel å†…éƒ¨æ ¹æ® mask åªåšæœ‰ token çš„ä¸“å®¶ slot.
5. è¾“å‡ºæ‹¼å› (gather / reshape) æˆ per-token per-expert outputsï¼Œå†åŠ æƒ / sum / gate / activation â†’ å®Œæˆ MoE å±‚ forwardã€‚

å¯¹äº training backward / weight update å¦‚æœ K ç»´ (ç‰¹å¾ç»´) æœ‰åˆ†ç‰‡ / varyingï¼Œä½¿ç”¨ K-grouped GEMMï¼Œé€šè¿‡ `k_grouped_fp8_gemm_tn_contiguous` ç­‰æ¥å£å¯¹å¤šä¸ªä¸“å®¶çš„æ¢¯åº¦åš batched updateã€‚

æ–‡æ¡£ /é›†æˆ (ä¾‹å¦‚åœ¨ vLLM) å°±æ˜¯é€šè¿‡ DeepGEMM çš„è¿™äº› grouped-GEMM æ¥å£ (contiguous/masked) æ¥å®ç° MoE layer çš„ fused GEMM + activation + routing +é‡åŒ–ã€‚([vLLM][7])

---

## âœ… ä¼˜åŠ¿ & è®¾è®¡æƒè¡¡ (ä¸ºä»€ä¹ˆè¿™ç§å®ç°æ–¹å¼å¯¹ top-k MoE ç‰¹åˆ«åˆé€‚)

* å‡å°‘ kernel-launch overheadï¼šä¸ç”¨ä¸ºæ¯ä¸ªä¸“å®¶ / æ¯ä¸ª token æ‰¹åˆ†åˆ« launch GEMMã€‚
* é«˜ç¡¬ä»¶åˆ©ç”¨ç‡ï¼šå› ä¸ºæ˜¯å¤§çŸ©é˜µ â†’ tensor core / TMA / cache / L2 / shared memory éƒ½èƒ½å……åˆ†åˆ©ç”¨ã€‚
* åŠ¨æ€é€‚åº” top-k è·¯ç”±å˜åŠ¨ (masked) â†’ decode é˜¶æ®µä¹Ÿé«˜æ•ˆã€‚
* æ”¯æŒé‡åŒ– (FP8) + scaling + accumulate â†’ memory / bandwidth èŠ‚çœ + æ•°å€¼ç¨³å®šã€‚
* JIT + layout + alignment â†’ å¯¹ä¸åŒ shape / expert count / token count éƒ½è‡ªåŠ¨é€‚é… (å¯æ’æ‹”)ã€‚

---

## âš ï¸ é™åˆ¶ / è¦æ³¨æ„çš„åœ°æ–¹

* æ‰€æœ‰ä¸“å®¶å¿…é¡»â€œå…±äº« shape (N, K)â€ â€”â€” è¿™æ˜¯ grouped GEMM çš„å‰æ (å¦åˆ™ N/K ä¸ä¸€è‡´å°±ä¸èƒ½åˆå¹¶ B çŸ©é˜µ)ã€‚DeepGEMM README æ˜ç¡®è¿™æ ·è®¾è®¡ã€‚([artivio.eu - Art Through Technology][4])
* å¯¹äº contiguous æ¨¡å¼ï¼Œæ¯ä¸ªä¸“å®¶åˆ†é…åˆ°çš„ token æ•° (group size) å¿…é¡»å¯¹é½åˆ° GEMM block å¤§å° (alignment constraint)ï¼Œéœ€è¦ä½¿ç”¨ `get_mk_alignment_for_contiguous_layout()`ã€‚([artivio.eu - Art Through Technology][4])
* FP8 + fine-grained scaling + accumulate è™½ç„¶å…¼é¡¾é€Ÿåº¦ä¸ç²¾åº¦ï¼Œä½†é‡åŒ– + dequantization + scaling overhead +å¯èƒ½æ•°å€¼è¯¯å·®ï¼Œéœ€è¦è°¨æ…éªŒè¯ (å°¤å…¶è®­ç»ƒåœºæ™¯) â€” DeepGEMM æœ¬èº«é€šè¿‡ â€œä¸¤çº§ç´¯åŠ  (promotion)â€ æ¥å‡å°è¯¯å·®ã€‚([MarkTechPost][8])

---

## ğŸ§ª æ€»ç»“

DeepGEMM ç”¨ **M-grouped (contiguous / masked) + FP8 + JIT + hardware-native GEMM kernel** çš„ç»„åˆï¼Œå®ç°äº†å¯¹ top-k expert MoE çš„ **fused GEMM** â€” æŠŠå¤šä¸“å®¶ / å¤š token çš„åˆ†æ•£è®¡ç®—ï¼Œæ±‡èšä¸ºä¸€æ¬¡ (æˆ–å°‘é‡) é«˜æ•ˆçŸ©é˜µä¹˜æ³•ã€‚è¿™æ ·æ—¢ä¿ç•™äº† MoE çš„ sparsity + routing + per-expert specializationï¼Œä¹Ÿæå¤§æå‡äº† GPU utilization, memory / bandwidth efficiency, å¹¶é™ä½ overheadã€‚

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å¸®ä½  **è¯»ä¸€ä¸‹ DeepGEMM æºç  (fp8_gemm.cuh + groupedç›¸å…³éƒ¨åˆ†)**ï¼Œåšä¸€ä¸ª **ä¼ªä»£ç  + kernel flowï¼ˆå« memory layout / thread block / warp mapping / quantize-dequantize + accumulateï¼‰** çš„è¯¦ç»†è‰å›¾ã€‚è¿™æ ·ä½ å¯ä»¥éå¸¸ç›´è§‚çœ‹åˆ°å®ƒ â€œæ€ä¹ˆ fuse top-k experts + GEMMâ€ çš„ã€‚ä½ è¦å—ï¼Ÿ

[1]: https://github.com/deepseek-ai/DeepGEMM?utm_source=chatgpt.com "GitHub - deepseek-ai/DeepGEMM: DeepGEMM: clean and efficient FP8 GEMM kernels with fine-grained scaling"
[2]: https://deepwiki.com/deepseek-ai/DeepGEMM/1.1-getting-started?utm_source=chatgpt.com "Getting Started | deepseek-ai/DeepGEMM | DeepWiki"
[3]: https://deepwiki.com/deepseek-ai/DeepGEMM?utm_source=chatgpt.com "deepseek-ai/DeepGEMM | DeepWiki"
[4]: https://artivio.eu/2025/02/26/deepseek-open-sources-deepgemm-clean-and-efficient-fp8-gemm-kernels/?utm_source=chatgpt.com "DeepSeek Open Sources DeepGEMM: Clean and efficient FP8 GEMM kernels - artivio.eu"
[5]: https://www.sohu.com/a/863998204_122014422?utm_source=chatgpt.com "DeepSeekæ¨å‡ºé«˜æ•ˆå¼€æºDeepGEMMåº“ï¼Œæ”¯æŒMoEåˆ†ç»„è¿ç®—_ä»£ç _æ ¸å¿ƒ_è®¾è®¡"
[6]: https://www.deepep.org/en/deepgemm?utm_source=chatgpt.com "DeepGEMM - Efficient FP8 Matrix Multiplication Library"
[7]: https://docs.vllm.ai/api/vllm/model_executor/layers/fused_moe/deep_gemm_moe.html?utm_source=chatgpt.com "deep_gemm_moe - vLLM"
[8]: https://www.marktechpost.com/2025/02/25/deepseek-ai-releases-deepgemm-an-fp8-gemm-library-that-supports-both-dense-and-moe-gemms-powering-v3-r1-training-and-inference/?utm_source=chatgpt.com "DeepSeek AI Releases DeepGEMM: An FP8 GEMM Library that Supports both Dense and MoE GEMMs Powering V3/R1 Training and Inference - MarkTechPost"
