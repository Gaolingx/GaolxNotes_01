# DeepGEMM Top-k GEMMå®ç°

### ğŸ§‘â€ğŸ’» User

[deepseek-ai-deepgemm-8a5edab282632443.txt](./deepseek-ai-deepgemm-8a5edab282632443.txt)

åœ¨deepseek-aiçš„deepgemmï¼ˆhttps://github.com/deepseek-ai/DeepGEMMï¼‰ä¸­ï¼Œè¿™å…·ä½“æ˜¯æ€ä¹ˆå®ç°top-kä¸ªä¸“å®¶çš„fused gemmçš„ï¼Ÿ

---

### ğŸ¤– Assistant

å¥½çš„ï¼Œæˆ‘ä»¬æ¥æ·±å…¥åˆ†æ DeepGEMM ä¸­ä¸º Top-K ä¸“å®¶ï¼ˆMoE æ¨¡å‹ï¼‰è®¾è®¡çš„ Fused GEMM æ˜¯å¦‚ä½•å®ç°çš„ã€‚

å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†å¤šä¸ªå°å‹çš„ GEMM æ“ä½œèåˆæˆä¸€ä¸ªå¤§çš„ã€å•ä¸€çš„ CUDA Kernel æ¥æ‰§è¡Œï¼Œä»è€Œé¿å…å¤šæ¬¡ Kernel å¯åŠ¨çš„å¼€é”€ï¼Œå¹¶æœ€å¤§åŒ– GPU çš„åˆ©ç”¨ç‡ã€‚è¿™ä¸»è¦æ˜¯é€šè¿‡ä¸€ç§ç§°ä¸º **M-Grouped Contiguous GEMM** çš„ç­–ç•¥æ¥å®ç°çš„ã€‚

### 1. æ ¸å¿ƒæ¦‚å¿µï¼šM-Grouped Contiguous GEMM

ä¼ ç»Ÿçš„ Grouped GEMM (ä¾‹å¦‚ CUTLASS ä¸­çš„) é€šå¸¸å¤„ç†ä¸€æ‰¹å½¢çŠ¶å„å¼‚çš„çŸ©é˜µä¹˜æ³•ã€‚ç„¶è€Œï¼Œåœ¨ MoE æ¨¡å‹ä¸­ï¼Œæ‰€æœ‰ä¸“å®¶çš„æƒé‡çŸ©é˜µå½¢çŠ¶é€šå¸¸æ˜¯ç›¸åŒçš„ã€‚DeepGEMM åˆ©ç”¨äº†è¿™ä¸€ç‰¹æ€§ã€‚

å®ƒçš„ç­–ç•¥ä¸æ˜¯ä¸ºæ¯ä¸ªä¸“å®¶å¯åŠ¨ä¸€ä¸ªç‹¬ç«‹çš„ GEMMï¼Œä¹Ÿä¸æ˜¯ä½¿ç”¨ä¼ ç»Ÿçš„ Batched GEMMï¼Œè€Œæ˜¯ï¼š

1.  **æ•°æ®é‡æ’ (Data Layout)**ï¼š
    *   **è¾“å…¥æ¿€æ´» (Activation) `A`**ï¼šå°†æ‰€æœ‰è¢«è·¯ç”±åˆ°è¿™ `k` ä¸ªä¸“å®¶çš„ tokens çš„æ¿€æ´»å‘é‡ï¼ŒæŒ‰ç…§ä¸“å®¶åˆ†ç»„ã€è¿ç»­æ’åˆ—ï¼Œå½¢æˆä¸€ä¸ªå¤§çš„ `A` çŸ©é˜µã€‚å…¶å½¢çŠ¶ä¸º `[M_total, K]`ï¼Œå…¶ä¸­ `M_total` æ˜¯æ‰€æœ‰ `k` ä¸ªä¸“å®¶è¦å¤„ç†çš„ token æ€»æ•°ã€‚
    *   **ä¸“å®¶æƒé‡ (Weights) `B`**ï¼šå°† `k` ä¸ªä¸“å®¶çš„æƒé‡çŸ©é˜µå †å ï¼ˆstackï¼‰èµ·æ¥ï¼Œå½¢æˆä¸€ä¸ª `B` å¼ é‡ã€‚å…¶å½¢çŠ¶ä¸º `[num_experts, N, K]`ã€‚
    *   **è¾“å‡º (Output) `D`**ï¼šè¾“å‡ºå¼ é‡çš„å½¢çŠ¶ä¸è¾“å…¥ `A` ç±»ä¼¼ï¼Œä¸º `[M_total, N]`ã€‚

2.  **è·¯ç”±ä¿¡æ¯ (`grouped_layout`)**ï¼šä¸ºäº†åœ¨èåˆçš„ Kernel ä¸­çŸ¥é“å“ªä¸ª token åº”è¯¥ä½¿ç”¨å“ªä¸ªä¸“å®¶çš„æƒé‡ï¼Œéœ€è¦ä¸€ä¸ªé¢å¤–çš„ç´¢å¼•å¼ é‡ï¼Œåœ¨ DeepGEMM ä¸­è¢«ç§°ä¸º `grouped_layout`ã€‚
    *   è¿™æ˜¯ä¸€ä¸ªä¸€ç»´æ•´æ•°å¼ é‡ï¼Œå½¢çŠ¶ä¸º `[M_total]`ã€‚
    *   `grouped_layout[i]` çš„å€¼ä»£è¡¨ç¬¬ `i` ä¸ª token åº”è¯¥ä½¿ç”¨çš„ä¸“å®¶ç´¢å¼•ï¼ˆexpert indexï¼‰ã€‚

3.  **å…³é”®çº¦æŸ**ï¼šä¸ºäº†å®ç°é«˜æ•ˆçš„å†…å­˜è®¿é—®å’Œè®¡ç®—ï¼ŒDeepGEMM æ–½åŠ äº†ä¸€ä¸ªé‡è¦çš„çº¦æŸï¼š**æ‰€æœ‰åœ¨åŒä¸€ä¸ªè®¡ç®—å—ï¼ˆ`BLOCK_M`ï¼‰å†…çš„ tokens å¿…é¡»è¢«è·¯ç”±åˆ°åŒä¸€ä¸ªä¸“å®¶**ã€‚è¿™æ„å‘³ç€åœ¨å°† tokens é€å…¥ DeepGEMM ä¹‹å‰ï¼Œéœ€è¦å¯¹å®ƒä»¬è¿›è¡Œæ’åºå’Œå¡«å……ï¼ˆpaddingï¼‰ï¼Œä»¥ç¡®ä¿æ¯ä¸ªä¸“å®¶çš„ token æ•°é‡æ˜¯ `BLOCK_M` çš„æ•´æ•°å€ã€‚è¿™ä¸ªå¯¹é½å¤§å°å¯ä»¥é€šè¿‡ `deep_gemm.get_mk_alignment_for_contiguous_layout()` è·å–ã€‚

### 2. ä»£ç å®ç°è¿½è¸ª

ä¸‹é¢æˆ‘ä»¬ä» Python API åˆ° CUDA Kernel å±‚é¢ï¼Œè¿½è¸ªè¿™ä¸€å®ç°ã€‚

#### æ­¥éª¤ 1ï¼šPython API å’Œ C++ ç»‘å®š

ç”¨æˆ·ä» Python ä¸­è°ƒç”¨ç±»ä¼¼ `m_grouped_fp8_gemm_nt_contiguous` çš„å‡½æ•°ã€‚
`deep_gemm/__init__.py`:
```python
from ._C import (
    # ...
    m_grouped_fp8_gemm_nt_contiguous,
    # ...
)
```
è¿™ä¸ªè°ƒç”¨ä¼šè¿›å…¥ `csrc/python_api.cpp` ä¸­æ³¨å†Œçš„ C++ å‡½æ•°ï¼Œæœ€ç»ˆè°ƒç”¨åˆ° `csrc/apis/gemm.hpp` ä¸­çš„ `m_grouped_fp8_fp4_gemm_nt_contiguous` å‡½æ•°ã€‚

`csrc/apis/gemm.hpp`:
```cpp
static void m_grouped_fp8_fp4_gemm_nt_contiguous(
    const std::pair<torch::Tensor, torch::Tensor>& a,
    const std::pair<torch::Tensor, torch::Tensor>& b,
    const torch::Tensor& d,
    const torch::Tensor& grouped_layout, // æ ¸å¿ƒï¼šè·¯ç”±ä¿¡æ¯
    // ...
) {
    // å½¢çŠ¶æ£€æŸ¥: A æ˜¯ [M, K], B æ˜¯ [G, N, K], d æ˜¯ [M, N]
    // G (num_groups) æ˜¯ä¸“å®¶æ•°é‡
    const auto [m , k ] = check_ab_fp8_fp4(a.first, ...);
    const auto [num_groups, n, k_] = check_grouped_ab_fp8_fp4(b.first, ...);
    const auto [m_, n_] = get_shape<2>(d);

    // grouped_layout æ£€æŸ¥
    // ...
    const auto& [m__] = get_shape<1>(grouped_layout);
    DG_HOST_ASSERT(m == m__); // ç¡®è®¤ grouped_layout çš„é•¿åº¦ç­‰äº token æ€»æ•° M

    // ... çœç•¥äº†ç¼©æ”¾å› å­(scaling factor)å’Œæ¶æ„ç›¸å…³çš„å‡†å¤‡ä»£ç 

    // åˆ†å‘åˆ°å…·ä½“æ¶æ„çš„å®ç°
    if (arch_major == 9 ...) {
        sm90_m_grouped_fp8_gemm_contiguous_1d2d(a.first, sfa, b.first, sfb, d, grouped_layout, ...);
    } else if (arch_major == 10 ...) {
        sm100_m_grouped_fp8_fp4_gemm_contiguous_1d1d(a.first, sfa, b.first, sfb, d, grouped_layout, ...);
    }
}
```
è¿™ä¸ª C++ å‡½æ•°ä¸»è¦åšå½¢çŠ¶æ£€æŸ¥ã€å‡†å¤‡æ•°æ®ï¼Œç„¶åæ ¹æ® GPU æ¶æ„ï¼ˆSM90 æˆ– SM100ï¼‰è°ƒç”¨æ›´åº•å±‚çš„å®ç°ã€‚

#### æ­¥éª¤ 2ï¼šKernel å¯åŠ¨å’Œ JIT ç¼–è¯‘

æˆ‘ä»¬ä»¥ SM100 (Hopper æ¶æ„) ä¸ºä¾‹ï¼Œè¿›å…¥ `csrc/jit_kernels/impls/sm100_bf16_gemm.hpp` (FP8çš„é€»è¾‘ç±»ä¼¼)ã€‚

```cpp
static void sm100_m_grouped_bf16_gemm_contiguous(
    // ...
    const torch::Tensor& grouped_layout,
    // ...
) {
    // 1. ä½¿ç”¨å¯å‘å¼æ¨¡å‹é€‰æ‹©æœ€ä½³é…ç½® (å—å¤§å°, æµæ°´çº¿é˜¶æ®µæ•°ç­‰)
    const auto& config = get_best_config<SM100ArchSpec>(
        GemmType::MGroupedContiguous, // << æŒ‡å®šGEMMç±»å‹
        ...);

    // 2. åˆ›å»ºTMAæè¿°ç¬¦ï¼Œç”¨äºç¡¬ä»¶åŠ é€Ÿçš„å†…å­˜æ‹·è´
    const auto& tensor_map_a = make_tma_a_desc(...);
    const auto& tensor_map_b = make_tma_b_desc(..., num_groups, ...); // bçš„æè¿°ç¬¦ä¼šè€ƒè™‘ä¸“å®¶æ•°é‡
    const auto& tensor_map_cd = make_tma_cd_desc(...);

    // 3. å‡†å¤‡å¯åŠ¨å‚æ•°
    const SM100BF16GemmRuntime::Args& args = {
        // ...
        .gemm_config = config,
        .grouped_layout = grouped_layout.data_ptr(), // ä¼ é€’ grouped_layout çš„æŒ‡é’ˆ
        // ...
    };

    // 4. JITç¼–è¯‘å¹¶å¯åŠ¨Kernel
    const auto& code = SM100BF16GemmRuntime::generate(args);
    const auto& runtime = compiler->build("sm100_bf16_m_grouped_gemm_contiguous", code);
    SM100BF16GemmRuntime::launch(runtime, args);
}
```
è¿™ä¸€å±‚æ˜¯â€œä¸»æœºç«¯(Host)â€ä»£ç ï¼Œå®ƒä¸æ‰§è¡Œ GEMM è®¡ç®—ï¼Œè€Œæ˜¯é…ç½®ã€ç¼–è¯‘å¹¶å¯åŠ¨å°†åœ¨ GPU ä¸Šè¿è¡Œçš„ CUDA Kernelã€‚

#### æ­¥éª¤ 3ï¼šCUDA Kernel å†…éƒ¨å®ç°

çœŸæ­£çš„é­”æ³•å‘ç”Ÿåœ¨ CUDA Kernel å†…éƒ¨ï¼Œä¾‹å¦‚ `deep_gemm/include/deep_gemm/impls/sm100_bf16_gemm.cuh` ä¸­çš„ `sm100_bf16_gemm_impl`ã€‚

è¿™ä¸ª Kernel çš„æ ¸å¿ƒæ˜¯ `Scheduler` ç±» (`deep_gemm/include/deep_gemm/common/scheduler.cuh`)ï¼Œå®ƒè´Ÿè´£ä¸ºæ¯ä¸ª CUDA å—ï¼ˆCTAï¼‰åˆ†é…å·¥ä½œã€‚

```cpp
// deep_gemm/include/deep_gemm/common/scheduler.cuh (æ¦‚å¿µæ€§ç®€åŒ–)

template <GemmType kGemmType, ...>
struct Scheduler {
    // ...
    int* grouped_layout;

    __device__ __forceinline__ explicit Scheduler(..., int* grouped_layout = nullptr) {
        // ...
        if constexpr (kGemmType == GemmType::MGroupedContiguous) {
            this->grouped_layout = grouped_layout;
        }
    }

    template <bool kWithGroupOffset, ...>
    __device__ __forceinline__ uint32_t get_global_idx(...) {
        if constexpr (kGemmType == GemmType::MGroupedContiguous) {
            // å¦‚æœéœ€è¦ç»„åç§» (ä¾‹å¦‚ï¼Œè®¿é—®æƒé‡Bæ—¶)
            if (kWithGroupOffset) {
                // è¯»å– grouped_layout å¼ é‡æ¥è·å–ä¸“å®¶ç´¢å¼•
                // __ldg æ˜¯åªè¯»ç¼“å­˜åŠ è½½æŒ‡ä»¤ (load via texture cache)
                // m_block_idx * BLOCK_M å®šä½åˆ°å½“å‰tokenå—çš„èµ·å§‹ä½ç½®
                const auto offset = cute::max(0, __ldg(grouped_layout + m_block_idx * BLOCK_M));
                // è¿”å›: expert_idx * N + n_offset
                return offset * shape_dim + block_idx * block_size;
            } else {
                 return block_idx * block_size;
            }
        }
        // ... å…¶ä»– GEMM ç±»å‹çš„é€»è¾‘
    }
};
```

**å·¥ä½œæµè§£é‡Š**:

1.  **Kernel å¯åŠ¨**: GPU å¯åŠ¨ä¸€ä¸ªå¤§çš„ Gridï¼ŒåŒ…å«è®¸å¤š CUDA å— (CTAs)ã€‚
2.  **ä»»åŠ¡åˆ†é…**: æ¯ä¸ª CTA è¿›å…¥ä¸€ä¸ª `while` å¾ªç¯ï¼Œé€šè¿‡ `scheduler.get_next_block(m_block_idx, n_block_idx)` è·å–å®ƒè¦å¤„ç†çš„ `D` çŸ©é˜µçš„å—åæ ‡ã€‚`m_block_idx` å¯¹åº”äº `A` çŸ©é˜µçš„è¡Œå—ï¼Œ`n_block_idx` å¯¹åº”äº `B` çŸ©é˜µçš„åˆ—å—ã€‚
3.  **åŠ è½½æƒé‡ (Fused çš„å…³é”®)**:
    *   å½“ä¸€ä¸ª CTA éœ€è¦åŠ è½½å…¶å¯¹åº”çš„ `B` çŸ©é˜µï¼ˆä¸“å®¶æƒé‡ï¼‰çš„ç“¦ç‰‡ï¼ˆtileï¼‰æ—¶ï¼Œå®ƒä¼šè°ƒç”¨ `scheduler.get_global_idx` æ¥è®¡ç®—å†…å­˜åœ°å€ã€‚
    *   `get_global_idx` å‡½æ•°ä¼šæ‰§è¡Œ `__ldg(grouped_layout + m_block_idx * BLOCK_M)`ã€‚å› ä¸ºä¹‹å‰å·²ç»ä¿è¯äº†ä¸€ä¸ª `BLOCK_M` å†…çš„æ‰€æœ‰ tokens å±äºåŒä¸€ä¸ªä¸“å®¶ï¼Œæ‰€ä»¥åªéœ€è¦è¯»å–è¿™ä¸ªå—çš„ç¬¬ä¸€ä¸ª token å¯¹åº”çš„ä¸“å®¶ç´¢å¼•å³å¯ã€‚
    *   è¿™ä¸ªç´¢å¼•ï¼ˆ`offset`ï¼‰è¢«ç”¨æ¥è®¡ç®— `B` å¼ é‡ `[num_experts, N, K]` çš„åç§»é‡ï¼Œç¡®ä¿ CTA ä»æ­£ç¡®çš„ä¸“å®¶æƒé‡ä¸­åŠ è½½æ•°æ®ã€‚
    *   ä¾‹å¦‚ï¼Œè¦åŠ è½½ç¬¬ `i` ä¸ªä¸“å®¶çš„æƒé‡ï¼Œåœ°å€åç§»å°±æ˜¯ `i * N * K`ã€‚
4.  **åŠ è½½æ¿€æ´»**: åŠ è½½ `A` çŸ©é˜µçš„ç“¦ç‰‡åˆ™æ¯”è¾ƒç›´æ¥ï¼Œä»…ä½¿ç”¨ `m_block_idx` è®¡ç®—è¡Œåç§»ã€‚
5.  **è®¡ç®—ä¸å­˜å‚¨**: CTA åŠ è½½äº†æ­£ç¡®çš„ `A` ç“¦ç‰‡å’Œ `B` ç“¦ç‰‡åï¼Œæ‰§è¡ŒçŸ©é˜µä¹˜æ³•ï¼Œå¹¶å°†ç»“æœå†™å› `D` çŸ©é˜µçš„ç›¸åº”ä½ç½®ã€‚

### æ€»ç»“

DeepGEMM å®ç° Top-K ä¸“å®¶ Fused GEMM çš„æ–¹æ³•å¯ä»¥æ¦‚æ‹¬ä¸ºï¼š

-   **æ•°æ®é¢„å¤„ç†**: å°† tokens æŒ‰ä¸“å®¶æ’åºï¼Œèšåˆæˆä¸€ä¸ªå¤§çš„æ¿€æ´»çŸ©é˜µ `A`ã€‚
-   **å•ä¸€ Kernel å¯åŠ¨**: å¯åŠ¨ä¸€ä¸ªå•ä¸€çš„ã€å¤§è§„æ¨¡çš„ M-Grouped GEMM Kernelï¼Œè€Œä¸æ˜¯å¤šä¸ªå° Kernelã€‚
-   **æ™ºèƒ½ç´¢å¼•**: åœ¨ Kernel å†…éƒ¨ï¼Œé€šè¿‡ä¸€ä¸ª `grouped_layout` è¾…åŠ©å¼ é‡ï¼Œåœ¨è¿è¡Œæ—¶åŠ¨æ€åœ°ä¸ºæ¯ä¸ªè®¡ç®—å—ï¼ˆCTAï¼‰é€‰æ‹©æ­£ç¡®çš„ä¸“å®¶æƒé‡çŸ©é˜µã€‚
-   **ç¡¬ä»¶åˆ©ç”¨**: è¿™ç§æ–¹å¼å°†å¤§é‡å¹¶è¡Œçš„è®¡ç®—ä»»åŠ¡æ‰“åŒ…åˆ°ä¸€ä¸ª Kernel ä¸­ï¼Œé€šè¿‡ `Scheduler` åœ¨ SMï¼ˆStreaming Multiprocessorï¼‰ä¹‹é—´é«˜æ•ˆåˆ†å‘ï¼Œå……åˆ†åˆ©ç”¨äº† GPU çš„å¹¶è¡Œè®¡ç®—èƒ½åŠ›ï¼Œå¹¶å‡å°‘äº†å¯åŠ¨å¼€é”€ã€‚

è¿™ç§è®¾è®¡å·§å¦™åœ°å°†åœ¨ MoE è·¯ç”±ä¸­äº§ç”Ÿçš„ã€ç¦»æ•£çš„ã€å°è§„æ¨¡çš„è®¡ç®—ä»»åŠ¡ï¼Œè½¬æ¢æˆäº†é€‚åˆ GPU å¤§è§„æ¨¡å¹¶è¡Œæ¶æ„çš„ã€è¿ç»­çš„ã€å¤§è§„æ¨¡çš„è®¡ç®—ä»»åŠ¡ã€‚