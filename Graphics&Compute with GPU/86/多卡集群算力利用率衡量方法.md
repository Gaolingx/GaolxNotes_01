# 多卡集群算力利用率衡量方法

### 🧑‍💻 User

如何衡量多卡集群的算力利用率，用什么性能指标？

---

### 🤖 Assistant

当然，衡量多卡集群（尤其是用于深度学习的GPU集群）的算力利用率是一个多维度的问题，不能仅靠单一指标。一个高效的集群运维和优化策略需要结合多个性能指标来全面评估。

核心思想是：**既要看硬件的“忙碌”程度，也要看它“有效产出”的多少。**

以下是衡量多卡集群算力利用率的关键性能指标，可以分为**硬件层面**和**模型/任务层面**两大类。

---

### 一、 硬件层面指标

这类指标直接从GPU硬件计数器获取，反映了GPU物理资源的占用情况。这是最基础也是最重要的指标。

1.  **GPU利用率**
    *   **指标**： `GPU-Util` (通过 `nvidia-smi` 命令查看)
    *   **含义**： 表示过去采样周期（通常是1秒）内，GPU上有一个或多个内核（Kernel）执行时间的百分比。可以粗略理解为GPU的“忙碌”程度。
    *   **优点**： 易于获取，是快速诊断的“第一眼”指标。
    *   **缺点**： **具有误导性**。一个内核可能只占用了GPU的一部分计算单元（如只有部分SM在工作），但`GPU-Util`仍然显示为100%。它只关心“是否在忙”，而不关心“有多忙”。

2.  **算力利用率**
    *   **指标**： **FP32, FP64, FP16/TF32, INT8 等计算精度下的利用率** (通过 `nvprof`, `Nsight Systems`, `DCGM` 等工具查看)
    *   **含义**： 衡量GPU的核心计算单元（如CUDA Cores/Tensor Cores）实际用于执行计算的时间比例。这是比`GPU-Util`更精确的指标。
    *   **解读**：
        *   **理想状态**： 在运行计算密集型算子（如大矩阵乘法）时，该指标应持续接近100%。
        *   **偏低原因**： 通常意味着存在**瓶颈**，例如：
            *   **数据I/O瓶颈**： CPU数据预处理慢、数据加载慢、PCIe传输慢。
            *   **内核 launch 开销**： 过多的小型内核操作。
            *   **同步开销**： 过多的CPU-GPU同步或GPU间同步。

3.  **显存利用率**
    *   **指标**： `Memory Usage` (通过 `nvidia-smi` 查看)
    *   **含义**： GPU显存的使用量（通常为 MB/GB）。
    *   **重要性**： 显存容量是硬性限制。如果模型或数据太大，无法放入显存，则根本无法运行。即使算力利用率很高，但如果显存只用了一小部分，可能意味着**Batch Size设置过小**，没有充分利用显存资源来提升计算效率。

4.  **显存带宽利用率**
    *   **指标**： **Memory Read/Write Throughput** (通过 `nvidia-smi -l 1` 或 `DCGM` 查看)
    *   **含义**： 衡量GPU读写显存的速度（通常为 GB/s）。可以与GPU的理论显存带宽（如A100为1555 GB/s）对比得到利用率。
    *   **解读**： 对于**内存密集型**任务（如深度学习中的Embedding查找、某些预处理），该指标可能先于算力利用率达到瓶颈。

5.  **PCIe带宽利用率**
    *   **含义**： 衡量GPU与CPU之间数据交换的速率。对于需要频繁与CPU交换数据的任务（如小Batch Size训练），PCIe带宽可能成为瓶颈，导致GPU等待数据而空闲。

---

### 二、 模型/任务层面指标

这类指标从业务出发，衡量集群完成有效工作的效率，更能反映“性价比”。

1.  **吞吐量**
    *   **指标**： **Samples / Second** 或 **Tokens / Second** (训练/推理)
    *   **含义**： 单位时间内处理的数据量。这是**最直接、最重要的业务指标**。
    *   **用法**： 在相同的硬件和模型配置下，吞吐量越高，代表算力利用效率越高。优化所有硬件指标的最终目的就是为了提升吞吐量。

2.  **效率指标**
    *   **模型FLOPs利用率**
        *   **计算方式**： `实际吞吐量 * 模型理论FLOPs / (GPU数量 * 单GPU峰值FLOPs)`
        *   **含义**： 衡量你的代码效率有多接近硬件的理论峰值算力。这是衡量算力利用率的**黄金标准**。
        *   **例如**： 你的模型一次前向+后向传播需要100 TFLOPs，集群有8张峰值算力为100 TFLOPS的GPU，理论峰值是800 TFLOPS。如果你的吞吐量是每秒5个样本，那么实际算力是 `5 * 100 = 500 TFLOPS`，MFU为 `500 / 800 = 62.5%`。
    *   **HFU**
        *   **含义**： 由于现代GPU有专为AI设计的Tensor Cores，其峰值算力远高于标准CUDA Cores。HFU使用Tensor Cores的峰值算力作为分母，是比MFU更严格的指标。

---

### 三、 集群级别指标

当从单机多卡扩展到多机多卡时，还需要以下指标：

1.  **缩放效率**
    *   **含义**： `多卡吞吐量 / (单卡吞吐量 * 卡数)`
    *   **衡量**： 增加GPU数量带来的收益。理想是线性缩放（效率100%），但由于通信开销，通常低于100%。缩放效率过低表明**多卡通信**（如NCCL）是主要瓶颈。

2.  **资源分配率**
    *   **含义**： `已分配GPU数量 / 集群总GPU数量`
    *   **衡量**： 集群的资源是否被充分申请使用，避免资源闲置。

---

### 实践建议与工具推荐

1.  **监控工具**：
    *   **NVIDIA DCGM**: **行业标准**，专为数据中心集群监控设计，能采集所有上述硬件指标，并集成到Prometheus+Grafana监控大屏中。
    *   **Nsight Systems**: 用于性能剖析，定位瓶颈的根源（是数据加载慢？还是某个算子慢？还是同步等待长？）。
    *   **nvtop**: 类似于`htop`的GPU监控工具，方便在终端实时查看。
    *   **Prometheus + Grafana**: 搭建自定义监控看板，长期跟踪趋势。

2.  **诊断流程**：
    *   **第1步**： 查看`GPU-Util`和`吞吐量`。如果两者都低，问题可能出在数据加载或预处理（CPU端）。
    *   **第2步**： 如果`GPU-Util`高但`吞吐量`低，使用`DCGM`或`Nsight Systems`深入查看**算力利用率**和**显存带宽利用率**。
        *   算力利用率低： 可能模型本身包含大量非计算操作（如控制流、小算子）。
        *   显存带宽利用率高： 模型可能是内存访问密集型。
    *   **第3步**： 如果是多卡训练，吞吐量缩放效率低，使用`Nsight Systems`分析**多卡通信时间**占比。

### 总结

| 指标类别 | 关键指标 | 描述 | 工具 |
| :--- | :--- | :--- | :--- |
| **硬件层面** | GPU利用率 | GPU的“忙碌”程度，粗略指标 | `nvidia-smi` |
| | **算力利用率** | **计算单元的实际使用率**，核心指标 | DCGM, Nsight |
| | 显存使用量 & 带宽 | 显存资源是否充分利用 | DCGM, `nvidia-smi` |
| **任务层面** | **吞吐量** | **单位时间的有效工作量**，终极指标 | 业务代码 |
| | **MFU/HFU** | **对比硬件峰值的效率**，黄金标准 | 手动计算 |
| **集群层面** | **缩放效率** | 多卡扩展的有效性 | 业务代码 |

**最终，衡量算力利用率的目的是为了优化它。** 你的目标应该是：在保证**高吞吐量（业务高效）** 的前提下，让**硬件层面的算力利用率（资源高效）** 尽可能接近100%，并拥有较高的**MFU（算法高效）**。