# PyTorch Lightning ä¼˜åŠ¿æ€»ç»“

### ğŸ§‘â€ğŸ’» User

ä½¿ç”¨pytorch lightningæ„å»ºtrain pipelineæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ

---

### ğŸ¤– Assistant

ä½¿ç”¨ PyTorch Lightning æ„å»ºè®­ç»ƒæµæ°´çº¿çš„ä¸»è¦ä¼˜åŠ¿åœ¨äºå®ƒ**è§£è€¦äº†ç ”ç©¶ä»£ç ï¼ˆæ¨¡å‹æ¶æ„ã€è®­ç»ƒé€»è¾‘ï¼‰ä¸å·¥ç¨‹ä»£ç ï¼ˆç¡¬ä»¶ç®¡ç†ã€æ—¥å¿—è®°å½•ã€åˆ†å¸ƒå¼è®­ç»ƒï¼‰**ã€‚è¿™ä½¿å¾—ä»£ç æ›´åŠ ç®€æ´ã€å¯è¯»ã€æ˜“äºå¤ç°ï¼Œå¹¶ä¸”æ— éœ€ä¿®æ”¹ä»£ç å³å¯åœ¨ CPUã€GPU æˆ– TPU ä¸Šè¿è¡Œã€‚

ä»¥ä¸‹æ˜¯è¯¦ç»†çš„ä¼˜åŠ¿åˆ†æï¼š

### 1. ä»£ç ç»“æ„æ›´æ¸…æ™°ã€æ›´æ¨¡å—åŒ–

åœ¨åŸç”Ÿ PyTorch ä¸­ï¼Œè®­ç»ƒå¾ªç¯é€šå¸¸åŒ…å«å¤§é‡åµŒå¥—çš„ `for` å¾ªç¯ã€è®¾å¤‡è½¬ç§» `.to(device)` å’Œæ¡ä»¶åˆ¤æ–­ï¼Œå®¹æ˜“æ¼”å˜æˆéš¾ä»¥ç»´æŠ¤çš„â€œé¢æ¡ä»£ç â€ã€‚

Lightning å¼ºåˆ¶å°†ä»£ç ç»„ç»‡æˆ `LightningModule` ç±»ï¼Œå°†è®­ç»ƒçš„ä¸åŒé˜¶æ®µæ‹†åˆ†ä¸ºç‹¬ç«‹çš„æ–¹æ³•ï¼š
*   `training_step()`: å®šä¹‰å•ä¸ª batch çš„è®­ç»ƒé€»è¾‘ã€‚
*   `validation_step()`: å®šä¹‰éªŒè¯é€»è¾‘ã€‚
*   `configure_optimizers()`: å®šä¹‰ä¼˜åŒ–å™¨ã€‚

**å¯¹æ¯”ç¤ºä¾‹ï¼š**

<details>
<summary><b>ğŸ” ç‚¹å‡»æŸ¥çœ‹ï¼šåŸç”Ÿ PyTorch vs Lightning ä»£ç å¯¹æ¯”</b></summary>

**åŸç”Ÿ PyTorch (å…¸å‹çš„è®­ç»ƒå¾ªç¯):**

```python
# è®¾å¤‡ç®¡ç†
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = Model().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

# è®­ç»ƒå¾ªç¯
for epoch in range(epochs):
    # è®­ç»ƒé˜¶æ®µ
    model.train()
    for batch in train_loader:
        x, y = batch
        x, y = x.to(device), y.to(device) # æ‰‹åŠ¨æ¬è¿æ•°æ®
      
        optimizer.zero_grad()
        y_hat = model(x)
        loss = F.cross_entropy(y_hat, y)
        loss.backward()
        optimizer.step()
  
    # éªŒè¯é˜¶æ®µ
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for batch in val_loader:
            x, y = batch
            x, y = x.to(device), y.to(device)
            y_hat = model(x)
            val_loss += F.cross_entropy(y_hat, y)
  
    print(f"Epoch {epoch}: Val Loss {val_loss}")
```

**PyTorch Lightning (ç»“æ„åŒ–):**

```python
import pytorch_lightning as pl

class LitModel(pl.LightningModule):
    def __init__(self, model):
        super().__init__()
        self.model = model

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self.model(x)
        loss = F.cross_entropy(y_hat, y)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self.model(x)
        loss = F.cross_entropy(y_hat, y)
        self.log("val_loss", loss)

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=1e-3)

# åªéœ€å®ä¾‹åŒ– Trainerï¼Œæ— éœ€æ‰‹åŠ¨å†™å¾ªç¯
trainer = pl.Trainer(max_epochs=10)
trainer.fit(model)
```
</details>

---

### 2. ç¡¬ä»¶æ— å…³æ€§

è¿™æ˜¯ Lightning æœ€å¼ºå¤§çš„åŠŸèƒ½ä¹‹ä¸€ã€‚åœ¨åŸç”Ÿ PyTorch ä¸­ï¼Œä»å• GPU è¿ç§»åˆ°å¤š GPU æˆ– TPU éœ€è¦ä¿®æ”¹å¤§é‡ä»£ç ï¼ˆå¦‚ `DataParallel`, `DistributedDataParallel`, `XLA` ç­‰ï¼‰ã€‚

åœ¨ Lightning ä¸­ï¼Œä½ **å®Œå…¨ä¸éœ€è¦ä¿®æ”¹æ¨¡å‹ä»£ç **ï¼Œåªéœ€ä¿®æ”¹ `Trainer` çš„å‚æ•°å³å¯ï¼š

```python
# å• GPU
trainer = Trainer(accelerator="gpu", devices=1)

# å¤š GPU (è‡ªåŠ¨å¤„ç† DistributedDataParallel)
trainer = Trainer(accelerator="gpu", devices=4, strategy="ddp")

# TPU
trainer = Trainer(accelerator="tpu", devices=8)
```

æ‰€æœ‰çš„ `.to(device)` è°ƒç”¨ã€æ¢¯åº¦åŒæ­¥ã€è¿›ç¨‹ç®¡ç†éƒ½ç”± Lightning åœ¨åå°è‡ªåŠ¨å®Œæˆã€‚

### 3. å†…ç½®æœ€ä½³å®è·µ

Lightning é»˜è®¤å¼€å¯äº†è®¸å¤šæ·±åº¦å­¦ä¹ çš„æœ€ä½³å®è·µï¼Œé¿å…äº†æ–°æ‰‹å› ä¸ç†Ÿæ‚‰è€Œè¸©å‘ï¼š

*   **è‡ªåŠ¨æ··åˆç²¾åº¦ (AMP)**: åªéœ€è®¾ç½® `precision="16-mixed"` å³å¯åŠ é€Ÿè®­ç»ƒå¹¶å‡å°‘æ˜¾å­˜å ç”¨ã€‚
*   **æ¢¯åº¦è£å‰ª**: é€šè¿‡ `gradient_clip_val` å‚æ•°ä¸€è¡Œä»£ç å®ç°ã€‚
*   **æƒé‡è§£è€¦**: è‡ªåŠ¨å¤„ç†æŸäº›ä¼˜åŒ–å™¨çš„æƒé‡è¡°å‡é€»è¾‘ã€‚
*   **ç¡®å®šæ€§è®­ç»ƒ**: è®¾ç½® `deterministic=True` ç¡®ä¿å®éªŒå¯å¤ç°ã€‚

### 4. ç®€åŒ–åˆ†å¸ƒå¼è®­ç»ƒ

åˆ†å¸ƒå¼è®­ç»ƒï¼ˆå¦‚ DDPï¼‰åœ¨åŸç”Ÿ PyTorch ä¸­éå¸¸ç¹çï¼Œéœ€è¦å¤„ç†è¿›ç¨‹å¯åŠ¨ã€æ•°æ®é‡‡æ ·å™¨ã€æ¢¯åº¦åŒæ­¥ç­‰ã€‚

Lightning å°è£…äº†å¤šç§åˆ†å¸ƒå¼ç­–ç•¥ï¼š
*   `ddp`: åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œã€‚
*   `deepspeed`: é›†æˆ DeepSpeedï¼Œæ”¯æŒå¤§æ¨¡å‹è®­ç»ƒã€‚
*   `fsdp`: Fully Sharded Data Parallelï¼Œæ˜¾è‘—é™ä½æ˜¾å­˜å³°å€¼ã€‚

### 5. ä¸°å¯Œçš„å›è°ƒç³»ç»Ÿ

`Callback` ç³»ç»Ÿå…è®¸ä½ åœ¨è®­ç»ƒç”Ÿå‘½å‘¨æœŸçš„ä»»ä½•èŠ‚ç‚¹ï¼ˆå¦‚ epoch å¼€å§‹ã€batch ç»“æŸï¼‰æ’å…¥è‡ªå®šä¹‰é€»è¾‘ï¼Œè€Œæ— éœ€æ±¡æŸ“æ ¸å¿ƒè®­ç»ƒä»£ç ã€‚

å¸¸ç”¨å†…ç½®å›è°ƒåŒ…æ‹¬ï¼š
*   `EarlyStopping`: æ—©åœæ³•ã€‚
*   `ModelCheckpoint`: è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹ã€‚
*   `LearningRateMonitor`: ç›‘æ§å­¦ä¹ ç‡å˜åŒ–ã€‚

### 6. å¼ºå¤§çš„æ—¥å¿—ä¸å¯è§†åŒ–é›†æˆ

Lightning ä¸ä¸»æµçš„å®éªŒè·Ÿè¸ªå·¥å…·æ— ç¼é›†æˆã€‚ä½ åªéœ€è°ƒç”¨ `self.log()`ï¼ŒLightning ä¼šè‡ªåŠ¨å¤„ç†è·¨è®¾å¤‡çš„æ—¥å¿—åŒæ­¥ã€‚

```python
# åœ¨ LightningModule ä¸­
def training_step(self, batch, batch_idx):
    # ...
    self.log("train_loss", loss, on_step=True, on_epoch=True, prog_bar=True)
    return loss
```

é…åˆ `loggers` å‚æ•°ï¼Œå¯ä»¥ä¸€é”®æ¥å…¥ TensorBoard, Weights & Biases (WandB), MLFlow, Neptune ç­‰ã€‚

### æ€»ç»“

ä½¿ç”¨ PyTorch Lightning çš„æ ¸å¿ƒä»·å€¼åœ¨äº**â€œå…³æ³¨ç‚¹åˆ†ç¦»â€**ï¼š

| ç‰¹æ€§ | åŸç”Ÿ PyTorch | PyTorch Lightning |
| :--- | :--- | :--- |
| **è®­ç»ƒå¾ªç¯** | éœ€è¦æ‰‹åŠ¨ç¼–å†™ `for` å¾ªç¯ | å°è£…åœ¨ `Trainer` ä¸­ |
| **è®¾å¤‡ç®¡ç†** | æ‰‹åŠ¨ `.to(device)`ï¼Œå¤„ç†å¤æ‚é€»è¾‘ | è‡ªåŠ¨å¤„ç†ï¼Œå®Œå…¨é€æ˜ |
| **åˆ†å¸ƒå¼è®­ç»ƒ** | éœ€é‡æ„ä»£ç ï¼Œå¤„ç†è¿›ç¨‹é€šä¿¡ | ä»…éœ€ä¿®æ”¹ `Trainer` å‚æ•° |
| **ä»£ç å¤ç”¨** | éš¾ä»¥å¤ç”¨å·¥ç¨‹ä»£ç  | `Trainer` å’Œ `Callback` ææ˜“å¤ç”¨ |
| **è°ƒè¯•éš¾åº¦** | é«˜ï¼ˆå·¥ç¨‹ä»£ç ä¸æ¨¡å‹ä»£ç è€¦åˆï¼‰ | ä½ï¼ˆæ¨¡å‹é€»è¾‘ç‹¬ç«‹æ¸…æ™°ï¼‰ |

ç®€è€Œè¨€ä¹‹ï¼ŒPyTorch Lightning è®©ç ”ç©¶äººå‘˜ä¸“æ³¨äº**â€œæ¨¡å‹å¦‚ä½•å­¦ä¹ â€**ï¼ˆæ•°å­¦ä¸æ¶æ„ï¼‰ï¼Œè€Œè®©æ¡†æ¶å¤„ç†**â€œå¦‚ä½•é«˜æ•ˆè¿è¡Œâ€**ï¼ˆç¡¬ä»¶ä¸å·¥ç¨‹ï¼‰ã€‚