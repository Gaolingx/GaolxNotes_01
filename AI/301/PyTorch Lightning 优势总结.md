# PyTorch Lightning ä¼˜åŠ¿æ€»ç»“

### ğŸ§‘â€ğŸ’» User

ä½¿ç”¨pytorch lightningæ„å»ºtrain pipelineæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ

---

### ğŸ¤– Assistant

ä½¿ç”¨ PyTorch Lightning (PL) æ„å»ºè®­ç»ƒç®¡é“ï¼ˆTraining Pipelineï¼‰çš„æ ¸å¿ƒç†å¿µæ˜¯å°† **"å­¦æœ¯ç ”ç©¶ä»£ç "ï¼ˆæ¨¡å‹ä¸ç®—æ³•ï¼‰** ä¸ **"å·¥ç¨‹ä»£ç "ï¼ˆç¡¬ä»¶ç®¡ç†ã€è®­ç»ƒå¾ªç¯ã€æ—¥å¿—è®°å½•ï¼‰** è§£è€¦ã€‚

ç®€å•æ¥è¯´ï¼ŒPyTorch Lightning å°±åƒæ˜¯ PyTorch çš„ä¸€ä¸ªæ ‡å‡†åŒ–æ¥å£æˆ–æ¡†æ¶ï¼Œå®ƒä¸ä¼šé™åˆ¶ä½ çš„çµæ´»æ€§ï¼Œä½†ä¼šå¸®ä½ å¤„ç†æ‰ç¹ççš„æ ·æ¿ä»£ç ï¼ˆBoilerplateï¼‰ã€‚

ä»¥ä¸‹æ˜¯ä½¿ç”¨ PyTorch Lightning æ„å»º Pipeline çš„ä¸»è¦ä¼˜åŠ¿ï¼š

### 1. ä»£ç ç»“æ„çš„æ ‡å‡†åŒ–ä¸è§£è€¦ (Standardization)
åœ¨çº¯ PyTorch ä¸­ï¼Œæ¯ä¸ªäººçš„è®­ç»ƒå¾ªç¯ï¼ˆTraining Loopï¼‰å†™æ³•éƒ½ä¸ä¸€æ ·ï¼Œéš¾ä»¥ç»´æŠ¤å’Œé˜…è¯»ã€‚PL å¼ºåˆ¶å°†ä»£ç ç»„ç»‡æˆç‰¹å®šçš„æ¨¡å—ï¼š
*   **LightningModule**: åŒ…å«æ¨¡å‹æ¶æ„ã€å‰å‘ä¼ æ’­ã€Loss è®¡ç®—ï¼ˆ`training_step`ï¼‰ã€ä¼˜åŒ–å™¨å®šä¹‰ã€‚
*   **LightningDataModule**: åŒ…å«æ•°æ®çš„ä¸‹è½½ã€é¢„å¤„ç†ã€`DataLoader` çš„å°è£…ã€‚
*   **Trainer**: å¤„ç†å¾ªç¯ã€ç¡¬ä»¶è°ƒç”¨ã€Checkpoints ç­‰ã€‚

**ä¼˜åŠ¿ï¼š** å›¢é˜Ÿåä½œæ—¶ï¼Œä»»ä½•äººéƒ½èƒ½ç«‹åˆ»çœ‹æ‡‚åˆ«äººçš„ä»£ç ç»“æ„ï¼›æ¨¡å‹é€»è¾‘ä¸æ•°æ®é€»è¾‘åˆ†ç¦»ï¼Œå¤ç”¨æ€§æé«˜ã€‚

### 2. ç¡¬ä»¶åŠ é€Ÿä¸åˆ†å¸ƒå¼è®­ç»ƒçš„æ— ç¼åˆ‡æ¢ (Hardware Agnostic)
è¿™æ˜¯ PL æœ€å¼ºå¤§çš„åŠŸèƒ½ä¹‹ä¸€ã€‚åœ¨çº¯ PyTorch ä¸­ï¼Œä» CPU åˆ‡æ¢åˆ° GPUï¼Œæˆ–è€…ä»å•å¡åˆ‡æ¢åˆ°å¤šå¡ï¼ˆDDPï¼‰ï¼Œé€šå¸¸éœ€è¦ä¿®æ”¹å¤šå¤„ä»£ç ï¼ˆå¦‚ `.to(device)`, `DistributedSampler`, `rank` ç®¡ç†ç­‰ï¼‰ã€‚

åœ¨ PL ä¸­ï¼Œåªéœ€æ›´æ”¹ `Trainer` çš„å‚æ•°ï¼š

```python
# CPU
trainer = Trainer(accelerator="cpu", devices=1)

# å• GPU
trainer = Trainer(accelerator="gpu", devices=1)

# å¤š GPU (DDPæ¨¡å¼) - ä»£ç æ— éœ€ä»»ä½•æ”¹åŠ¨
trainer = Trainer(accelerator="gpu", devices=4, strategy="ddp")

# TPU
trainer = Trainer(accelerator="tpu", devices=8)
```

### 3. è‡ªåŠ¨åŒ–çš„æ··åˆç²¾åº¦è®­ç»ƒ (Mixed Precision)
ä½¿ç”¨åŠç²¾åº¦ï¼ˆFP16ï¼‰æˆ– BFloat16 è®­ç»ƒå¯ä»¥æ˜¾è‘—å‡å°‘æ˜¾å­˜å ç”¨å¹¶åŠ å¿«é€Ÿåº¦ã€‚åœ¨çº¯ PyTorch ä¸­éœ€è¦å¼•å…¥ `torch.cuda.amp` å¹¶ç®¡ç† Scalerã€‚

åœ¨ PL ä¸­ï¼Œåªéœ€ä¸€ä¸ª flagï¼š
```python
# è‡ªåŠ¨å¤„ç† Loss Scaling å’Œç±»å‹è½¬æ¢
trainer = Trainer(precision="16-mixed") 
```

### 4. æ¶ˆé™¤æ ·æ¿ä»£ç  (Eliminating Boilerplate)
PL å¸®ä½ è‡ªåŠ¨å¤„ç†äº†è®­ç»ƒå¾ªç¯ä¸­å®¹æ˜“å‡ºé”™çš„ç»†èŠ‚ã€‚ä½ **ä¸å†éœ€è¦**æ‰‹åŠ¨ç¼–å†™ä»¥ä¸‹ä»£ç ï¼š
*   `optimizer.zero_grad()`
*   `loss.backward()`
*   `optimizer.step()`
*   `model.eval()` å’Œ `model.train()` çš„åˆ‡æ¢
*   éªŒè¯é›†å’Œæµ‹è¯•é›†çš„å¾ªç¯éå†
*   Tensor çš„è®¾å¤‡ç§»åŠ¨ (`.to(device)`)

### 5. å¼€ç®±å³ç”¨çš„é«˜çº§åŠŸèƒ½ (Advanced MLOps Features)
æ„å»ºå®Œå–„çš„ Pipeline é€šå¸¸éœ€è¦å¾ˆå¤šè¾…åŠ©åŠŸèƒ½ï¼ŒPL é€šè¿‡ **Callbacks** æœºåˆ¶ç›´æ¥æä¾›ï¼š
*   **ModelCheckpoint**: æ ¹æ®éªŒè¯é›† Loss è‡ªåŠ¨ä¿å­˜æœ€å¥½çš„ $K$ ä¸ªæ¨¡å‹ã€‚
*   **EarlyStopping**: å½“æŒ‡æ ‡ä¸å†æå‡æ—¶è‡ªåŠ¨åœæ­¢è®­ç»ƒã€‚
*   **Logging**: è‡ªåŠ¨å¯¹æ¥ TensorBoard, WandB, MLFlow ç­‰ï¼Œåªéœ€è°ƒç”¨ `self.log('loss', loss)`ã€‚
*   **Gradient Clipping**: æ¢¯åº¦è£å‰ªåªéœ€åœ¨ Trainer ä¸­è®¾ç½®å‚æ•°ã€‚
*   **Accumulate Gradients**: æ¢¯åº¦ç´¯ç§¯åªéœ€è®¾ç½® `accumulate_grad_batches=4`ã€‚

### 6. è°ƒè¯•ä¸å¥å£®æ€§ (Debugging & Robustness)
PL æä¾›äº†ä¸€äº›éå¸¸å®ç”¨çš„è°ƒè¯•å·¥å…·ï¼š
*   **`fast_dev_run=True`**: è·‘å®Œ 1 ä¸ª batch çš„è®­ç»ƒå’ŒéªŒè¯å°±ç«‹å³åœæ­¢ã€‚ç”¨äºå¿«é€Ÿæ£€æŸ¥ä»£ç æ˜¯å¦æœ‰ Bugï¼ˆä¸ç”¨ç­‰å®Œä¸€ä¸ª epoch æ‰çŸ¥é“æŠ¥é”™ï¼‰ã€‚
*   **`overfit_batches=0.01`**: ä»…ä½¿ç”¨ 1% çš„æ•°æ®åå¤è®­ç»ƒï¼Œç”¨äºæ£€æŸ¥æ¨¡å‹æ˜¯å¦æœ‰èƒ½åŠ›è¿‡æ‹Ÿåˆï¼ˆéªŒè¯æ¨¡å‹å®¹é‡ï¼‰ã€‚
*   **Sanity Check**: è®­ç»ƒå¼€å§‹å‰ï¼ŒPL ä¼šé»˜è®¤è·‘ä¸€å°è½®éªŒè¯é›†ï¼Œç¡®ä¿éªŒè¯ä»£ç æ²¡é—®é¢˜ï¼Œé˜²æ­¢è®­ç»ƒå‡ å°æ—¶ååœ¨éªŒè¯é˜¶æ®µå´©æºƒã€‚

---

<details>
<summary><strong>ğŸ” ç‚¹å‡»å±•å¼€ï¼šä»£ç å¯¹æ¯” (Pure PyTorch vs. PyTorch Lightning)</strong></summary>

ä¸ºäº†ç›´è§‚å±•ç¤ºä¼˜åŠ¿ï¼Œæˆ‘ä»¬å¯¹æ¯”ä¸€ä¸‹å®ç°ç›¸åŒåŠŸèƒ½çš„ä»£ç é‡å’Œç»“æ„ã€‚

#### 1. Pure PyTorch (ç¹ççš„å¾ªç¯)

ä½ éœ€è¦æ‰‹åŠ¨ç®¡ç†çŠ¶æ€ã€è®¾å¤‡å’Œå¾ªç¯ã€‚

```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = MyModel().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

# è®­ç»ƒå¾ªç¯
for epoch in range(10):
    model.train()
    for batch in train_loader:
        x, y = batch
        x, y = x.to(device), y.to(device) # æ‰‹åŠ¨ç®¡ç†è®¾å¤‡
      
        optimizer.zero_grad()
        y_hat = model(x)
        loss = F.cross_entropy(y_hat, y)
        loss.backward() # æ‰‹åŠ¨åå‘ä¼ æ’­
        optimizer.step()
      
    # éªŒè¯å¾ªç¯
    model.eval()
    with torch.no_grad():
        val_loss = 0
        for batch in val_loader:
            x, y = batch
            x, y = x.to(device), y.to(device)
            y_hat = model(x)
            val_loss += F.cross_entropy(y_hat, y).item()
          
    print(f"Epoch {epoch}: Val Loss {val_loss}")
    # è¿˜éœ€è¦æ‰‹åŠ¨å†™ä¿å­˜æ¨¡å‹çš„é€»è¾‘...
```

#### 2. PyTorch Lightning (ç»“æ„åŒ–)

é€»è¾‘è¢«å°è£…åœ¨ç±»ä¸­ï¼Œå¾ªç¯ç”± Trainer æ¥ç®¡ã€‚

```python
import lightning.pytorch as pl

class LitModel(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.model = MyModel()

    def training_step(self, batch, batch_idx):
        # ä¸éœ€è¦æ‰‹åŠ¨ .to(device)
        x, y = batch
        y_hat = self.model(x)
        loss = F.cross_entropy(y_hat, y)
        self.log("train_loss", loss) # è‡ªåŠ¨è®°å½•
        return loss # åªéœ€è¦è¿”å› Loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self.model(x)
        loss = F.cross_entropy(y_hat, y)
        self.log("val_loss", loss)

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=1e-3)

# è¿è¡Œ Pipeline
model = LitModel()
#è¿™é‡Œç”šè‡³ä¸éœ€è¦å†™å¾ªç¯ï¼Œç›´æ¥æŒ‡å®š GPU æ•°é‡
trainer = pl.Trainer(accelerator="gpu", devices=1, max_epochs=10) 
trainer.fit(model, train_loader, val_loader)
```

</details>

---

### æ€»ç»“ï¼šä»€ä¹ˆæ—¶å€™ä½¿ç”¨ PyTorch Lightningï¼Ÿ

*   **æ¨èä½¿ç”¨**ï¼šç»å¤§å¤šæ•°æ·±åº¦å­¦ä¹ é¡¹ç›®ï¼Œç‰¹åˆ«æ˜¯å½“ä½ éœ€è¦å¿«é€Ÿå®éªŒã€å¤šå¡è®­ç»ƒã€æˆ–è€…å¸Œæœ›ä»£ç ç»“æ„æ¸…æ™°ã€æ˜“äºå¤ç°æ—¶ã€‚å®ƒåœ¨å·¥ä¸šç•Œå’Œå­¦æœ¯ç•Œï¼ˆå¦‚ Kaggle ç«èµ›ï¼‰éƒ½éå¸¸æµè¡Œã€‚
*   **ä¸æ¨èä½¿ç”¨**ï¼šå½“ä½ éœ€è¦å¯¹è®­ç»ƒå¾ªç¯è¿›è¡Œæåº¦åº•å±‚çš„é­”æ”¹ï¼ˆä¾‹å¦‚æå…¶å¤æ‚çš„å¼ºåŒ–å­¦ä¹ å¾ªç¯ï¼Œæˆ–è€…éæ ‡å‡†çš„æ¢¯åº¦æ›´æ–°é€»è¾‘ï¼‰ï¼Œä¸” PL çš„ `Callback` æˆ– `Manual Optimization` æ¨¡å¼ä»æ— æ³•æ»¡è¶³éœ€æ±‚æ—¶ï¼ˆè¿™ç§æƒ…å†µæå°‘ï¼‰ã€‚

ä»æ•°å­¦è§’åº¦çœ‹ï¼ŒPL å¹¶æ²¡æœ‰æ”¹å˜æ¨¡å‹è®¡ç®—çš„æœ¬è´¨ã€‚ä¾‹å¦‚ï¼Œç›®æ ‡ä»ç„¶æ˜¯æœ€å°åŒ–æŸå¤±å‡½æ•° $J(\theta)$ï¼š
$$ \theta_{t+1} = \theta_t - \eta \cdot \nabla_\theta J(\theta_t) $$
PL åªæ˜¯å¸®ä½ æ›´ä¼˜é›…ã€æ›´é«˜æ•ˆåœ°æ‰§è¡Œè¿™ä¸ª $\nabla$ è®¡ç®—å’Œå‚æ•°æ›´æ–°çš„è¿‡ç¨‹ã€‚