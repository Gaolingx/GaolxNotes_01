# LLM self-attention QKV偏置原因分析

### 🧑‍💻 User

为什么一些llm的self-attention模块的qkv投影层，是带偏置（*.attn_qkv.bias）的？

---

### 🤖 Assistant

## 深入解析：为何部分LLM的自注意力机制QKV投影层选择保留偏置项（bias）？

在大型语言模型（LLM）中，自注意力（Self-Attention）机制是其核心构成，而其中的查询（Query）、键（Key）和值（Value）——即QKV——的线性投影层是否包含偏置项（bias），是模型架构设计中的一个微妙却重要的选择。尽管在某些情况下，这些偏置项看似多余，但许多先进的LLM依然选择保留它们。这背后是模型设计者在模型表达能力、训练稳定性以及计算效率之间深思熟虑的权衡。

### 保留偏置项的核心理由：增强模型的灵活性与表达能力

在神经网络的任何线性层中，偏置项（bias）的基本作用类似于线性函数 $y = mx + b$ 中的截距 $b$。它允许激活函数在输入空间中进行平移，从而增强了模型的拟合能力。若缺少偏置项，无论输入如何，神经元的激活都必须经过原点，这在一定程度上限制了模型的表达潜力。

在QKV投影层中，保留偏置项的具体优势体现在以下几个方面：

* **提供可学习的基准偏移**：QKV投影本质上是将输入的词嵌入向量线性变换到三个不同的子空间。带有偏置项的投影，例如 $Q = XW_q + b_q$ ，允许模型为每个投影学习一个固定的、可调整的偏移量。这意味着，即使所有输入都为零，QKV向量也可以是非零的，这为模型提供了更大的灵"活度，使其能够更好地捕捉数据中的复杂模式。

* **值（Value）偏置的直接影响**：在QKV的三个偏置项中，值（Value）的偏置项（`*.attn_v.bias`）被认为尤为重要。根据亚马逊科学家Mahdi Namazifar等人的研究论文《Role of Bias Terms in Dot-Product Attention》，值的偏置项直接作用于注意力头的输出。自注意力的最终输出是注意力权重与值向量的加权和。因此，值的偏置项允许模型向最终的上下文向量中添加一个可学习的、与当前输入无关的向量，从而能够平移整个输出空间，这对于模型生成多样化的表征至关重要。

* **查询（Query）偏置的间接作用**：查询的偏置项（`*.attn_q.bias`）虽然不像值的偏置那样直接作用于输出，但它会影响注意力分数的计算。通过调整查询向量，模型可以改变其与其他词的“共鸣方式”，从而微调注意力分布。

### 为何可以省略偏置项：冗余性与效率的考量

与保留偏置项的理由相对应，一些LLM架构选择省略它们，主要基于以下几点：

* **层归一化（Layer Normalization）的中心化效应**：在Transformer架构中，线性投影层之后通常会紧跟一个层归一化（Layer Normalization）层。层归一化的一个核心作用是将数据中心化（使其均值为0）。当一个带有偏置的线性层的输出被立即中心化时，偏置项的效果在很大程度上会被抵消。因此，为了简化模型、减少不必要的参数，一些设计者选择省略这些偏置项。

* **键（Key）偏置的数学冗余性**：前述的《Role of Bias Terms in Dot-Product Attention》论文通过数学证明指出，键（Key）的偏置项（`*.attn_k.bias`）实际上是冗余的。这是因为注意力分数的计算涉及到查询和键的点积，而后会经过Softmax函数。Softmax函数的一个特性是其对于输入的平移不变性，即 $Softmax(z_i + c) = Softmax(z_i)$。由于键的偏置项在与所有查询向量点积后，相当于给Softmax的输入增加了一个常数，因此并不会改变最终的注意力权重分布。

* **提升训练稳定性与计算效率**：省略偏置项可以减少模型的总参数量，从而略微降低内存占用和计算开销。更重要的是，在一些模型中，如阿里巴巴的Qwen2和Qwen3系列，去除QKV偏置被认为是提升训练稳定性的措施之一。谷歌的ViT-22B模型也为了提升利用率而省略了QKV投影中的偏置。

### 架构选择的实例

* **包含偏置项的模型**：
    * **GPT-2** 等早期且影响深远的模型在其QKV投影中包含了偏置项，这为后续许多模型的设计奠定了基础。
    * 许多开源模型默认情况下也会保留这些偏置，因为它们被认为是增加模型容量的标准做法。

* **不包含或部分不包含偏置项的模型**：
    * **Qwen系列**：为了提升训练的稳定性，在其后续版本中移除了QKV偏置。
    * **ViT-22B**：出于提升计算效率和利用率的考虑，省略了QKV投影和层归一化中的偏置。
    * **LLaMA系列**：虽然其具体实现细节可能因版本而异，但在一些分析中也观察到其可能不使用偏置项，或者其影响被后续的RMSNorm所减弱。

### 结论

总而言之，LLM的自注意力模块QKV投影层是否带偏置，反映了模型设计哲学中的一种权衡。保留偏置项，尤其是查询和值的偏置，为模型提供了更强的表达能力和灵活性，使其能够学习到更丰富的数据表征。而省略偏置项，则更多是出于对模型简洁性、训练稳定性和计算效率的追求，特别是在层归一化可能使其效果减弱的背景下，以及键偏置被证明在数学上是冗余的情况下。未来的LLM架构设计，将继续在这些因素之间寻找最优的平衡点。