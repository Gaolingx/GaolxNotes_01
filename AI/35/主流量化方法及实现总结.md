# 主流量化方法及实现总结

### 🧑‍💻 User

主流的groupwise/channelwise量化有哪些具体的实现？（给出具体的论文或者github链接）

---

### 🤖 Assistant

以下是主流的 **Groupwise/Channelwise量化方法** 的具体实现、论文及代码链接。这些方法通过**分组量化权重或激活值**，显著提升了低精度模型的性能，尤其适用于Transformer等复杂模型：

---

### **一、Channelwise量化（Per-Channel Quantization）**
对**每个通道（Channel）独立量化**，为不同通道分配独立的缩放因子（scale）和零点（zero-point）。

#### 1. **PyTorch 官方实现**
   - **论文**：无专门论文，属于基础量化方法  
   - **代码**：  
     - [PyTorch Per-Channel Quantization Tutorial](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html)  
     - 关键API：`torch.quantization.quantize_dynamic`（动态量化）或 `torch.ao.quantization.QuantStub`（静态量化）

#### 2. **TensorRT 的 Per-Channel 量化**
   - **论文**：无专门论文，由NVIDIA实现  
   - **代码**：  
     - [TensorRT Quantization Toolkit](https://github.com/NVIDIA/TensorRT/tree/main/tools/pytorch-quantization)  
     - 支持INT8 per-channel权重量化，激活值可per-tensor或per-channel。

#### 3. **HAWQ (Hessian Aware Quantization)**
   - **论文**：[_HAWQ: Hessian AWare Quantization of Neural Networks with Mixed-Precision_ (ICCV 2019)](https://arxiv.org/abs/1905.03696)  
   - **核心思想**：基于Hessian矩阵敏感性分析，为不同层分配不同量化精度（如4/8-bit）。  
   - **代码**：  
     - [HAWQ官方实现（PyTorch）](https://github.com/Zhen-Dong/HAWQ)

---

### **二、Groupwise量化（Per-Group Quantization）**
将权重按**组（Group）分割**，每组独立量化（如将一层的权重分为多组，每组有自己的缩放因子）。

#### 1. **Q-BERT（分组量化+BERT压缩）**
   - **论文**：[_Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT_ (AAAI 2020)](https://arxiv.org/abs/1909.05840)  
   - **核心思想**：对BERT权重分组量化（如128元素为一组），结合Hessian分析选择混合精度。  
   - **代码**：  
     - [Q-BERT官方实现（PyTorch）](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/Q-BERT)

#### 2. **GOBO (Group-wise Outlier Block Quantization)**
   - **论文**：[_GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference_ (MICRO 2020)](https://arxiv.org/abs/2007.06887)  
   - **核心思想**：将权重分组（如8×8块），每组独立量化并保留异常值（outliers）。  
   - **代码**：  
     - [GOBO官方实现（C++/Python）](https://github.com/IST-DASLab/gobo)

#### 3. **ZeroQ（动态分组量化）**
   - **论文**：[_ZeroQ: A Novel Zero-Shot Quantization Framework_ (CVPR 2020)](https://arxiv.org/abs/2001.00281)  
   - **核心思想**：无需数据校准，支持per-channel和group-wise量化。  
   - **代码**：  
     - [ZeroQ官方实现（PyTorch）](https://github.com/amirgholami/ZeroQ)

---

### **三、硬件友好的Groupwise实现**
#### 1. **NVIDIA 的 **
   - **论文**：[_Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation_ (2020)](https://arxiv.org/abs/2004.09602)  
   - **核心思想**：在TensorRT中支持4-bit group-wise量化（组大小≥64）。  
   - **代码**：集成于[TensorRT](https://github.com/NVIDIA/TensorRT)。

#### 2. **TVM 的 Group Quantization**
   - **实现**：TVM支持将卷积层权重按组量化（如`group_size=64`）。  
   - **代码**：  
     - [TVM Quantization Example](https://github.com/apache/tvm/tree/main/tutorials/quantize)  
     - 使用`relay.transform.quantize`配置`group_size`参数。

---

### **四、其他重要实现**
#### 1. **AWQ (Activation-aware Weight Quantization)**
   - **论文**：[_AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration_ (2023)](https://arxiv.org/abs/2306.00978)  
   - **核心思想**：基于激活分布保护重要权重（per-group缩放因子）。  
   - **代码**：  
     - [AWQ官方实现（PyTorch）](https://github.com/mit-han-lab/llm-awq)

#### 2. **GPTQ（分组量化+二阶优化）**
   - **论文**：[_GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers_ (ICLR 2023)](https://arxiv.org/abs/2210.17323)  
   - **核心思想**：对权重分组（如128列一组），使用Cholesky分解优化量化误差。  
   - **代码**：  
     - [GPTQ官方实现](https://github.com/IST-DASLab/gptq)

---

### **关键总结**
| **方法**       | 类型          | 论文/框架          | 代码链接 |
|----------------|---------------|-------------------|----------|
| PyTorch官方    | Per-Channel   | PyTorch Docs      | [链接](https://pytorch.org/docs/stable/quantization.html) |
| HAWQ           | Per-Channel   | ICCV 2019         | [链接](https://github.com/Zhen-Dong/HAWQ) |
| Q-BERT         | Groupwise     | AAAI 2020         | [链接](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/Q-BERT) |
| GPTQ           | Groupwise     | ICLR 2023         | [链接](https://github.com/IST-DASLab/gptq) |
| AWQ            | Groupwise     | 2023              | [链接](https://github.com/mit-han-lab/llm-awq) |
| TensorRT       | Per-Channel   | NVIDIA Docs       | [链接](https://github.com/NVIDIA/TensorRT) |

> **注意**：  
> - **Per-Channel** 常用于权重（如Conv2d的`out_channels`），激活值通常用Per-Tensor。  
> - **Groupwise** 的组大小（`group_size`）需平衡精度与硬件支持（如GPU对`group_size=128`更友好）。  
> - 实际部署时需检查硬件支持（如英伟达GPU支持Per-Channel，但某些NPU仅支持Per-Tensor）。