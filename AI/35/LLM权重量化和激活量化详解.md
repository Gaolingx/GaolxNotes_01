# LLM权重量化和激活量化详解

### 🧑‍💻 User

llm的权重量化和激活量化分别是怎么一回事？

---

### 🤖 Assistant

好的，我们来分别解释一下LLM（大语言模型）中的**权重量化**和**激活量化**。它们都是模型量化的核心组成部分，目标都是**降低模型的存储需求、内存占用和计算复杂度，从而提升推理速度和效率**，但作用的对象和时机不同。

**核心目标：** 将原本用高精度浮点数（通常是FP32或BF16）表示的模型参数和计算过程中的中间值，转换为低精度格式（如INT8, INT4, FP16, FP8等），同时尽可能保持模型的精度。

## 1. 权重量化

*   **作用对象：** 模型的**权重参数**。这些是模型在训练后学习到的固定值，存储在模型中（如Transformer中的注意力权重、全连接层权重、嵌入层权重等）。
*   **时机：** **推理之前**。权重量化通常在模型部署前完成，是一个一次性的过程（静态的）。量化后的权重直接存储在模型文件中。
*   **核心思想：** 为所有权重找到一个合适的**量化范围**（最小值和最大值），然后在这个范围内将高精度的权重值映射到低精度的整数或浮点值上。
*   **关键步骤：**
    1.  **确定范围：** 分析所有权重值的分布，找到全局或局部的最大值和最小值。有时会通过少量校准数据来微调这个范围。
    2.  **计算缩放因子：** 缩放因子 `scale = (max_value - min_value) / (quant_max - quant_min)`。例如，将FP32量化到INT8（范围-128到127）时，`quant_max = 127`, `quant_min = -128`。
    3.  **计算零点：** 对于**对称量化**（通常用于权重），零点通常是0。对于**非对称量化**（常用于激活），零点 `zero_point` 通常对应原始数据中的最小值或均值。
    4.  **量化：** 使用缩放因子和零点（如果需要）将每个权重值转换为低精度整型或浮点数：`weight_quant = round(weight_fp32 / scale) + zero_point` (INT量化) 或 `weight_quant = weight_fp32 * scale` (浮点量化)。
*   **优势：**
    *   **显著减小模型文件大小：** INT8权重比FP32小4倍，INT4小8倍。这对于移动端、嵌入式设备或大规模部署至关重要。
    *   **减少内存带宽需求：** 加载低精度权重所需的内存带宽更少，这是许多硬件上的瓶颈。
    *   **为后续低精度计算做准备：** 量化后的权重可以直接用于低精度的矩阵乘法等操作。
*   **挑战：**
    *   **精度损失：** 压缩过程不可避免地会丢失信息，可能导致模型输出质量下降。需要仔细选择量化范围和算法来最小化损失。
    *   **敏感层处理：** 某些层（如输出层）的权重可能对量化更敏感，需要特殊处理（如更高精度或混合精度）。

## 2. 激活量化

*   **作用对象：** 模型**前向传播过程中产生的中间计算结果（激活值）**。例如，输入数据经过线性层或卷积层后的输出、注意力分数、ReLU/GELU激活函数的输出等。
*   **时机：** **推理过程中**。激活量化是动态发生的，因为激活值取决于当前的输入数据。
*   **核心思想：** 为当前层的输入激活值或输出激活值找到一个合适的**量化范围**，并将这些中间结果转换为低精度格式，以便后续的计算（如矩阵乘、卷积）也能在低精度下进行。
*   **关键步骤（通常需要校准）：**
    1.  **校准：** 在正式部署前，使用一组有代表性的输入数据（校准数据集）运行模型（或部分模型）。
    2.  **收集统计信息：** 记录各层激活值（通常是输出）在运行校准数据时的分布情况（最小值、最大值、直方图等）。
    3.  **确定范围：** 基于收集到的统计信息（例如，取某个百分位数如99.99%作为最大值，避免极端离群点的影响），为每一层的激活确定一个固定的量化范围（`min_act`, `max_act`）和相应的缩放因子/零点。这个过程是**静态**的（在推理前固定好）。
    4.  **量化：** 在推理时，当数据流过该层时，使用预先确定的该层的缩放因子和零点，将该层的输出激活值量化：`act_quant = round(act_fp32 / scale_act) + zero_point_act`。
*   **动态量化：** 一种变体，不进行离线校准，而是在**每次推理时实时计算**当前输入数据激活值的范围（如最小/最大值）。这避免了校准步骤，但增加了运行时的计算开销。
*   **优势：**
    *   **大幅加速计算：** 核心计算操作（如GEMM - 通用矩阵乘）可以在低精度（如INT8）下执行，这比高精度计算快得多，尤其是在支持低精度加速的硬件（如GPU的Tensor Cores, NPU, TPU）上。这是推理速度提升的关键。
    *   **减少中间结果内存占用：** 低精度的激活值占用的内存更少，缓解了内存瓶颈。
*   **挑战：**
    *   **对输入数据敏感：** 激活值高度依赖于输入数据。不同输入可能导致激活值范围差异很大。离群值的存在是主要难题。
    *   **精度损失风险更高：** 激活值的动态范围通常比权重更大，且可能包含极端值。不恰当的量化范围（尤其是对离群值处理不当）会严重损害模型精度。
    *   **范围确定复杂：** 找到鲁棒的量化范围（`min_act`, `max_act`）比权重更困难，需要精心设计的校准策略（如直方图、移动平均、百分位数截断）。
    *   **量化-反量化开销：** 在层与层之间传递数据时，有时需要在低精度激活用于计算前，将其反量化回高精度进行某些操作（如LayerNorm, Softmax），然后再量化回去。这个QDQ操作本身也有计算成本。

## 总结与关系

| 特性         | 权重量化                          | 激活量化                              |
| :----------- | :-------------------------------- | :------------------------------------ |
| **作用对象** | 模型**权重** (静态参数)           | 模型**中间计算结果** (动态值)         |
| **主要时机** | 部署前 (一次性的，静态)           | 推理过程中 (动态的，依赖输入)         |
| **主要目的** | **减小模型大小，降低内存带宽需求** | **加速核心计算操作，减少计算内存占用** |
| **关键优势** | 模型压缩，加载更快                | 计算速度大幅提升                      |
| **主要挑战** | 精度损失，敏感层处理              | 对输入敏感，离群值处理，范围确定复杂  |
| **量化方式** | 通常静态，对称量化为主            | 需要校准(静态)或动态计算范围          |
| **依赖性**   | 相对独立                          | 通常**依赖权重量化** (W量化是A量化的前提) |

**协同工作：**
通常，为了获得最大的推理加速和内存节省，权重量化和激活量化会**结合使用**。最常见的组合是 **W8A8**（权重和激活都量化为INT8）或 **W4A16**（权重INT4，激活保持FP16/BF16）。激活量化是实现端到端低精度计算、最大化硬件加速潜力的关键一步，但它也往往是精度损失的主要来源，需要更精细的策略（如分组量化、混合精度、更好的校准方法）来缓解。

简单来说：
*   **权重量化 = 压缩模型文件本身。**
*   **激活量化 = 加速模型运行时的计算过程。**

两者结合，才能在大幅提升LLM推理效率的同时，尽可能地保持模型的性能。