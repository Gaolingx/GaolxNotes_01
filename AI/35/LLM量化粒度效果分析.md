# LLM量化粒度效果分析

### 🧑‍💻 User

llm的int8量化，虽然质量一般都比较高，但是也有做的好的，比如pergroup或者perchannel，也有做的差的，像pertensor那种粗粒度大的量化，能不能具体分析下？

---

### 🤖 Assistant

LLM（大语言模型）的 Int8 量化质量差异，核心在于**量化粒度（Quantization Granularity）**与**数据分布（Data Distribution）**之间的矛盾。

LLM 的参数（Weights）和激活值（Activations）通常服从某种分布（如高斯分布），但往往伴随着显著的**离群值（Outliers）**。量化的本质是将浮点数范围映射到有限的整数范围（[-128, 127]）。

如果量化粒度太粗（如 Per-Tensor），一个极大的离群值会将整个量化范围“撑大”，导致大部分处于正常范围内的小数值被压缩到同一个整数刻度上，造成严重的精度损失。

下面我们具体分析 Per-Tensor、Per-Channel 和 Per-Group 三种粒度的原理、优缺点及其数学直觉。

---

### 1. Per-Tensor 量化 (粗粒度)

这是最粗糙的量化方式。对于整个张量（Tensor），例如整个权重矩阵 $W$ 或整个激活层输入 $X$，只使用**一个**缩放因子（Scale Factor, $S$）和零点（Zero Point, $Z$）。

*   **数学表达：**
    假设 Tensor 中的最大绝对值为 $M = \max(|T|)$。
    $$ S = \frac{M}{127} $$
    $$ T_{q} = \text{round}(\frac{T}{S}) $$

*   **问题分析（离群值灾难）：**
    LLM 的权重或激活值中常会出现极个别的“尖刺”（Outliers）。
    *   假设大部分数值在 $[-1.0, 1.0]$ 之间。
    *   突然有一个离群值是 $100.0$。
    *   **后果：** 为了包容这个 $100.0$，Scale $S$ 必须变得很大 ($100/127 \approx 0.78$)。
    *   **精度崩塌：** 此时，原本的数值 $1.0$ 被量化为 $1.0 / 0.78 \approx 1$。原本的 $0.5$ 被量化为 $0$。这导致 $[-0.4, 0.4]$ 之间的所有细微信息全部变成了 0。这就好比用一把量程为 100米的尺子去测量蚂蚁的腿长，根本测不准。

*   **适用场景：** 数据分布非常均匀，没有极端离群值的小模型或特定层。在 LLM 中几乎**不可用**，除非配合复杂的平滑技术（如 SmoothQuant）。

### 2. Per-Channel / Per-Token 量化 (中粒度)

这是目前 LLM 量化（尤其是 Weight Only 量化）的**主流标准**。

*   **对于权重（Weights）：Per-Channel**
    权重矩阵通常形状为 $[C_{out}, C_{in}]$。我们为**每一个输出通道**（或输入通道，取决于矩阵乘法方向）单独计算一个 Scale。
    *   **原理：** 矩阵的每一行（或列）都有自己的 $S_i$。
    *   **优势：** 如果第 1 个通道有一个离群值 $100$，它只会拉低第 1 个通道的精度。第 2 个通道如果最大值只有 $1.0$，它的 Scale 就可以很小，从而保留极高的精度。
    *   **实现友好：** 在矩阵乘法 $Y = XW$ 中，Per-Channel 的 Scale 可以直接提取到矩阵乘法之外，计算开销极小。

*   **对于激活（Activations）：Per-Token**
    对于输入 $X$ (形状 $[Batch, Seq, Hidden]$)，针对每一个 Token（即每一行）计算一个 Scale。
    *   **必要性：** LLM 的激活值存在 **"Outlier Features"** 现象（某些特定维度的值在所有 Token 上都很大）。Per-Token 可以动态适应每个 Token 的幅度。

*   **局限性：** 虽然比 Per-Tensor 好很多，但如果单个 Channel 内的数据分布依然极度不均匀（例如长尾分布严重），精度仍会受损。

### 3. Per-Group 量化 (细粒度)

这是追求极致压缩比和高精度时的选择（如 GPTQ, AWQ 常用的设置）。

*   **原理：**
    将一个 Channel 内的元素进一步切分成多个**组（Group）**，比如每 128 个参数为一组（Group Size = 128）。为每一个组单独计算一个 Scale 和 Zero-point。
    *   如果权重矩阵是 $[4096, 4096]$，Per-Channel 有 4096 个 Scale。
    *   如果是 Group-128，则有 $4096 \times (4096 / 128) \approx 13$ 万个 Scale。

*   **数学直觉：**
    $$ S_{group\_j} = \frac{\max(|W_{group\_j}|)}{127} $$
    这种方法通过缩小“视场”，让 Scale 更加贴合局部的数值范围。离群值的影响被限制在极小的 128 个参数范围内，不会污染整个通道。

*   **优缺点：**
    *   **优点：** 精度最高，甚至可以逼近 FP16 的表现。它能极其细致地捕捉参数的局部波动。
    *   **缺点：** 存储开销变大（因为要存很多 Scale），解量化（Dequantization）时的计算开销略微增加，对显存带宽和计算核心的配合有更高要求。

---

### 数值举例对比

为了直观理解，我们看一个简单的向量例子：
$$ V = [0.1, 0.2, -0.1, 0.05, \mathbf{100.0}] $$
目标：量化到 Int8 $[-127, 127]$（简化起见忽略 Zero-point，只看 Scale）。

#### 方案 A: Per-Tensor (全局量化)
*   绝对值最大值 $Max = 100.0$
*   $Scale = 100 / 127 \approx 0.787$
*   量化结果 $Q = \text{round}(V / Scale)$:
    *   $0.1 \to 0$
    *   $0.2 \to 0$
    *   $-0.1 \to 0$
    *   $0.05 \to 0$
    *   $100.0 \to 127$
*   **结论：** 前面 4 个正常数值的信息**全部丢失**，变成了 0。这就是 Per-Tensor 效果差的原因。

#### 方案 B: Per-Group (假设 Group Size = 4)
我们将 $V$ 分为两组（为了演示，切分一下）：
Group 1: $[0.1, 0.2, -0.1, 0.05]$
Group 2: $[100.0]$ (假设后面还有其他数)

*   **Group 1 计算：**
    *   $Max = 0.2$
    *   $Scale_1 = 0.2 / 127 \approx 0.00157$
    *   量化结果：
        *   $0.1 \to 64$
        *   $0.2 \to 127$
        *   $-0.1 \to -64$
        *   $0.05 \to 32$
    *   **恢复时：** $64 \times 0.00157 \approx 0.10048$ (精度极高！)

*   **Group 2 计算：**
    *   $Max = 100$
    *   $Scale_2 = 100 / 127 \dots$
    *   量化离群值。

*   **结论：** 通过分组，离群值 $100$ 只影响了它自己所在的组，而 Group 1 的微小数值通过微小的 Scale 被完美地保留了下来。

### 总结分析表

| 特性 | Per-Tensor | Per-Channel / Per-Token | Per-Group (Block-wise) |
| :--- | :--- | :--- | :--- |
| **粒度** | 极粗 (整个矩阵) | 中等 (行或列) | 极细 (小块，如128个) |
| **Scale 数量** | 1 个 | $N$ (维度大小) | $N \times M / \text{BlockSize}$ |
| **离群值容忍度** | **极差** (一点坏汤) | **良好** (行/列隔离) | **极好** (局部隔离) |
| **计算效率** | 最高 (矩阵乘法最快) | 高 (标准操作) | 较低 (需复杂的 Kernel 支持) |
| **LLM 现状** | 基本不用 (或仅用于KV Cache) | **权重/激活的标准做法** | **高性能权重压缩 (GPTQ/AWQ)** |

### 为什么 LLM 必须关注这些？

LLM 涌现出了 **Outlier Features** 现象（如论文 *LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale* 中指出的）。在 6B 参数以上的模型中，激活值中会出现幅度比其他值大 100 倍的通道。

*   如果是 **Per-Tensor**，这些 Outliers 会摧毁整个推理精度。
*   如果是 **Per-Channel (Weight) + Per-Token (Activation)**，或者更激进的 **Per-Group**，我们就能在保留这些 Outliers 的同时，不牺牲普通数值的精度。

这就是为什么像 `bitsandbytes` (LLM.int8), `GPTQ`, `AWQ` 这些技术虽然都是 Int4/Int8，但效果远好于朴素的 `round()` 量化的原因：它们在数学上选择了更合理的粒度来隔离误差。