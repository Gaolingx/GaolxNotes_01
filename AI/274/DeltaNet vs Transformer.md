# DeltaNet vs Transformer

### ğŸ§‘â€ğŸ’» User

gated deltanet(manba)å¯¹æ¯”transformeræœ‰ä»€ä¹ˆåŒºåˆ«å’Œåœ¨å®ç°linear attentionçš„ä¼˜åŠ¿ï¼Ÿ

---

### ğŸ¤– Assistant

ä¸‹é¢æŠŠ **Gated DeltaNetï¼ˆManbaï¼‰** ä¸ **Transformerï¼ˆå°¤å…¶æ˜¯æ ‡å‡†è‡ªæ³¨æ„åŠ› + Linear Attention æ–¹æ³•ï¼‰** åšä¸€ä¸ªç³»ç»ŸæŠ€æœ¯å¯¹æ¯”ï¼Œå¹¶è§£é‡Šå®ƒåœ¨ **é«˜æ•ˆ Linear Attention å®ç°** ä¸­çš„å…³é”®ä¼˜åŠ¿ã€‚

---

# ğŸŒŸ 1. æœ¬è´¨ç»“æ„åŒºåˆ«ï¼šDelta è¡¨è¾¾ vs è‡ªæ³¨æ„åŠ›è¡¨è¾¾

## **Transformerï¼šåŸºäº Attention çš„å…³è”å»ºæ¨¡**

* æ ¸å¿ƒæ˜¯ï¼š
  [
  \mathrm{Attn}(Q,K,V)=\mathrm{softmax}(QK^\top / \sqrt{d}) V
  ]
* è®¡ç®—å¤æ‚åº¦ï¼š

  * æ ‡å‡† Attention = **O(NÂ²)**
  * Linear Attention = **O(NÂ·d)**ï¼ˆä½†éœ€è¦ kernel trick é€¼è¿‘ softmaxï¼‰

Transformer çš„è¡¨è¾¾æ–¹å¼æ˜¯ä¾èµ– pairwise ç›¸ä¼¼åº¦ï¼ˆQKáµ€ï¼‰ï¼Œæ˜¯ä¸€ç§ **å…¨å±€çš„äºŒå…ƒå…³è”å‡½æ•°**ã€‚

---

## **DeltaNetï¼ˆManbaï¼‰ï¼šåŸºäºåºåˆ—å¢é‡æ›´æ–°ï¼ˆÎ”ï¼‰**

* Manba æ˜¯ä¸€ç§ **åŠ¨æ€çŠ¶æ€æ›´æ–°ï¼ˆstate-space-likeï¼‰æ¶æ„ + gating**ã€‚
* æ¯ä¸ª token ä¸è®¡ç®— QK å…³ç³»ï¼Œè€Œæ˜¯ï¼š
  [
  h_t = g_t \odot \sigma(W x_t) + (1-g_t)\odot h_{t-1}+ \text{delta update}
  ]
* å®ƒæ˜¯ **å•åºåˆ—é€’æ¨æ¨¡å¼**ï¼ˆunidirectional recurrenceï¼‰ï¼š

  * ç±»ä¼¼ SSMï¼ˆå¦‚ S4ã€Mamba/Mamba2ï¼‰
  * ä½†æ›´ç®€å•ï¼ˆæ²¡æœ‰å¤æ•° SSMï¼‰ï¼Œæ›´åƒå¢é‡è®°å¿†å™¨

å…³é”®ç‚¹ï¼š**DeltaNet ç”¨ Î”(h) çš„è¡¨è¾¾èƒ½åŠ›æ›¿ä»£äº† QKáµ€ çš„ pairwise attention è¡¨è¾¾èƒ½åŠ›ã€‚**

---

# ğŸŒŸ 2. ä¸ºä»€ä¹ˆ DeltaNet æ›´é€‚åˆ Linear Attention ï¼Ÿ

### Transformer Linear Attention çš„ç—›ç‚¹ï¼ˆæ ¸æŠ€å·§çš„é™åˆ¶ï¼‰

Softmax attention è¦åšçº¿æ€§åŒ–ï¼Œéœ€è¦ï¼š
[
\mathrm{softmax}(QK^\top)=\phi(Q)(\phi(K)^\top V)
]
å¿…é¡»ä¾èµ– **æ ¸å‡½æ•° Ï†(Â·)**ï¼ˆå¦‚ FAVOR+ï¼Œrandom feature kernelï¼‰ã€‚

**é—®é¢˜ï¼š**

1. kernel approximation ä¼šæŸå¤±ç²¾åº¦
2. æŸäº› attention patternï¼ˆå¦‚ sharp peaksï¼‰éš¾é€¼è¿‘
3. KV accumulation éœ€è¦å¤æ‚æ€§ï¼šprefix KV scanï¼ˆéœ€è¦éšè—çŠ¶æ€çš„çŸ©é˜µç´¯åŠ ï¼‰
4. å®ç°ä¸æ˜“ï¼šéœ€è¦ç‰¹æ®Š CUDA kernelï¼ˆflash-likeï¼‰

---

### DeltaNet çš„ä¼˜åŠ¿ï¼šå¤©ç„¶æ˜¯çº¿æ€§é€’æ¨ï¼ˆO(N)ï¼‰

DeltaNetï¼ˆManbaï¼‰ä¸­çš„ state æ›´æ–°æ˜¯ï¼š
[
h_t = f(h_{t-1}, x_t)
]

è¿™æ˜¯ **ä¸¥æ ¼ O(N)** çš„å•å‘é€’æ¨ç»“æ„ï¼š

* æ— éœ€ kernel trick
* æ— éœ€ softmax
* æ— éœ€ QKáµ€
* ä¸éœ€è¦ç»´æŒå¤§é‡ KV ç¼“å­˜
* å¯ä»¥ç›´æ¥ **prefix scan / streaming**
* å®ç°æ¯” transformer-linear-attention æ›´ç®€å•

å› ä¸ºå®ƒçš„è¡¨è¾¾èƒ½åŠ›æ¥è‡ª **å¢é‡ state å˜åŒ– Î”(h)**ï¼Œè€Œé pairwise attentionã€‚

æ‰€ä»¥å®ƒå¤©ç„¶å°±æ˜¯ â€œlinear attentionâ€ï¼ˆä¸¥æ ¼çº¿æ€§ï¼‰ã€‚

---

# ğŸŒŸ 3. Gated DeltaNetï¼ˆManbaï¼‰ç»“æ„ç›¸æ¯” Transformer çš„å…³é”®ä¼˜åŠ¿

| æ–¹é¢          | Transformer         | DeltaNet / Manba          |
| ----------- | ------------------- | ------------------------- |
| å¤æ‚åº¦         | O(NÂ²) æˆ–çº¿æ€§è¿‘ä¼¼         | **ä¸¥æ ¼ O(N)**               |
| ä¾èµ–          | QKáµ€ ç›¸ä¼¼åº¦             | **çŠ¶æ€å¢é‡ Î”**                |
| Softmax     | å¿…é¡»                  | ä¸éœ€è¦                       |
| KV Cache    | éœ€è¦ï¼ŒO(NÂ·d)           | **ä¸éœ€è¦ï¼ˆåªç»´æŠ¤ hï¼‰**            |
| æ¨ç†å†…å­˜        | é«˜                   | éå¸¸ä½                       |
| é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›      | ä¸ç¨³å®šï¼Œå¯èƒ½å´©             | **ç¨³å®šæ€§æé«˜ï¼Œå¤©ç”Ÿé€‚åˆé•¿åºåˆ—**         |
| CUDA kernel | å¤æ‚                  | ç®€å•ï¼ˆå‰ç¼€é€’æ¨å³å¯ï¼‰                |
| ç“¶é¢ˆ          | Attention bandwidth | **å‰ç¼€ scan compute-bound** |

---

# ğŸŒŸ 4. â€œGatedâ€ çš„æ„ä¹‰ï¼ˆä¸ºä»€ä¹ˆ Manba > Mamba > SSMï¼‰

DeltaNet çš„æ ¸å¿ƒè®¾è®¡ï¼š

### **1. Gatingï¼šæ§åˆ¶ Î” çš„å¼ºåº¦**

[
h_t = h_{t-1} + g_t \cdot \Delta_t
]

gated ç»“æ„å¸¦æ¥ä¸¤ä¸ªå¥½å¤„ï¼š

#### **â‘  é˜²æ¢¯åº¦çˆ†ç‚¸**

* Î”(h) çš„å¤§å°å¯ä»¥è¢« gate è°ƒæ•´
* è§£å†³çº¯ Delta RNN çš„ä¸ç¨³å®šé—®é¢˜
* è®­ç»ƒè¡Œä¸ºç±»ä¼¼äº GRU / RNN+SSM

#### **â‘¡ æå‡è¡¨è¾¾èƒ½åŠ›**

Transformer çš„ attention å…·æœ‰éçº¿æ€§ gatingï¼ˆsoftmax æƒé‡ï¼‰ã€‚
DeltaNet é€šè¿‡ gate æ¨¡æ‹Ÿ â€œsoftmax é€‰é€šæ•ˆæœâ€ã€‚

**â†’ åœ¨ä¸ä½¿ç”¨ Attention çš„å‰æä¸‹å®ç° Transformer çº§åˆ«çš„ gating æ•ˆæœ**

---

# ğŸŒŸ 5. ä¸ºä»€ä¹ˆ Manba æ“…é•¿é•¿ä¸Šä¸‹æ–‡ï¼ˆLinear Attention æœ€å¤§çš„ç—›ç‚¹ä¹‹ä¸€ï¼‰

Transformer åœ¨è¶…é•¿åºåˆ—ä¸Šï¼š

* KV Cache çˆ†å†…å­˜
* Softmax ä¸ç¨³å®š
* æ³¨æ„åŠ›èŒƒæ•°æ‰©æ•£
* éœ€è¦ RoPE extrapolationï¼ˆYARNï¼ŒNTKï¼Œhyena ropeï¼‰

Manba ä¸ä¾èµ– QK å¯¹æ¯”ï¼š

* é€’æ¨ç»“æ„ä¿è¯èŒƒæ•°ç¨³å®š
* å†…éƒ¨åªå­˜å‚¨ä¸€ä¸ªéšçŠ¶æ€ hï¼ˆç±»ä¼¼ RNNï¼‰
* Î” æ›´æ–°å…·å¤‡åŠ æ€§ç»“æ„
* ä¸éœ€è¦è®¡ç®— O(NÂ²)

å› æ­¤åœ¨ 1M+ token é•¿ä¸Šä¸‹æ–‡ä¸­æ€§èƒ½æå¼ºã€‚

---

# ğŸŒŸ 6. ä¸ºä»€ä¹ˆ DeltaNet / Manba å¯ä»¥æ›¿ä»£ Linear Attentionï¼Ÿ

æ€»ç»“å…³é”®ç‚¹ï¼š

### **Transformerï¼šéœ€è¦ç‰¹æ®Š tricks æ‰èƒ½çº¿æ€§åŒ–**

* kernel trick
* approximation
* flash kernel
* prefix-sum ä¸­é—´ tensor å¤§
* memory bandwidth bottleneck

### **Manbaï¼šç»“æ„æœ¬èº«å°±æ˜¯çº¿æ€§çš„**

* ä¸¥æ ¼ O(N)
* ä¸éœ€è¦è¿‘ä¼¼
* å†…å­˜å ç”¨éšåºåˆ—é•¿åº¦å‡ ä¹ä¸å¢é•¿
* kernel ç»“æ„ç®€å•ï¼ˆrnn-like scanï¼‰
* compute-bound â†’ åˆ©äº GPU è®¡ç®—å•å…ƒåˆ©ç”¨ç‡

å› æ­¤ **åœ¨çœŸæ­£é•¿ä¸Šä¸‹æ–‡å’Œé«˜æ•ˆè®­ç»ƒä¸­ï¼ŒManba > Transformer Linear Attention**ã€‚

---

# ğŸŒŸ 7. æ€»ç»“ï¼ˆæœ€ç²¾ç®€ç‰ˆæœ¬ï¼‰

### **Transformerï¼ˆSelf-Attentionï¼‰**

* ä¾§é‡ pairwise token äº¤äº’
* å¤æ‚åº¦é«˜
* Linear attention éœ€è¦ kernel é€¼è¿‘ softmax

### **Gated DeltaNetï¼ˆManbaï¼‰**

* åŸºäº Î”(h) çš„å¢é‡é€’æ¨
* å¤©ç„¶ Linearï¼ˆä¸¥æ ¼ O(N)ï¼‰
* ä¸éœ€è¦ softmaxã€ä¸éœ€è¦ QK
* ç”¨ gating æ¨¡æ‹Ÿæ³¨æ„åŠ›çš„é€‰é€šèƒ½åŠ›
* æ›´ç¨³å®šã€æ›´é«˜ååã€æ›´èƒ½è·‘è¶…é•¿åºåˆ—

**â†’ å®ƒä¸æ˜¯ç”¨ approximation å»è¿‘ä¼¼ Attention
è€Œæ˜¯å®Œå…¨æ¢ä¸€ä¸ªæœºåˆ¶å®ç° Attention çš„åŠŸèƒ½ã€‚**

---

# å¦‚æœä½ éœ€è¦ï¼Œæˆ‘å¯ä»¥ç»§ç»­æ·±å…¥ï¼š

* ç»™ä½  Manba å®ç°çš„é«˜æ•ˆ CUDA kernel åŸç†åˆ†æï¼ˆscan + parallel prefixï¼‰
* Manba åœ¨æ¨ç†é˜¶æ®µä¸ºä»€ä¹ˆå‡ ä¹ä¸éœ€è¦ KV cache
* Manba å¯¹ MoE / FFN çš„èåˆç­–ç•¥
* Manba å’Œ Mamba2ã€RWKV6 çš„æ¯”è¾ƒ
* Manba å¦‚ä½•åœ¨ä½ æ­£åœ¨è°ƒè¯•çš„ AVX2 CPU ä¸Šä¼˜åŒ–ï¼ˆç‰¹åˆ«æ˜¯ Î” æ›´æ–°å‘é‡åŒ–ï¼‰

ä½ æƒ³ç»§ç»­å“ªä¸ªæ–¹å‘ï¼Ÿ
