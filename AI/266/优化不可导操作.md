# 优化不可导操作

### 🧑‍💻 User

深度学习中的绝大多数模型都是靠求导梯度下降来进行参数更新. 但是如果遇到不可求导的操作该怎么办? 这时候如何优化我们的模型呢？

---

### 🤖 Assistant

在深度学习中，如果计算图中包含不可导（non-differentiable）的操作（例如：取整 `round`、取最大值索引 `argmax`、阶跃函数 `step`、离散采样 `sampling` 或调用外部不可导的模拟器），梯度的反向传播会被截断，导致无法使用标准的随机梯度下降（SGD）来更新不可导操作之前的参数。

解决这个问题通常有以下几类主流方法：

### 1. 连续松弛 (Continuous Relaxation)

这是最常见的方法，思路是将“硬”的不可导操作替换为近似的“软”可导操作。

*   **Softmax 代替 Argmax:**
    `argmax` 是离散的，无法求导。我们可以使用带**温度参数（Temperature, $\tau$）**的 Softmax 来近似。
    $$ \text{softmax}(x_i / \tau) = \frac{e^{x_i / \tau}}{\sum_j e^{x_j / \tau}} $$
    当 $\tau \to 0$ 时，分布趋近于 One-hot 向量（即近似 Argmax）；当 $\tau \to \infty$ 时，分布趋向均匀分布。

*   **Sigmoid/Tanh 代替 Step Function:**
    阶跃函数（大于0为1，小于0为0）在0处不可导，其余处梯度为0。可以用 Sigmoid 函数（通常乘以一个大的系数来增加陡峭度）来模拟。

### 2. 直通估计器 (Straight-Through Estimator, STE)

这是一种工程上的“欺骗”技巧。
*   **原理：** 在**前向传播**（Forward Pass）时，我们仍然执行不可导的操作（例如量化、二值化、取整）；但在**反向传播**（Backward Pass）时，我们假定该操作是恒等映射（Identity Mapping）或者某种平滑函数的导数，直接将梯度“透传”过去。
*   **应用：**
    *   **量化神经网络 (Quantized NN)：** 权重的量化操作是阶梯状的，STE 允许梯度穿过量化层更新浮点数权重。
    *   **VQ-VAE (Vector Quantized VAE)：** 其中的 Codebook 查找操作使用了 STE 将梯度从解码器传回编码器。

    **代码示意：**
    ```python
    class STE(torch.autograd.Function):
        @staticmethod
        def forward(ctx, input):
            # 前向传播：执行不可导操作（如取整）
            return input.round()

        @staticmethod
        def backward(ctx, grad_output):
            # 反向传播：直接透传梯度（视为 Identity）
            return grad_output
  
    # 使用
    x_discrete = STE.apply(x_continuous)
    ```

### 3. 重参数化技巧 (Reparameterization Trick)

如果不可导性来源于**随机采样**（例如从高斯分布或分类分布中采样），我们需要将随机性从参数中剥离出来。

*   **高斯分布 (VAE中的应用):**
    直接对 $z \sim N(\mu, \sigma^2)$ 采样不可导。
    **做法：** 引入辅助噪声 $\epsilon \sim N(0, 1)$，令 $z = \mu + \sigma \cdot \epsilon$。这样 $z$ 对 $\mu$ 和 $\sigma$ 就可导了。

*   **Gumbel-Softmax (针对离散分类分布):**
    如果你需要从分类分布中采样一个离散类别（Categorical Sampling），这是不可导的。
    **做法：** 使用 Gumbel-Softmax 分布。
    $$ y_i = \frac{\exp((\log(\pi_i) + g_i) / \tau)}{\sum_{j=1}^K \exp((\log(\pi_j) + g_j) / \tau)} $$
    其中 $g_i$ 是从 Gumbel(0, 1) 分布中采样的噪声。这就把离散采样转化为了可微的计算图，允许梯度回传到概率 $\pi$ 上。

### 4. 强化学习 (Reinforcement Learning) - 策略梯度

如果不可导操作是一个黑盒（Black Box），或者系统的输出必须是离散的（如生成文本时的选词、Hard Attention），我们可以将该模块视为一个**Agent**，将参数视为**Policy**。

*   **REINFORCE (Score Function Estimator):**
    我们不直接计算输出对参数的梯度，而是利用对数导数技巧：
    $$ \nabla_\theta J(\theta) = \mathbb{E}_{x \sim p_\theta} [R(x) \nabla_\theta \log p_\theta(x)] $$
    其中 $R(x)$ 是奖励函数（Loss的相反数）。
*   **优缺点：** 这种方法数学上是无偏的，但**方差极高**，训练很难收敛。通常需要结合 **Baseline**（减小方差）或者 **Actor-Critic** 方法使用。

### 5. 替代梯度 / 梯度估计 (Surrogate Gradients / Numerical Differentiation)

当无法获得解析梯度时，可以用数值方法估计。

*   **有限差分 (Finite Differences):**
    $$ f'(x) \approx \frac{f(x + \epsilon) - f(x - \epsilon)}{2\epsilon} $$
    这种方法计算成本极高（需要多次前向传播），对于高维参数空间（深度学习模型）通常不可行，但在参数量极少时可用。

### 6. 无导数优化 (Derivative-Free Optimization)

如果整个模型都不可导，或者梯度信息完全不可靠，可以使用进化算法。

*   **进化策略 (Evolution Strategies, ES):** 如 OpenAI 曾证明在某些强化学习任务中，简单的 ES 可以媲美复杂的梯度算法。
*   **遗传算法 (Genetic Algorithms, GA):** 通过变异和杂交来搜索参数空间。
*   **粒子群优化 (PSO):** 适用于低维参数优化。

### 总结：如何选择？

1.  **首选：** 如果能用 **Reparameterization (如 VAE, Gumbel-Softmax)** 或 **Soft Relaxation (如 Soft Attention)**，优先使用。这通常能保留梯度的准确性，收敛最快。
2.  **次选：** 如果必须进行硬离散化（如模型量化、VQ-VAE），使用 **STE (直通估计器)**。
3.  **最后手段：** 如果中间包含外部黑盒模拟器或复杂的离散决策序列，考虑 **REINFORCE (强化学习)**，但要做好调参困难的心理准备。

---

### 🧑‍💻 User

Gumbel-Softmax在moe路由训练中很常用，对于MoE模型，它的router的top-k操作是不可微分的，它如何实现梯度的反向传播？

---

### 🤖 Assistant

这是一个非常专业且关键的问题。在 Mixture of Experts (MoE) 模型中，Router（或 Gate）决定将输入 $x$ 发送给哪 $k$ 个专家（Experts）。

通常 Router 的输出计算如下：
$$ h = x \cdot W_g $$
$$ p = \text{Softmax}(h) $$
$$ \text{indices} = \text{TopK}(p, k) $$

这里的 `TopK` 操作确实是**不可导**的，因为它是一个离散的排序和索引选择过程。如果参数发生微小变化，选中的 Top-K 索引可能不会变，导致梯度为 0；或者索引突然跳变，导致梯度不连续。

然而，现有的主流 MoE 模型（如 Switch Transformer, Mixtral, DeepSeek-MoE）**并不是通过让 Top-K 操作本身变得可导**来解决这个问题的，而是通过**计算图的构建技巧**绕过了这个问题。

以下是几种主流的实现梯度反向传播的方法：

---

### 1. 标准做法：加权乘法 (Weighted Multiplication)

这是目前最主流的方法（如 Google 的 Switch Transformer, Mistral 的 Mixtral-8x7B）。

**核心思想：**
虽然我们无法对“索引的选择”求导，但我们可以对“被选中专家的权重（Gate Value）”求导。

**具体步骤：**
1.  **路由计算：** Router 计算出所有专家的概率分数 $p$。
2.  **Top-K 选择：** 选出 $k$ 个专家的索引 $indices$ 和它们对应的原始概率值 $p_{top}$。通常会对这 $k$ 个概率重新归一化（Renormalize），记为 $g(x)$。
3.  **计算输出：** 模型的输出**不仅仅是专家的输出**，而是**专家输出与路由权重的乘积**。
    $$ y = \sum_{i \in \text{TopK}} g_i(x) \cdot E_i(x) $$

**梯度如何回传？**
根据链式法则，虽然我们不能通过 $i \in \text{TopK}$ 这个离散操作回传梯度来改变“选谁”，但是因为输出 $y$ 中显式地乘以了 $g_i(x)$：
*   如果某个专家 $E_i$ 的表现好（Loss 降低），梯度会鼓励 $g_i(x)$ 变大（即让 Router 在未来更倾向于给这个专家更高的权重）。
*   梯度路径：$\text{Loss} \to y \to g_i(x) \to \text{Softmax} \to \text{Router Weights}$。

**局限性：**
这种方法只能更新**被选中**的专家的路由权重。对于那些没有被选中的专家，其权重 $g_j(x)$ 在前向传播中被置为 0，因此没有梯度回传。这会导致“富者越富”的马太效应，部分专家永远不被选中（Dead Experts）。
*   **解决方案：** 引入 **负载均衡损失 (Load Balancing Loss)** 作为辅助 Loss，强制 Router 给每个专家的概率总和趋于平均。

---

### 2. Gumbel-Softmax 技巧 (如果你想用的话)

虽然标准 MoE 常用上述的乘法技巧，但如果你确实想用 Gumbel-Softmax 来处理 Top-K 路由（使其在某种程度上更加“软”化），是可以的。

**Top-1 路由：**
直接使用标准的 Gumbel-Softmax 采样代替 Argmax。
$$ y_{soft} = \text{Softmax}((\log p + g) / \tau) $$
在前向传播中，你可能取 One-hot 向量（硬路由）；在反向传播中，使用 $y_{soft}$ 的梯度（直通估计器 STE）。

**Top-K 路由 (无需替换的一般化):**
对于 $k > 1$，标准的 Gumbel-Softmax 不适用（因为它只选 Top-1）。若要实现可微的 Top-K 采样，通常采用 **Iterative Gumbel-Softmax** 或 **Gumbel-Top-k** 技巧：
1.  进行一次 Gumbel-Softmax 采样，拿到最大的那个（Top-1）。
2.  将该类别的 Logits 设为 $-\infty$（Mask掉）。
3.  重复步骤 1，直到选出 $k$ 个。

**为何主流大模型很少用 Gumbel-Softmax 做 MoE？**
*   **计算成本：** 迭代采样计算量大。
*   **稳定性：** 温度参数 $\tau$ 很难调，且在大规模模型中 Gumbel 引入的随机性可能导致训练不稳定。
*   **效果：** 简单的“加权乘法 + 负载均衡 Loss”已被证明在大规模下足够有效且鲁棒。

---

### 3. 噪声门控 (Noisy Top-K Gating)

这是 Google 在《Outrageously Large Neural Networks》(2017) 中提出的经典方法，旨在改善梯度流并鼓励探索。

**原理：**
在 Logits 上加入高斯噪声，然后再做 Top-K。
$$ H(x) = x \cdot W_g + \text{StandardNormal}() \cdot \text{Softplus}(x \cdot W_{noise}) $$
$$ \text{KeepTopK}(H(x)) $$

**梯度的妙用：**
虽然 Top-K 依然不可导，但因为引入了噪声项：
1.  它让决策边界变得模糊。
2.  即使 Router 权重处于“临界点”，噪声也能让它偶尔跳到另一个专家，从而获得那个专家的梯度信息。这实际上是一种通过随机性来“平滑”不可导操作的策略。

---

### 总结：代码层面的实现逻辑

在 PyTorch 等框架中，MoE 的反向传播**并不依赖于 Top-K 的导数**，而是依赖于最后那个乘法。

```python
import torch
import torch.nn.functional as F

# 假设 router_logits: [batch_size, num_experts]
# 假设 expert_outputs: [batch_size, num_experts, hidden_dim] (实际只会计算topk个)

def moe_output(router_logits, expert_outputs, k=2):
    # 1. 计算概率
    routing_probs = F.softmax(router_logits, dim=-1)
  
    # 2. 执行不可导的 Top-K
    # indices 用于选择专家，values 用于加权
    # 这里的 indices 在 backward 时是被视为常数的
    topk_probs, topk_indices = torch.topk(routing_probs, k=k, dim=-1)
  
    # 3. 归一化选中的概率 (可选，Mixtral等模型会做)
    topk_probs = topk_probs / topk_probs.sum(dim=-1, keepdim=True)
  
    # 4. 关键步骤：加权求和
    # 我们根据 indices 选出了对应的 expert_outputs
    # 然后让 topk_probs 参与运算
  
    final_output = torch.zeros_like(x)
    for i in range(k):
        idx = topk_indices[:, i]
        prob = topk_probs[:, i].unsqueeze(-1) # 梯度将流过这里！
      
        # 选出第 i 个被选中的专家的输出
        selected_expert_out = gather_expert_output(expert_outputs, idx)
      
        # 乘法！这是梯度回传给 router_logits 的桥梁
        final_output += prob * selected_expert_out
      
    return final_output
```

**结论：** MoE 并没有让 Top-K 变得可微，而是通过**将路由概率值乘在最终结果上**，利用链式法则让梯度流过概率值（Values），从而更新 Router，同时利用辅助 Loss 解决未被选中专家的训练问题。