# 归一化技术对比

### 🧑‍💻 User

神经网络中的Batch Normalization/Group Normalization有什么作用？

---

### 🤖 Assistant

在深度学习（尤其是卷积神经网络 CNN）中，**Batch Normalization (BN)** 和 **Group Normalization (GN)** 是两种非常关键的归一化技术。它们的核心目的都是为了**加速模型收敛**并**提高训练的稳定性**。

简单来说，它们通过调整神经网络中间层输出的分布（使其均值为 0，方差为 1），解决了“Internal Covariate Shift”（内部协变量偏移）的问题。

以下是两者的详细作用、原理对比及适用场景。

---

### 1. Batch Normalization (BN, 批归一化)

BN 是由 Google 在 2015 年提出的，是目前最常用的归一化方法。

#### **核心原理**
对于形状为 $(N, C, H, W)$ 的输入特征图（$N$为Batch Size，$C$为通道数，$H, W$为高宽）：
BN 在 **Batch ($N$)** 维度上进行归一化。它会计算**同一个通道**在**所有样本**上的均值和方差。

$$
\mu_c = \frac{1}{N \cdot H \cdot W} \sum_{n,h,w} x_{n,c,h,w}
$$
$$
\sigma_c^2 = \frac{1}{N \cdot H \cdot W} \sum_{n,h,w} (x_{n,c,h,w} - \mu_c)^2
$$

然后进行归一化并缩放平移：
$$
\hat{x} = \frac{x - \mu_c}{\sqrt{\sigma_c^2 + \epsilon}}, \quad y = \gamma \hat{x} + \beta
$$
其中 $\gamma$ 和 $\beta$ 是可学习的参数，用于恢复网络的表达能力。

#### **主要作用**
1.  **加速收敛**：通过规范化每一层的输入分布，使损失函数的景观（Landscape）更加平滑，梯度传播更顺畅，从而可以使用更大的学习率。
2.  **降低对初始化的敏感度**：不再极度依赖参数的随机初始化方式。
3.  **防止梯度消失/爆炸**：特别是在深层网络中，限制了激活值的范围。
4.  **轻微的正则化效果**：由于训练时使用的是当前 Batch 的均值和方差（带有噪声），这引入了随机性，类似 Dropout，有助于防止过拟合。

#### **缺点**
*   **依赖 Batch Size**：如果 Batch Size 太小（例如 $<4$），样本的统计量（均值/方差）不能代表全局分布，会导致模型性能剧烈下降。
*   **训练与推理不一致**：训练时用 Batch 统计量，推理时用移动平均（Running Mean/Var）统计量，这在某些特定场景（如生成模型）可能导致问题。
*   **RNN 中表现不佳**：因为 RNN 的序列长度不一致，BN 很难应用。

---

### 2. Group Normalization (GN, 组归一化)

GN 是由 Kaiming He 等人在 2018 年提出的，主要为了解决 BN 在小 Batch Size 下性能失效的问题。

#### **核心原理**
GN **不再依赖 Batch 维度**。它将通道 $C$ 分成 $G$ 个组（Group），在**每个样本内部**计算每组的均值和方差。

假设我们将通道分为 $G$ 组，每组有 $C/G$ 个通道。GN 在 $(C/G, H, W)$ 维度上计算统计量。

*   如果是 **Instance Normalization (IN)**：$G = C$（每组 1 个通道）。
*   如果是 **Layer Normalization (LN)**：$G = 1$（所有通道 1 组）。
*   **GN** 是 LN 和 IN 的折中方案。

#### **主要作用**
1.  **解耦 Batch Size**：计算均值和方差时不涉及 $N$ 维度。因此，**无论 Batch Size 是 1 还是 1024，GN 的效果是一样的**。
2.  **适用于小批量场景**：在显存受限的任务中（如高分辨率图像分割、物体检测、3D 医疗影像），Batch Size 往往很小（如 1 或 2），此时 GN 表现远优于 BN。
3.  **推理与训练一致**：因为计算仅依赖当前样本，训练和推理时的计算方式完全相同，不需要维护 Running Mean/Var。

#### **缺点**
*   **在大 Batch Size 下略逊于 BN**：如果显存足够，Batch Size 很大，BN 的正则化效果通常会让其精度略高于 GN。

---

### 3. 直观对比与总结

为了理解两者的区别，我们可以想象一个数据立方体 $(N, C, H, W)$。

<details>
<summary><strong>📐 点击展开：归一化维度的数学可视化</strong></summary>

假设输入特征图为 $x$。

*   **Batch Norm (BN)**:
    在 $(N, H, W)$ 维度求平均，保留 $C$ 维度。
    *   统计量个数：$C$ 对 $(\mu, \sigma)$。
    *   **一句话：每个通道都有自己的一套均值和方差，跨越整个 Batch。**

*   **Layer Norm (LN)**:
    在 $(C, H, W)$ 维度求平均，保留 $N$ 维度。
    *   统计量个数：$N$ 对 $(\mu, \sigma)$。
    *   **一句话：每个样本都有自己的一套均值和方差，跨越所有通道。**

*   **Instance Norm (IN)**:
    在 $(H, W)$ 维度求平均，保留 $(N, C)$ 维度。
    *   统计量个数：$N \times C$ 对 $(\mu, \sigma)$。
    *   **一句话：每个样本的每个通道独立归一化（常用于风格迁移）。**

*   **Group Norm (GN)**:
    在 $(C/G, H, W)$ 维度求平均。
    *   统计量个数：$N \times G$ 对 $(\mu, \sigma)$。
    *   **一句话：每个样本将通道分组，组内进行归一化。**

</details>

#### **对比表格**

| 特性 | Batch Normalization (BN) | Group Normalization (GN) |
| :--- | :--- | :--- |
| **归一化维度** | Batch ($N$), Height, Width | Channel Group ($C/G$), Height, Width |
| **依赖 Batch Size** | **是** (严重依赖) | **否** (完全独立) |
| **训练/推理一致性** | 不一致 (训练用 Batch 统计，推理用全局统计) | **一致** (始终使用当前样本统计) |
| **最佳场景** | 图像分类、Batch Size 较大 (>16) 的任务 | 目标检测、语义分割、高分辨率图像、小 Batch Size |
| **主要优点** | 训练速度快，大 Batch 下精度最高 | 稳定性好，不受 Batch Size 限制 |
| **典型应用** | ResNet, VGG, Inception | Mask R-CNN, FCOS, 3D CNN |

### 4. 应该如何选择？

*   **首选 BN**：如果你做的是标准的图像分类，且显存允许 Batch Size 较大（例如每张卡 $\ge 32$），BN 通常是首选，因为它的效果经过了最广泛的验证且自带正则化。
*   **改用 GN**：如果你做的是**物体检测、分割、视频处理**，由于输入分辨率很高，导致每张显卡只能跑 1-2 个样本（Small Batch Size），此时 BN 会失效，必须使用 GN。
*   **Transformer/RNN**：通常不使用 BN 或 GN，而是使用 **Layer Normalization (LN)**，因为 NLP 数据的长度是变化的，且 Batch 间的统计量意义不大。