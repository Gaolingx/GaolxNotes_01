# ä½¿ç”¨Condaåˆ›å»ºè™šæ‹Ÿç¯å¢ƒè¿è¡Œ7Bæ¨¡å‹

### ğŸ§‘â€ğŸ’» User

å¦‚ä½•ç”¨condaåˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼Œå®‰è£…vllmè¿è¡Œä¸€ä¸ª7bæ¨¡å‹ï¼Ÿ

---

### ğŸ¤– Assistant

è¦ä½¿ç”¨ Conda åˆ›å»ºè™šæ‹Ÿç¯å¢ƒå¹¶å®‰è£… vLLM è¿è¡Œ 7B æ¨¡å‹ï¼Œè¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š

---

### **æ­¥éª¤ 1ï¼šåˆ›å»º Conda è™šæ‹Ÿç¯å¢ƒ**
```bash
conda create -n vllm_env python=3.9 -y  # æ¨è Python 3.9
conda activate vllm_env
```

---

### **æ­¥éª¤ 2ï¼šå®‰è£… PyTorch (CUDA ç‰ˆæœ¬)**
```bash
# æ ¹æ®ä½ çš„ CUDA ç‰ˆæœ¬é€‰æ‹©å®‰è£…å‘½ä»¤ (ç¤ºä¾‹ä¸º CUDA 11.8)
pip3 install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118
```

> æŸ¥çœ‹ CUDA ç‰ˆæœ¬ï¼š`nvidia-smi`ï¼ˆå³ä¸Šè§’æ˜¾ç¤ºç‰ˆæœ¬ï¼Œå¦‚ `12.x` éœ€å¯¹åº”å®‰è£… `cu121`ï¼‰

---

### **æ­¥éª¤ 3ï¼šå®‰è£… vLLM**
```bash
pip install vllm  # è‡ªåŠ¨å®‰è£…ä¾èµ–
```
> éªŒè¯å®‰è£…ï¼š`python -c "from vllm import LLM; print('Success')"`

---

### **æ­¥éª¤ 4ï¼šè¿è¡Œ 7B æ¨¡å‹**
#### **æ–¹å¼ 1ï¼šä½¿ç”¨ Hugging Face æ¨¡å‹ï¼ˆå¦‚ `mistralai/Mistral-7B-v0.1`ï¼‰**
```python
from vllm import LLM, SamplingParams

# åˆå§‹åŒ–æ¨¡å‹å’Œé‡‡æ ·å‚æ•°
llm = LLM(model="mistralai/Mistral-7B-v0.1")  # é¦–æ¬¡è¿è¡Œè‡ªåŠ¨ä¸‹è½½æ¨¡å‹
sampling_params = SamplingParams(temperature=0.7, max_tokens=100)

# è¾“å…¥æç¤º
prompts = ["Explain what AI is in 3 sentences:"]
outputs = llm.generate(prompts, sampling_params)

# æ‰“å°ç»“æœ
for output in outputs:
    print(f"Prompt: {output.prompt}")
    print(f"Generated: {output.outputs[0].text}\n")
```

#### **æ–¹å¼ 2ï¼šä½¿ç”¨æœ¬åœ°æ¨¡å‹**
1. æå‰ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ï¼š
   ```bash
   huggingface-cli download mistralai/Mistral-7B-v0.1 --local-dir ./mistral-7b
   ```
2. ä¿®æ”¹ä»£ç åŠ è½½è·¯å¾„ï¼š
   ```python
   llm = LLM(model="./mistral-7b")  # æŒ‡å‘æœ¬åœ°ç›®å½•
   ```

---

### **æ­¥éª¤ 5ï¼šæ‰§è¡Œè„šæœ¬**
```bash
python run_vllm.py  # ä¿å­˜ä»£ç ä¸º run_vllm.py
```

---

### **å¸¸è§é—®é¢˜è§£å†³**
1. **CUDA ç‰ˆæœ¬ä¸åŒ¹é…**ï¼š
   - ç¡®ä¿ `nvidia-smi` æ˜¾ç¤ºçš„é©±åŠ¨ç‰ˆæœ¬æ”¯æŒå®‰è£…çš„ CUDA ç‰ˆæœ¬ã€‚
   - é‡æ–°å®‰è£…åŒ¹é…çš„ PyTorchï¼ˆå¦‚ CUDA 11.8 ç”¨ `cu118`ï¼‰ã€‚

2. **å†…å­˜ä¸è¶³**ï¼š
   - 7B æ¨¡å‹éœ€è¦çº¦ **15GB GPU æ˜¾å­˜**ã€‚
   - å¯ç”¨é‡åŒ–ï¼ˆä¿®æ”¹ `LLM` åˆå§‹åŒ–ï¼‰ï¼š
     ```python
     llm = LLM(model="mistralai/Mistral-7B-v0.1", quantization="awq")  # éœ€æå‰è½¬æ¢æ¨¡å‹
     ```

3. **ä¸‹è½½æ¨¡å‹å¤±è´¥**ï¼š
   - ä½¿ç”¨å›½å†…é•œåƒæºï¼š
     ```bash
     export HF_ENDPOINT=https://hf-mirror.com
     huggingface-cli download --resume-download mistralai/Mistral-7B-v0.1 --local-dir ./mistral-7b
     ```

---

### **å®Œæ•´ç¤ºä¾‹ä»£ç **
```python
# run_vllm.py
from vllm import LLM, SamplingParams

# é…ç½®æ¨¡å‹å’Œç”Ÿæˆå‚æ•°
llm = LLM(
    model="mistralai/Mistral-7B-v0.1",  # æˆ–æœ¬åœ°è·¯å¾„ "./mistral-7b"
    tensor_parallel_size=1,            # å¤š GPU å¹¶è¡Œæ•°é‡
)
sampling_params = SamplingParams(
    temperature=0.8,
    top_p=0.95,
    max_tokens=128,
)

# ç”Ÿæˆæ–‡æœ¬
prompts = [
    "æ³•å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œï¼Ÿ",
    "How to bake a chocolate cake?",
]
outputs = llm.generate(prompts, sampling_params)

# è¾“å‡ºç»“æœ
for i, output in enumerate(outputs):
    print(f"Prompt {i+1}: {output.prompt}")
    print(f"Result {i+1}: {output.outputs[0].text.strip()}\n{'='*50}")
```

---

### **å¤‡æ³¨**
- **ç¡¬ä»¶è¦æ±‚**ï¼šNVIDIA GPUï¼ˆæ¨èæ˜¾å­˜ â‰¥ 16GBï¼‰ï¼Œéœ€å®‰è£… NVIDIA é©±åŠ¨ã€‚
- **æ¨¡å‹é€‰æ‹©**ï¼šæ›¿æ¢ `mistralai/Mistral-7B-v0.1` ä¸ºå…¶ä»– 7B æ¨¡å‹ï¼ˆå¦‚ `meta-llama/Llama-2-7b-chat-hf`ï¼Œéœ€ç™»å½• Hugging Faceï¼‰ã€‚
- **æ€§èƒ½ä¼˜åŒ–**ï¼šå¤šå¡ç”¨æˆ·å¯å¢åŠ  `tensor_parallel_size`ï¼ˆå¦‚ 2 å¡è®¾ä¸º `2`ï¼‰ã€‚

é€šè¿‡ä»¥ä¸Šæ­¥éª¤ï¼Œå³å¯åœ¨ Conda è™šæ‹Ÿç¯å¢ƒä¸­é«˜æ•ˆè¿è¡Œ 7B æ¨¡å‹ã€‚