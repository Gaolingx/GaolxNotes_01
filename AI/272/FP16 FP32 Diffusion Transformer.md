# FP16 FP32 Diffusion Transformer

### 🧑‍💻 User

为什么如今fp16甚至fp8精度在机器学习领域（尤其是transformer）广泛使用，diffusion模型的vae还是需要fp32精度训练？

---

### 🤖 Assistant

这是一个非常深刻且触及底层数值计算原理的问题。

确实，在Transformer（如LLM）和Diffusion模型的UNet/DiT部分，FP16（半精度）甚至FP8（8位浮点）已经成为标配，因为它们能显著减少显存占用并加速计算，同时对模型性能影响微乎其微。

然而，**VAE（变分自编码器）** 作为Latent Diffusion Model（如Stable Diffusion）的“眼睛”和“画笔”，对数值精度极为敏感，训练时通常必须使用FP32，甚至在推理时如果强制使用FP16也经常会出现图像崩坏（如变全黑、色彩断层）。

造成这种差异的核心原因主要集中在以下几点：

### 1. 统计量的数值稳定性 (Numerical Stability of Statistics)

VAE（特别是KL-VAE）的核心机制涉及对概率分布的建模。

*   **KL散度计算：** 在训练阶段，Encoder需要预测潜在变量的**均值（Mean）**和**方差的对数（Log-Variance）**。为了采样（Reparameterization trick），需要执行指数运算 $e^{\text{log\_var}}$ 来获取标准差。
*   **溢出与下溢（Overflow/Underflow）：** `exp`（指数）和 `pow`（幂）是数值上非常激进的操作。FP16的最大表示范围只有 $65504$，且在接近0的地方精度很低。
    *   如果方差估计稍大，取指数后容易在FP16下溢出变成 `inf`（无穷大）。
    *   如果数值极小，容易下溢变成 `0`。
    *   一旦出现 `inf` 或 `NaN`（Not a Number），整个梯度传播就会炸毁，导致模型训练失败。FP32拥有更大的动态范围，能容纳这些激进的数学变换。

### 2. GroupNorm 与 InstanceNorm 的精度累积

CNN架构（VAE主要是卷积网络）中广泛使用的归一化层（Group Normalization）依赖于计算特征图的均值和方差。

*   **方差计算公式：** $\sigma^2 = \frac{1}{N}\sum(x_i - \mu)^2$。
*   **精度灾难：** 当 $x_i$ 和 $\mu$ 非常接近时，减法操作会产生“相减抵消”（Catastrophic Cancellation），导致有效数字大量丢失。在FP16下，这种精度丢失会被平方操作放大，导致计算出的方差为0或出现巨大误差。
*   **结果：** 归一化层失效，激活值瞬间爆炸，输出图像出现伪影或直接变黑/变花。

### 3. VAE作为“压缩/解压”模块的特殊地位

在Latent Diffusion架构中，UNet/Transformer 和 VAE 的分工不同：

*   **UNet (FP16 OK):** 负责**语义生成**和**去噪**。它是一个概率模型，预测的是“噪声”。即使预测稍微有一点数值误差（比如 0.1234 变成了 0.1230），经过多步去噪和采样，这些误差往往会被平均掉或修正，不会造成灾难性后果。
*   **VAE (Need FP32):** 负责**像素级重构**。它是确定性的（在解码阶段）。它需要把 $64 \times 64$ 的潜变量精确映射回 $512 \times 512$ 的像素空间。
    *   Latent空间的一点点微小数值偏差，在经过Decoder的上采样和非线性变换后，会在像素空间被放大成明显的视觉伪影（Artifacts），比如色彩波纹、网格状噪声或颜色失真。
    *   FP16的尾数位（Mantissa）只有10位，精度约为 $10^{-3}$ ~ $10^{-4}$，这对于需要高保真还原纹理和色彩的VAE来说，往往是不够的。

### 4. 梯度消失与梯度爆炸

VAE的训练目标通常包含两部分：**重构损失（L2或L1 Loss）** 和 **正则化损失（KL Divergence）**。

*   这两部分Loss的量级（Scale）有时差异很大。
*   在反向传播过程中，Decoder层数较深，梯度传回Encoder时，如果使用FP16，微小的梯度值很容易因为下溢直接变成0（梯度消失），或者因为某些层的数值不稳定导致梯度爆炸。
*   FP32 提供了足够的梯度精度，确保微小的调整也能被模型捕捉到。

### 5. 实战中的“黑图”问题 (The Black Image Issue)

在Stable Diffusion的早期应用中，社区广泛发现一个Bug：当开启FP16推理时，生成的图片偶尔会变成全黑或全绿。

*   **原因：** VAE解码过程中某个中间层的激活值瞬间超过了FP16的最大范围（65504），导致变为NaN。由于卷积运算的特性，一个NaN会像病毒一样传播到整张图，最终导致输出全黑。
*   **解决方案：** 即使UNet使用FP16，推理代码通常也会强制将Latents转回 FP32，然后再喂给 VAE Decoder，或者使用特定的 `Clamp` 操作限制数值范围，但这本质上证明了VAE对FP16的不兼容性。

### 总结

简单来说，**Transformer/UNet 是在“画构图”，容错率高，所以 FP16/FP8 没问题；但 VAE 是在“洗照片”，任何显影液配比（数值精度）的微小误差都会毁掉底片。**

因此，目前业界的最佳实践通常是：
*   **UNet/Transformer:** 使用 BF16 (Brain Float 16) 或 FP16 训练/推理。
*   **VAE:** 始终保持 FP32 训练；推理时尽量使用 FP32，或者使用 BF16（因为BF16保留了FP32的动态范围，比FP16更不易溢出）。

---

### 🧑‍💻 User

所以关键问题在于，vae是连续特征预测对精度要求高，transformer llm是离散特征对精度要求相对较低？

---

### 🤖 Assistant

这个直觉非常敏锐，**“离散 vs 连续”** 确实是其中一个非常核心的原因，但它还不是全部的故事。

为了彻底讲清楚这个问题，我们需要引入一个更重要的对比对象：**Diffusion UNet**。因为 Diffusion UNet 预测的也是**连续**的噪声（或者速度 v），但它却可以用 FP16。

所以，逻辑链条应该是这样的：

### 1. LLM (Transformer) vs. VAE：分类 vs. 回归

这一层就是你提到的“离散 vs 连续”的问题。

*   **LLM (分类/排序任务)：**
    *   LLM 的最终输出经过一个 `Softmax` 层。
    *   假设这里的任务是预测下一个词。
        *   FP32 计算出来的 Logits 可能是：`[猫: 10.5, 狗: 5.2, 鸟: 2.1]` -> 选“猫”。
        *   FP16 丢失精度后变成：`[猫: 10.48, 狗: 5.25, 鸟: 2.05]` -> **依然选“猫”。**
    *   **结论：** 对于离散预测，我们只在乎**相对大小（Ranking）**。只要精度损失没有大到改变排名的顺序，输出结果就是完全一致的。这种机制天然对精度噪声有极高的容忍度。

*   **VAE (回归/重构任务)：**
    *   VAE 的 Decoder 是要还原像素值。
    *   FP32 计算出像素值 `0.5000`（灰色）。
    *   FP16 偏差导致算出 `0.5050`（稍亮的灰色）。
    *   **结论：** 在回归任务中，误差是**实打实地累积**在画面上的。虽然单个像素看不出来，但全图几百万个像素的误差累积起来，加上通道之间的偏差，就会导致色偏、伪影。

---

### 2. Diffusion UNet vs. VAE：迭代修正 vs. 一锤定音

这是比“离散/连续”更深层的原因。为什么同样是预测连续数值，Diffusion UNet 敢用 FP16，而 VAE 不敢？

*   **Diffusion UNet (迭代去噪)：**
    *   Diffusion 的生成过程是几十步甚至上百步的（Denoising Steps）。
    *   假设在第 10 步，UNet 因为 FP16 精度不够，预测的噪声偏了一点点。
    *   到了第 11 步，模型会根据当前的输入再次预测。**这是一个负反馈调节的过程**。之前的微小误差，会在后续的迭代中被“平均”掉或者修正回来。
    *   **比喻：** 就像你开车（连续动作），方向盘稍微抖了一下（精度误差），你下一秒就能修正回来，车依然在直线上。

*   **VAE Decoder (单次映射)：**
    *   VAE 的解码是 **One-pass (单次前向传播)** 的。
    *   Latent $\rightarrow$ 卷积 $\rightarrow$ 上采样 $\rightarrow$ 卷积 $\rightarrow$ Image。
    *   如果在中间某一层出现了精度误差（例如上采样时的插值误差），这个误差会随着后续卷积层层传递，甚至被非线性激活函数放大，**没有任何回头的机会**。
    *   **比喻：** 就像你在打印照片，喷墨头稍微歪了一点，那整张纸的那一行就全废了，你无法在打印过程中修正它。

---

### 3. 核心算子的数值敏感度差异

除了任务性质，模型内部使用的数学算子决定了“会不会炸”。

*   **LLM / Transformer:**
    *   核心是 `MatMul` (矩阵乘法) 和 `LayerNorm`。
    *   这些操作主要涉及加法和乘法。虽然量级大，但在 BF16/FP16 范围内通常是可控的。`Softmax` 虽然有指数，但通常会对输入减去最大值（LogSumExp技巧）来保持稳定。

*   **VAE (尤其是 KL-Regularized VAE):**
    *   核心痛点在于 **KL 散度计算** 和 **高斯采样**。
    *   公式：$z = \mu + \sigma \cdot \epsilon$，其中 $\sigma = e^{0.5 \cdot \log\text{var}}$。
    *   这个 $e^{\text{something}}$ （指数运算）非常可怕。在 FP16 中，如果 `logvar` 稍微大一点，取指数后立刻溢出变成 `Infinity`；稍微小一点，立刻变成 `0`。
    *   **这种指数级的动态范围需求，是 FP16 (范围仅 $10^{-5} \sim 6 \times 10^4$) 无法满足的。**

### 总结

你的理解是对的，**离散特征（LLM）比连续特征（VAE/Diffusion）更抗造**。

但为了解释“为什么 Diffusion UNet 也是连续的却能用 FP16”，我们需要补上：
1.  **容错机制不同：** Diffusion 是多步迭代修正（容错高），VAE 是单步确定性映射（容错低）。
2.  **算子危险度不同：** VAE 涉及激进的指数分布变换，极易触碰 FP16 的数值边界。

所以，VAE 是整个生成式 AI 链路中“最娇气”的组件，必须得用 FP32 供着。