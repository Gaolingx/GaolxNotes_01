# 扩散模型预测目标线性可逆

### 🧑‍💻 User

为什么 x₀-prediction / v-prediction 与 score 也都线性可逆？

---

### 🤖 Assistant

在扩散模型（Diffusion Models）中，**$x_0$-prediction**（预测原图）、**$v$-prediction**（预测速度）、**$\epsilon$-prediction**（预测噪声）以及 **Score**（分数函数）之所以都是**线性可逆**（linearly invertible）或者是线性相关的，根本原因在于扩散过程的**前向方程是一个线性高斯变换**。

只要给定了当前的时刻 $t$（即已知的噪声系数）和当前的输入 $x_t$，这些预测目标本质上只是对同一个基本方程的不同代数变形。

以下是详细的数学推导和直观解释。

---

### 1. 核心基础：前向扩散方程

扩散模型的所有推导都始于这一个方程。假设 $x_0$ 是原始数据，$\epsilon \sim \mathcal{N}(0, I)$ 是噪声。在任意时刻 $t$，加噪后的图像 $x_t$ 可以表示为：

$$
x_t = \alpha_t x_0 + \sigma_t \epsilon
$$

其中 $\alpha_t$ 和 $\sigma_t$ 是预先定义的噪声调度参数（Noise Schedule）。在常见的 Variance Preserving (VP) 设定下，通常满足 $\alpha_t^2 + \sigma_t^2 = 1$。

**关键点：** 在训练或推理的某一步骤中，**$x_t$（网络输入）、$\alpha_t$、$\sigma_t$ 都是已知常数**。因此，上式变成了一个关于 $x_0$ 和 $\epsilon$ 的线性方程。

---

### 2. 各个预测目标的线性关系推导

我们要证明 $x_0$、$v$、Score 都可以互相线性表示。为了方便，我们通常以 $\epsilon$ 为桥梁，因为大多数推导最先是从 $\epsilon$-prediction 开始的。

#### A. $x_0$-prediction 与 $\epsilon$-prediction
从基础方程 $x_t = \alpha_t x_0 + \sigma_t \epsilon$ 中解出 $x_0$：

$$
x_0 = \frac{x_t - \sigma_t \epsilon}{\alpha_t} = \frac{1}{\alpha_t} x_t - \frac{\sigma_t}{\alpha_t} \epsilon
$$

可以看到，$x_0$ 是 $\epsilon$ 的线性函数（缩放并平移）。如果我们预测出了 $\epsilon$，就能算出 $x_0$，反之亦然。

#### B. $v$-prediction 与 $\epsilon$-prediction ($x_0$)
$v$-prediction 最早是在论文 *Progressive Distillation for Fast Sampling of Diffusion Models* 中提出的，其定义为圆周上的“速度”分量：

$$
v_t \equiv \alpha_t \epsilon - \sigma_t x_0
$$

这本身就是一个线性组合。我们可以将其展开，用纯 $\epsilon$ 或纯 $x_0$ 表示。
将 $x_0$ 的表达式代入 $v_t$：

$$
v_t = \alpha_t \epsilon - \sigma_t \left( \frac{x_t - \sigma_t \epsilon}{\alpha_t} \right) = \alpha_t \epsilon - \frac{\sigma_t}{\alpha_t} x_t + \frac{\sigma_t^2}{\alpha_t} \epsilon
$$

整理后（假设 $\alpha_t^2 + \sigma_t^2 = 1$）：
$$
v_t = \left( \alpha_t + \frac{\sigma_t^2}{\alpha_t} \right) \epsilon - \frac{\sigma_t}{\alpha_t} x_t = \frac{1}{\alpha_t} \epsilon - \frac{\sigma_t}{\alpha_t} x_t
$$

或者反过来，$v_t$ 也可以写成 $x_0$ 的线性变换：
$$
v_t = \frac{\alpha_t}{\sigma_t} x_t - \frac{1}{\sigma_t} x_0
$$

结论：$v$ 是 $x_0$ 和 $\epsilon$ 的线性组合。

#### C. Score 与 $\epsilon$-prediction
根据 **Tweedie's Formula (特威迪公式)**，高斯分布的 Score 函数（对数概率密度的梯度 $\nabla_{x_t} \log p_t(x_t)$）与去噪后的均值有直接关系。对于扩散模型，Score 定义为：

$$
\text{Score} \equiv \nabla_{x_t} \log p_t(x_t) = -\frac{\epsilon}{\sigma_t}
$$
*(注：这里 $\epsilon$ 指的是最优去噪估计)*

显然，Score 只是 $\epsilon$ 乘以一个标量系数 $-\frac{1}{\sigma_t}$。这也是完全线性的。

---

### 3. 总结与几何解释

我们可以把 $x_0$ 和 $\epsilon$ 看作二维平面上的两个正交基。
输入 $x_t$ 是这两个基的线性组合。

*   **$x_0$-prediction**: 试图直接恢复 $x_0$ 轴的分量。
*   **$\epsilon$-prediction**: 试图恢复 $\epsilon$ 轴的分量。
*   **$v$-prediction**: 本质上是旋转了坐标系。在 VP SDE 中，$(x_t, v_t)$ 实际上是 $(x_0, \epsilon)$ 旋转 $\phi_t$ 角度后的坐标（其中 $\alpha_t = \cos \phi_t, \sigma_t = \sin \phi_t$）。

**线性转换矩阵：**

如果我们将神经网络的输出记为 $f_\theta(x_t)$，那么这几个目标之间的转换矩阵如下（以预测 $\epsilon$ 为中心）：

| 目标 (Target) | 转换为 $\epsilon$ 的公式 | 说明 |
| :--- | :--- | :--- |
| $\epsilon$ | $\epsilon$ | 恒等变换 |
| $x_0$ | $\frac{x_t - \alpha_t x_0}{\sigma_t}$ | 缩放 + 平移 |
| $v$ | $\alpha_t v + \sigma_t x_t$ (近似) * | 需要结合 VP 假设 |
| Score | $-\sigma_t \cdot \text{Score}$ | 缩放 |

*\*注：$v$ 转 $\epsilon$ 的公式在 VP 假设下为 $\epsilon = \alpha_t v + \sigma_t x_t$。*

### 4. 既然是线性可逆的，为什么还要区分？

既然数学上它们是等价的，为什么研究人员会发明 $v$-prediction？为什么有时用 $\epsilon$ 有时用 $x_0$？

答案在于**数值稳定性（Numerical Stability）**和**信噪比（SNR）**。

虽然它们在代数上可逆，但在浮点数计算和神经网络优化中，**误差放大的程度不同**。

1.  **高信噪比 (High SNR, $t \to 0$)**:
    *   此时 $\sigma_t \approx 0, \alpha_t \approx 1$。
    *   $x_t \approx x_0$。
    *   如果你预测 $\epsilon$，你需要从几乎纯净的图中从头猜出那一丁点噪声，这很难且方差大。
    *   此时预测 $x_0$ 最简单（网络可以直接输出输入），或者预测 $v$ 也很稳定。

2.  **低信噪比 (Low SNR, $t \to T$)**:
    *   此时 $\sigma_t \approx 1, \alpha_t \approx 0$。
    *   $x_t \approx \epsilon$。
    *   如果你预测 $x_0$，你需要从纯噪声中凭空画出图像，这非常困难。
    *   此时预测 $\epsilon$ 最简单（网络可以直接映射输入），或者预测 $v$。

**$v$-prediction 的优势**：
$v$-prediction 的设计初衷就是为了在整个 $t \in [0, T]$ 区间内保持方差的稳定。它在 $t=0$ 时表现像 $x_0$，在 $t=T$ 时表现像 $\epsilon$，避免了另外两种预测方式在极端情况下的数值不稳定（除以接近 0 的数）。