# Reasoning RL挑战与前提

### 🧑‍💻 User

为什么llm微调的sft是强信号，rl是弱信号？

---

### 🤖 Assistant

在 LLM（大语言模型）的微调过程中，将 **SFT（监督微调）** 称为“强信号”而将 **RL（强化学习，如 RLHF）** 称为“弱信号”，主要是从**监督信号的密度、粒度以及反馈的直接性**这几个维度来定义的。

简单来说：**SFT 是手把手教每一个动作（过程导向），而 RL 只是给最终结果打分（结果导向）。**

以下是详细的深度解析：

### 1. SFT：强信号 (Token-level Supervision)

SFT 的本质是**行为克隆（Behavior Cloning）**。在训练过程中，模型不仅知道要“做什么”，还清楚地知道每一步“怎么做”。

*   **Token 级反馈（密度极大）：**
    SFT 使用的是交叉熵损失函数（Cross-Entropy Loss）。这意味着对于输入序列中的**每一个 Token**，都有一个确定的、唯一的“标准答案”（Ground Truth）。
    $$Loss = - \sum \log P(\text{target\_token} | \text{context})$$
    模型每生成一个词，梯度下降都会立即告诉它：“你错了，你应该生成 'apple' 而不是 'banana'”。这种反馈极其密集且精确。
*   **Teacher Forcing（强制教学）：**
    无论模型当前预测得多么离谱，SFT 都会强制将正确的 Token 作为下一步的输入。这就像老师握着学生的手在写字，学生不需要自己探索路径，只需要肌肉记忆。
*   **确定性目标：**
    SFT 的目标是拟合数据分布。信号非常明确：**“像我这样写”**。

### 2. RL：弱信号 (Outcome-level / Scalar Feedback)

RL（通常指 PPO 或 DPO 等算法）的本质是**试错学习（Trial and Error）**。模型需要自己探索如何获得高分。

*   **标量反馈（稀疏信号）：**
    RL 的反馈通常来自于奖励模型（Reward Model）。无论模型生成了多长的一段话（比如 500 个 Token），最终得到的通常只是**一个标量分数**（比如 Reward = 0.8）。
    这个单一的数字要用来指导这 500 个 Token 的更新，信号被极度稀释了。
*   **信用分配难题（Credit Assignment Problem）：**
    这是 RL 为“弱信号”的核心原因。
    如果模型写了一首诗得到了差评（负奖励），模型并不知道具体是**哪一个词**或**哪一句话**写得不好。是因为押韵没押好？还是意境太差？还是有错别字？
    RL 算法（如 PPO）必须通过大量的采样和统计概率，去“猜测”哪些 Token 对高分有贡献。这种反馈是不直接的、模糊的。
*   **黑盒优化：**
    SFT 告诉模型“正确答案是什么”，而 RL 甚至不知道正确答案是否存在，它只知道“这个比那个好”。模型必须在巨大的策略空间中盲目探索，试图撞出一个高分路径。

### 3. 直观类比

为了更好地理解，我们可以用**“学写作文”**来做类比：

*   **SFT (强信号) = 临摹/抄写**
    老师写好一篇满分作文，让你拿透明纸蒙在上面描。
    *   *信号：* 每一笔、每一划你都知道该怎么写。如果你写歪了，立刻就能看到偏差。
    *   *效果：* 学得极快，能迅速掌握基本语法和结构，但很难超过老师的水平。

*   **RL (弱信号) = 投稿打分**
    你自己写一篇文章交给老师，老师只给你打一个分数：“B-”。
    *   *信号：* 老师没告诉你哪句写错了，也没给你修改意见。你只能回去重写，尝试改改开头，或者换个结尾，再交上去看看分数变没变。
    *   *效果：* 学习效率低，需要尝试很多次。但如果你一旦“悟”到了（比如发现老师喜欢幽默的风格），你可能会写出比老师范文更好的文章。

### 4. 总结与对比

| 维度 | SFT (监督微调) | RL (强化学习/RLHF) |
| :--- | :--- | :--- |
| **信号粒度** | **Token 级** (每个词都有反馈) | **Sentence/Episode 级** (整段话只有一个反馈) |
| **信息密度** | **极高** (Dense) | **稀疏** (Sparse) |
| **学习方式** | **模仿** (Imitation) | **探索** (Exploration) |
| **告诉模型** | "你应该说这个词" | "这句话说得不错/不好" |
| **主要问题** | 容易过拟合，产生幻觉 (Hallucination) | 训练不稳定，收敛难 (Credit Assignment) |
| **信号强度** | **强** | **弱** |

### 5. 既然 RL 是弱信号，为什么还需要它？

虽然 SFT 信号强，效率高，但它有两个致命弱点，使得弱信号的 RL 必不可少：

1.  **天花板限制：** SFT 只能逼近训练数据的质量。如果标注员是普通人，模型就只能达到普通人的水平。
2.  **分布差异：** SFT 训练时有“标准答案”，但在推理时没有。这种差异（Exposure Bias）会导致长文本生成的逻辑崩坏。

**RL 的“弱信号”恰恰是它的优势：** 因为它不规定具体的词，只规定好坏（Reward），这给了模型**泛化**和**超越**的空间。模型可能会找到一种人类标注员没写过、但能获得更高奖励的回答方式（例如更优的逻辑链），从而实现“青出于蓝而胜于蓝”。

---

### 🧑‍💻 User

我们都知道sft可以让模型学会预训练阶段不会的东西，但是rl很难。目前的sota普遍带有cot，但是pertrain+instruct sft阶段的模型显然是不具备think step by step能力的，可能导致后续的reasoning RL阶段不稳定，为了让instruct模型能有一个good start，业界主流的做法是什么？

---

### 🤖 Assistant

这是一个非常深刻且切中当前大模型（LLM）前沿痛点的问题。

你说得很对：**SFT 负责“无中生有”（教会能力），RL 负责“优中选优”（激发潜力）。** 如果模型在 Pretrain + 通用 Instruct SFT 阶段连基本的“一步步推理”都不会，直接上 RL 就像让一个不会走路的婴儿去跑马拉松，模型根本探索不到那个高奖励的路径（Reward Hacking 除外），导致训练发散或无效。

为了解决这个问题，业界主流的做法是引入一个**“Reasoning Cold Start”（推理冷启动）**阶段。

简单来说，就是在正式的大规模 RL 之前，先进行一轮**专门针对 CoT（思维链）的高质量 SFT**。

以下是业界构建这个“Good Start”的几种主流核心方案：

### 1. 基于“拒绝采样”的冷启动数据构建 (Rejection Sampling / Best-of-N)

这是目前最有效、也是 **DeepSeek-R1** 等模型明确采用的方案。既然模型本身推理能力弱，我们就用“暴力”换“智能”。

*   **操作步骤：**
    1.  **强 Prompt 引导：** 使用预训练好的模型，通过精心设计的 Prompt（例如："Let's think step by step", "请详细展示你的推导过程"），强制模型针对数学或代码题目生成大量的回答（比如一个问题生成 100 个回答）。
    2.  **结果过滤器 (Verifier)：** 因为数学题和代码题有“标准答案”或“编译器”，我们可以自动判断这 100 个回答里哪些是最终结果正确的。
    3.  **反向筛选 CoT：** 哪怕模型很笨，生成 100 次可能也有 1 次蒙对了过程，或者虽然过程跌跌撞撞但结果对了。我们将这些**结果正确**的样本（包含其长思维链）保留下来。
    4.  **SFT 训练：** 把这些“瞎猫碰上死耗子”跑出来的正确 CoT 路径，作为 SFT 的数据喂给模型。

*   **目的：** 并不是为了教模型新的知识，而是**教会模型“输出长思维链”这种模式**。通过 SFT，将原本概率极低（比如 1%）的推理行为，提升到模型习以为常的行为（比如 80%），从而让后续的 RL 能够由点及面地进行优化。

### 2. 知识蒸馏 (Distillation from Stronger Models)

这是最经典且广泛使用的“捷径”。

*   **操作步骤：**
    利用目前市面上推理能力最强的闭源模型（如 OpenAI o1, GPT-4, Claude 3.5 Sonnet），针对复杂的 Query，要求它们输出详细的 CoT 过程。
*   **数据清洗：**
    将强模型生成的 CoT 作为 Target，去 SFT 自己的小模型或基座模型。
*   **业界现状：**
    DeepSeek-R1 的论文中提到，他们不仅使用了自身的强化学习探索，也使用了大量的蒸馏数据来让小模型（Distill models）具备推理能力。对于基座模型（R1-Zero 到 R1），他们利用了少量的**高质量长思维链数据**进行冷启动。

### 3. 人工构造与格式强加 (Human Annotation & Formatting)

仅仅有推理内容是不够的，还需要固定的**格式**，以便模型在推理阶段能稳定触发。

*   **特殊 Token 植入：**
    现在的 Reasoning 模型通常会训练模型识别特殊的 Token，例如 `<think>` 和 `</think>`。
*   **做法：**
    在冷启动的 SFT 数据中，人工或半自动地将推理过程包裹在这些标签中。
    *   *Input:* 求解 $\int x dx$
    *   *Target:* `<think>` 这里需要用幂函数积分公式... </think> 答案是 $0.5x^2+C$。
*   **作用：**
    这相当于给了模型一个“起手式”。SFT 强行让模型学会：**只要遇到复杂问题，先输出 `<think>`，然后开始碎碎念，直到想清楚了再输出 `</think>`**。这个“形式”本身就是一种极强的归纳偏置（Inductive Bias）。

### 4. 迭代式微调 (Iterative SFT)

这是一种进阶做法。Good start 不是一次性完成的，而是滚雪球。

1.  **Round 0:** 用少量高质量（哪怕是手写）的 CoT 数据做 SFT，得到模型 $M_0$。
2.  **Generation:** 用 $M_0$ 对新题目进行生成（带 CoT）。
3.  **Filtering:** 过滤出正确的答案。
4.  **Round 1:** 将正确的数据加入训练集，SFT 得到 $M_1$。
5.  **Loop:** 重复上述过程。

### 总结：如何解决“RL 之前的断层”？

要让 Instruct 模型具备 Reasoning RL 的资格，业界的核心共识是：**不要指望 RL 去“教会”模型思考，而是用 SFT 先把“思考的架子”搭好。**

这个“Good Start”必须包含两个要素：
1.  **Format（形式）：** 学会使用 `<think>` 标签，学会把话说长，学会分步骤。
2.  **Correctness Bias（正确性偏置）：** 通过“拒绝采样”或“蒸馏”，让模型见识过什么是“正确的推理路径”。

**DeepSeek-R1-Zero 的启示：**
值得注意的是，DeepSeek 论文中提到的 **R1-Zero** 其实是一个反例，它证明了**即使没有 SFT 冷启动，纯 RL 也是可能涌现出 Reasoning 能力的**（Aha moment），但过程极度不稳定，且输出格式非常难以阅读（中英文夹杂、自我重复）。
**所以，为了工程上的稳定和可用性，业界最终还是选择了：少量高质量 CoT SFT (Cold Start) -> 大规模 RL (PPO/GRPO) 的标准范式。**

---

### 🧑‍💻 User

通过阅读deepseek-r1/qwen3 Technical Report，我们发现在ReasoningRL阶段之前，会先进行个Long-CoT Cold Start的sft步骤，通过构建长思维链数据和采用特定的数据收集与处理方法（可以是reasoning模型合成推理路径+人工筛选），为模型注入基础的推理模式。这个是否是比较成熟的做法？

---

### 🤖 Assistant

这是一个非常准确且敏锐的观察。

**答案是肯定的：这不仅是“比较成熟”的做法，甚至已经成为当前训练高性能推理模型（Reasoning Models）的“行业标准范式”或“必经之路”。**

在 DeepSeek-R1 的技术报告发布后，这种**“Long-CoT SFT (Cold Start) $\rightarrow$ Reasoning RL”** 的两阶段（或多阶段）训练策略，基本上被确立为复现类 o1 模型的最优解。

以下是针对这一做法的深度技术拆解，解释为什么它是成熟且必要的：

### 1. 为什么称之为“成熟做法”？

虽然 DeepSeek-R1 刚刚发布，但这种做法背后的逻辑在业界（包括 OpenAI、Qwen 团队、Llama 社区）已经经过了多轮验证。它解决了纯 RL（Pure RL）面临的三个核心工程难题，因此被称为成熟的工程路径：

#### A. 解决“冷启动”难题（The Cold Start Problem）
正如你上一条问题提到的，如果基座模型连基本的 `<think>` 标签都不会输出，或者不知道“把步骤写长”有助于解题，直接上 RL（如 GRPO 或 PPO）会导致模型在巨大的搜索空间中迷失。
*   **做法：** 通过 Cold Start SFT，人工植入一种**“归纳偏置”（Inductive Bias）**。
*   **效果：** 告诉模型：“在回答困难问题前，先输出思维链，且思维链越详细越好”。这大大缩减了 RL 的探索空间。

#### B. 解决“可读性与格式”问题（Readability & Formatting）
DeepSeek-R1 的报告中对比了 `R1-Zero`（无 SFT 冷启动，纯 RL）和 `R1`（有 SFT 冷启动）。
*   **R1-Zero 的问题：** 虽然它也能做题，但思维链极其混乱，中英文夹杂，甚至没有标点，人类难以阅读。这是因为奖励函数（Reward）只关心答案对不对，不关心过程好不好看。
*   **Cold Start 的作用：** 通过少量高质量、排版精美、逻辑清晰的 SFT 数据，**规范了模型的输出格式**（例如：强制使用 `<think>\n 1. First... \n 2. Then...</think>`）。这让后续的 RL 只能在“好格式”的约束下探索“好逻辑”。

#### C. 稳定训练收敛（Stability）
纯 RL 训练极其不稳定，容易出现 Reward Hacking（模型通过作弊拿高分）或 Mode Collapse（模型只会输出一种模式）。SFT 提供了一个稳定的**锚点（Anchor）**，后续 RL 的 KL 散度约束（KL Divergence Constraint）通常也是基于这个 SFT 后的模型计算的，防止模型跑偏太远。

---

### 2. 这一阶段的核心数据构建方法（SOTA 做法）

你提到的“Reasoning 模型合成 + 人工筛选”确实是主流，具体流程通常更为精细，可以总结为 **“通过计算换智能”**：

#### 第一步：种子数据构造 (Seed Data)
这部分数据量很小（几千条），但质量极高。
*   **来源：** 通常由人类专家手写，或者利用超强模型（如 GPT-4o, Claude 3.5 Sonnet）通过极其复杂的 Prompt 工程生成。
*   **特征：** 包含详细的**反思（Reflection）**、**回溯（Backtracking）**和**验算（Verification）**过程。
*   *目的：* 定义“什么是完美的思维链”。

#### 第二步：大规模合成与拒绝采样 (Synthesis & Rejection Sampling)
有了种子数据微调后的模型，或者直接使用强模型，针对数十万道题目进行大规模生成。
*   **Best-of-N：** 对同一个问题，生成 $N$ 个（例如 64 个或 128 个）回答。
*   **自动过滤器 (Verifier)：**
    *   对于数学题：检查答案是否与标准答案一致。
    *   对于代码题：运行 Unit Test。
*   **清洗策略：** 保留那些**“过程包含长 CoT”且“最终答案正确”**的样本。
*   *关键点：* DeepSeek 提到，为了避免模型只学会“试错”而不学会“思考”，他们还会过滤掉那些“虽然答案对了，但思维链逻辑不通”的样本（这也是为什么要引入 Generative Reward Model 或人工介入的原因）。

#### 第三步：格式化混训 (Formatted SFT)
将上述清洗出的数十万条高质量 CoT 数据，混合通用的指令数据，进行 SFT。
*   这一步产生的模型，被称为 **Reasoning SFT Policy**。
*   它就是后续 Reasoning RL 阶段的 **Actor Model** 和 **Reference Model** 的初始化起点。

---

### 3. DeepSeek 与 Qwen 的策略对比与共识

*   **DeepSeek-R1：**
    非常明确地提出了 **"SFT (Cold Start) $\rightarrow$ RL $\rightarrow$ SFT (Distillation) $\rightarrow$ RL"** 的多阶段管线。
    特别是他们强调了在 Cold Start 阶段，数据不仅要有推理，还要有**“可读性总结”**，确保模型不仅会想，还会与人交流。

*   **Qwen (Qwen2.5-Math / Qwen-QwQ 等)：**
    Qwen 团队虽然未发布名为 "Qwen3" 的报告，但在其 Qwen-Math 系列论文中，也采用了类似的 **Iterative SFT + PPO/GRPO** 策略。他们非常擅长利用 **Self-Correction（自修正）** 数据，即让模型学会“我刚才算错了，应该是...”这种模式，这通常也是在 SFT 阶段先注入，再由 RL 强化的。

### 4. 总结：这是否是未来的唯一解？

**目前来看，是的。**

“Cold Start SFT” + “Reasoning RL” 的组合，完美地结合了监督学习和强化学习的优势：
1.  **SFT (Cold Start)：** 负责**“教会姿势”**（Teach the Pattern）。让模型学会如何分步思考，学会使用特定的 Token 结构。
2.  **RL (Reasoning)：** 负责**“提升智商”**（Scale the Intelligence）。让模型在 SFT 学会的姿势基础上，探索出人类数据中未曾包含的、更深层的逻辑路径。

可以说，**没有 Cold Start SFT，Reasoning RL 就是无源之水；而没有 Reasoning RL，SFT 模型就永远无法超越人类标注员的水平。**