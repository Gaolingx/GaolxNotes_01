# RLVR与模型能力边界

### 🧑‍💻 User

为什么以grpo算法为代表的rlvr，不能激发模型原来没有的能力，很多论文基本都证明了这一点，也就是预训练和sft决定了上限。

---

### 🤖 Assistant

这是一个已经逐渐形成**共识**、但仍然容易被误解的问题。以 **GRPO（Group Relative Policy Optimization）** 为代表的 **RLVR（Reinforcement Learning with Verifiable Reward）**，之所以**无法激发模型原本不存在的能力**，根本原因并不在于算法“弱”，而在于 **Transformer + 语言建模这一范式的能力边界**，以及 **RL 在当前设定下所能改变的只是“分布偏置”，而不是“表征空间本身”**。

下面从几个层面系统解释。

---

## 一句话结论（先给结论）

> **预训练决定了“能不能”，SFT 决定了“会不会用”，而 RLVR / GRPO 只能决定“更偏向用哪一种”。**

RL（包括 GRPO）**不能创造新的计算路径、表示维度或中间推理结构**，只能在已有能力子空间内重新分配概率质量。

---

## 一、从函数空间角度：RL 不能扩展可表达函数族

### 1. 预训练阶段做了什么？

预训练（尤其是大规模自回归 LM 训练）本质是在做：

[
\min_\theta ; \mathbb{E}*{x \sim \mathcal{D}}[-\log p*\theta(x)]
]

这一步的作用是：

* 学会 **token-level 的统计结构**
* 在参数空间中形成 **大量隐式的“子程序”**

  * 算术
  * 语法
  * 模板推理
  * 近似搜索
  * 模式匹配
* 决定了：

  * 表征维度
  * attention 能否表达某类关系
  * FFN 是否已经学到某种近似计算

**这些能力是否存在，完全由预训练决定。**

---

### 2. RL / GRPO 优化的是什么？

以 GRPO 为例，本质是：

[
\max_\theta ; \mathbb{E}*{y \sim \pi*\theta(\cdot|x)}
\left[
\frac{1}{|G|}\sum_{y' \in G} (r(y) - r(y'))
\right]
]

注意关键点：

* **reward 只作用在“已生成序列”上**
* gradient 通过 log-prob 回传
* 并 **不会引入新的监督信号结构**

这意味着：

> RL 只能调整 **已有生成路径的概率密度**，不能新增路径。

如果某种能力在 pretrain 阶段：

* 没有被编码成可调用的隐式子程序
* 或其 activation 路径从未被激活过

那么 RL 根本**没有梯度可以把它“拉出来”**。

---

## 二、从信息论视角：RL 不能注入新信息

这是很多论文反复强调但容易被忽略的一点。

### 1. RLVR 的 reward 是**低信息密度**的

典型 RLVR reward 形式：

* 正确 / 错误（0/1）
* rule-based verifier
* sparse scalar signal

而预训练的监督是：

* **完整 token-level 目标分布**
* 每一步都有数千 bits 的信息量

信息量对比极端悬殊。

因此：

> RL 的 reward signal 在信息论意义上，**远不足以“教会”一个新算法或新概念**。

它只能：

* 放大已有模式
* 抑制已有但不符合目标的模式

---

### 2. 为什么 SFT 还能“看起来”教会新能力？

因为 SFT 仍然是 **全序列监督**：

[
\min_\theta \sum_t \log p_\theta(y_t | y_{<t}, x)
]

如果：

* 预训练中已经存在“潜在能力”
* 但 activation threshold 太高、路径太不稳定

那么 SFT 可以：

* 稳定调用路径
* 教会模型“什么时候用”

但前提依然是：**能力已存在**。

---

## 三、从优化动力学角度：RL 极度偏好“最近邻策略”

这是 GRPO / RLHF / RLVR 的一个共性问题。

### 1. RL 的梯度是局部的

RL 优化的是：

[
\nabla_\theta \log \pi_\theta(y|x) \cdot A(y)
]

这意味着：

* 梯度只会强化 **已经被 sample 到的 y**
* 对未出现的结构是零梯度

所以 RL 天然倾向于：

* 找到一个“还行”的策略
* 然后在其邻域内反复 sharpen

而不会：

* 跳到一个完全不同的解空间区域

这就是所谓的 **policy collapse / mode locking**。

---

### 2. GRPO 只是在“相对排序”上更稳定

GRPO 的优势是：

* 减少 reward scale 问题
* 稳定 group 内比较
* 抑制单 sample 噪声

但它依然：

* 不改变探索空间
* 不提供新的中间监督

所以它**更稳**，但**不更强**。

---

## 四、从“能力涌现”的本质看：能力不是 RL 触发的

很多人误以为：

> “只要 reward 设计得好，RL 就能逼出新能力。”

这是对 **能力涌现（emergence）** 的误读。

### 实际情况是：

能力涌现来自：

* 模型规模扩大
* 数据分布覆盖到某个复杂结构阈值
* 表征维度足够大
* attention/FFN 组合能近似某种算法

**RL 只是让你更频繁地走那条路**。

所以论文中你会看到：

* RL 前后：

  * pass@1 提升
  * 稳定性提升
  * 减少 hallucination
* 但：

  * pass@k 上限不变
  * scratchpad depth 不变
  * 复杂推理任务的 scaling law 不变

---

## 五、为什么很多论文都“证伪”了 RL 能创造能力？

你提到“很多论文基本都证明了这一点”，这是非常准确的观察。

典型结论模式是：

* 若模型在 pretrain+SFT 后 **完全不会**

  * RL 几乎无效
* 若模型已经 **偶尔会**

  * RL 能显著提高成功率
* 若模型已经 **经常会**

  * RL 提升迅速饱和，甚至退化

这正是 **概率重分配而非能力扩展** 的典型证据。

---

## 六、那 RLVR / GRPO 的“正确定位”是什么？

在当前范式下，它们更像是：

* **策略对齐器**
* **分布整形器**
* **行为稳定器**
* **解空间偏置调节器**

而不是：

* 知识注入器
* 算法学习器
* 表征扩展器

---

## 七、真正能“扩展上限”的是什么？

如果目标是 **突破 pretrain 决定的能力上限**，方向通常在：

1. **架构层面**

   * recurrence / state space
   * tool / memory
   * execution-in-the-loop

2. **训练信号结构**

   * dense intermediate supervision
   * process-level reward（非 terminal）
   * latent variable modeling

3. **数据分布**

   * 合成可验证但高信息密度的数据
   * curriculum 设计

而不是单纯更“聪明”的 RL 算法。

---

## 总结一句话

> **GRPO / RLVR 的作用是“把模型已有的能力用到极致”，而不是“赋予模型原本没有的能力”。
> 能力上限来自预训练，RL 只能在这个上限之内重新分配概率质量。**

如果你愿意，下一步我们可以具体拆一篇你心里在想的论文（比如某个 GRPO on math / code 的 work），从实验设计层面看它是如何“无意中证明”这一结论的。
