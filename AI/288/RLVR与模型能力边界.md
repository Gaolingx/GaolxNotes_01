# RLVR与模型能力边界

### 🧑‍💻 User

为什么以grpo算法为代表的rlvr，不能激发模型原来没有的能力，很多论文基本都证明了这一点，也就是预训练和sft决定了上限。

---

### 🤖 Assistant

这是一个非常深刻且触及大模型本质的问题。你的观察非常准确：目前的学术共识和工业界实践（包括DeepSeek的报告）都指向同一个结论——**强化学习（RL，包括GRPO、PPO等）主要是在做“对齐”和“激发”，而非“注入知识”或“创造新能力”。**

以GRPO（Group Relative Policy Optimization）为代表的RLVR（Reinforcement Learning from Verifiable Rewards，基于可验证奖励的强化学习）之所以不能无中生有地创造能力，可以从以下几个核心维度从数学和机制上来解释：

### 1. 概率分布的本质：RL 只是在“重加权”

大语言模型的本质是一个基于上下文的概率分布预测器 $P(y|x)$。

*   **预训练（Pre-training）**：构建了巨大的参数空间，定义了所有可能的token组合及其基础概率。这时模型“读过”了所有的书，拥有了潜在的知识网络。
*   **SFT（有监督微调）**：教会模型特定的格式和指令遵循能力，将预训练的宽泛分布收敛到一种特定的“对话模式”上。
*   **RL（GRPO/PPO）**：
    RL的本质是在现有的策略分布 $\pi_\theta$ 中进行**搜索（Search）**和**剪枝（Pruning）**。
  
    假设对于一个数学问题，模型在预训练阶段潜在的生成路径中，生成正确答案的概率是 $P(\text{correct}) = 10^{-6}$（极低，但存在），生成错误答案的概率是 $P(\text{wrong}) \approx 1$。
  
    RL的作用是通过奖励信号（Reward），极大化正确路径的概率，极小化错误路径的概率。**但是，如果 $P(\text{correct})$ 在物理上为 0（即模型根本没有相关的知识表征或逻辑链路），RL 算法中的梯度 $\nabla J(\theta)$ 将无法获得有效的方向指引。**

    $$ \nabla J(\theta) \propto \mathbb{E}_{\tau \sim \pi_\theta} [A_t \nabla_\theta \log \pi_\theta(a_t|s_t)] $$
  
    如果采样出来的轨迹 $\tau$ 全部都是错误的（因为没有能力），那么优势函数 $A_t$ 就无法区分优劣（或者全是负反馈），模型就学不到任何东西。**你不能优化一个从未出现过的轨迹。**

### 2. GRPO 的机制限制：依赖“组内探索”

GRPO 算法的一个核心特征是**组内相对优势（Group Relative Advantage）**。它不使用额外的价值网络（Value Network），而是通过对同一个提示词（Prompt）采样一组输出 $\{o_1, o_2, ..., o_G\}$，然后计算相对奖励：

$$ A_i = \frac{r_i - \text{mean}(\{r_1, ..., r_G\})}{\text{std}(\{r_1, ..., r_G\}) + \epsilon} $$

这个公式揭示了RLVR能力的边界：
*   **前提是“碰运气”成功**：为了获得正的优势 $A_i > 0$，这组输出中**必须至少有一个**比其他的更好（或者是正确的）。
*   **冷启动问题（Cold Start）**：如果模型没有基础能力（由预训练和SFT决定），那么对于一个难题，它生成的 $G$ 个答案可能全是错的，且错得离谱程度一样。此时 $r_i$ 全部极低，方差极小或为0，梯度更新无效或充满了噪声。
*   **结论**：GRPO 就像是一个高明的“淘金者”，它能从沙子里淘出金子（筛选出最佳推理路径），但它不能通过炼金术把沙子变成金子。

### 3. 知识边界理论：LIMA 与 DeepSeek-R1 的证据

*   **LIMA 假设（Less Is More for Alignment）**：Meta 的 LIMA 论文曾提出“表面对齐假设”（Superficial Alignment Hypothesis）。它认为模型的知识和能力几乎完全来自预训练，SFT 和 RL 只是学会了如何**调用**这些知识来与用户交互。
*   **DeepSeek-R1 的发现**：在 DeepSeek-R1 的技术报告中，他们明确提到，如果直接在基础模型（Base Model）上跑 RL（DeepSeek-R1-Zero），虽然模型能进化出自我反思，但对于很多任务效果并不好，且输出难以阅读。**必须先通过少量的 SFT 数据（Cold Start Data）让模型具备基本的解题格式和初步的推理能力**，RL 才能起飞。

这证明了：**RL 是在逼近模型能力的上限（Pass@1 逼近 Pass@k），而不是提升上限本身。**

### 4. 探索（Exploration）与泛化（Generalization）的局限

有人可能会问：“RL不是有探索（Exploration）机制吗？难道不能探索出新能力？”

*   **组合 vs. 原创**：RL 的探索是在**已知概念的组合空间**中进行的。比如模型知道“勾股定理”和“求导”，RL 可以教会模型在解题时先把这两个概念组合起来使用（思维链 CoT）。这看起来像“新能力”，但这实际上是**逻辑链路的优化**。
*   **无法无中生有**：如果预训练语料中从未包含某种语言（比如克林贡语）的语法，或者从未包含量子力学的基础公式，无论怎么做 RL，模型都不可能通过“试错”来发明这门语言或推导出薛定谔方程。

### 总结

GRPO 等 RLVR 算法之所以不能激发模型没有的能力，是因为：

1.  **数学上**：RL 依赖于有效采样。如果正确路径的采样概率为 0，梯度无法下降。
2.  **机制上**：RL 是过滤器和放大器。它过滤掉错误的推理路径，放大正确的推理路径，但它不负责“生产”路径的原材料。
3.  **本质上**：
    *   **预训练（Pre-training）决定了“知识库”的容量（天花板）。**
    *   **SFT 决定了“检索工具”的可用性。**
    *   **RL (GRPO) 决定了“检索策略”的最优性。**

RL 让模型**“更聪明地使用已知的知识”**，表现为推理能力变强（DeepSeek-R1 的长思维链），但这本质上是对存量知识的深层挖掘，而非增量知识的注入。