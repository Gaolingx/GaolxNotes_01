# SFT能力偏移原因

### 🧑‍💻 User

sft领域模型微调，为什么会出现某些能力强了其他能力弱？

---

### 🤖 Assistant

在 **SFT（Supervised Fine-Tuning）** 阶段出现“**某些能力显著增强，而其他能力反而下降**”，并不是偶发现象，而是该训练范式与大模型表征结构共同作用下的**系统性结果**。可以从以下几个层面理解。

---

## 一、根本原因：SFT 本质上是**有偏分布下的再拟合**

SFT 的目标不是“整体能力最大化”，而是**在特定数据分布上最小化监督损失**。这必然引入偏置。

### 1. 数据分布偏移（distribution shift）

预训练学到的是一个覆盖极广的联合分布：

[
p_{\text{pretrain}}(x, y)
]

而 SFT 实际优化的是：

[
p_{\text{SFT}}(x, y) \subset p_{\text{pretrain}}(x, y)
]

当 SFT 数据在以下方面高度偏置时：

* 任务类型（如只强调 instruction-following / reasoning / chat）
* 语言风格（简短、礼貌、模板化）
* 推理路径（CoT / 非 CoT）
* 安全边界与拒答模式

模型参数会**向该子分布坍缩**，从而牺牲对其他区域的拟合能力。

这本质上就是一种 **catastrophic forgetting 的弱形式**。

---

## 二、参数空间视角：能力并非正交，存在强耦合

### 2. 不同能力在参数空间中高度共享

LLM 的“能力”并不是模块化的，而是：

* 共享 embedding
* 共享 attention / FFN 子空间
* 共享高层语义表征

因此 SFT 并不是：

> “在原有能力上叠加一个新能力”

而更像是：

> “在原有高维流形上重新拉伸、压缩、旋转”

结果就是：

* 某些方向（与 SFT loss 强相关）被强化
* 其他方向的“自由度”被压缩

典型例子：

* 强 instruction-following → 创造性写作、多样性下降
* 强安全对齐 → 边界推理能力下降
* 强 CoT 显性推理 → 隐式直觉任务（如常识补全）变差

---

## 三、优化动力学：SFT 是强监督、低噪声信号

### 3. SFT 梯度“过于干净”，反而有副作用

与预训练相比：

| 训练阶段     | 梯度特性         |
| -------- | ------------ |
| Pretrain | 高噪声、弱标签、覆盖面广 |
| SFT      | 低噪声、强标签、目标单一 |

低噪声 + 高一致性的梯度会导致：

* 表征塌缩（representation collapse）
* 模型过度对齐“标准答案路径”

从信息论角度看：

* 预训练最大化 mutual information 的覆盖
* SFT 强化的是 conditional likelihood 的尖峰区域

这会降低模型的**熵与多样性**，某些“看似不相关”的能力因此退化。

---

## 四、行为层面：SFT 实际上在“教模型如何说话”，而不是“如何思考”

### 4. SFT 强化的是输出行为策略（policy），而非底层世界模型

很多能力下降并非模型“不知道”，而是**不再表达**：

* 明明能推理，但被训练成“简短回答”
* 明明能展开，但被训练成“先拒绝 / 先免责声明”
* 明明能多路径探索，但被训练成“唯一规范解法”

这在评测中表现为：

* benchmark 分数下降
* 人类感知“变笨”

但内部表征未必完全丢失，只是被 policy 抑制。

---

## 五、与 RLHF / RLVR 的关系（呼应你的前一个问题）

你之前提到 **RLVR / GRPO 无法激发原本不存在的能力**，这一点在 SFT 上同样成立，甚至更明显：

* **SFT / RLHF / RLVR 都是 reweighting，而不是 new representation learning**
* 它们只能在已有能力流形上重分布概率质量
* 当概率质量被挪向某些能力，其他能力必然变轻

这不是工程失误，而是**容量受限 + 非正交能力空间下的必然 trade-off**。

---

## 六、工程上如何缓解（但无法彻底消除）

### 常见缓解手段及其本质

1. **多任务混合 SFT**
   → 降低分布偏移幅度，但会稀释目标能力提升

2. **Replay / Mixing pretrain data**
   → 防止 forgetting，本质是加正则

3. **LoRA / Adapter / partial fine-tune**
   → 限制可变参数子空间，减少全局扰动

4. **Instruction dropout / style randomization**
   → 保留表达多样性

5. **Post-SFT reranking / decoding tricks**
   → 行为层面补救，而非能力层面

但要强调一句：

> **不存在“只增强、不牺牲”的 SFT**，除非你回到预训练尺度的数据与算力。

---

## 七、一句话总结

> **SFT 不是能力创造器，而是能力再分配器。**
> 当你用有限、强偏置的数据去“精调”一个高容量模型时，
> 某些能力变强，必然意味着其他能力被压缩或抑制。

如果你愿意，下一步我可以结合：

* instruction-following vs reasoning 的具体冲突
* 多语言 / 多模态 SFT 的能力退化案例
* 为什么一些团队开始“弱 SFT + 强 pretrain + 轻对齐”

来继续展开。


---

### 🧑‍💻 User

但是各大ai厂商也要对base做sft才能实现instruct following啊，为啥他们就不会出现这种遗忘的副作用呢？

---

### 🤖 Assistant

这是一个**非常关键、而且经常被误解的问题**。结论先行：

> **各大厂商并不是“没有遗忘”，而是把遗忘压到你几乎感知不到的程度；同时他们付出了你在公开 SFT 场景里看不到的巨大代价。**

下面分层拆解。

---

## 一、首先澄清一个事实：**他们同样会遗忘**

内部与泄露评测、以及你能从模型行为侧面观察到的现象，都表明：

* instruct 版 **≠** base + 0 代价
* 所有商业 instruct 模型，在以下维度都**弱于 base**：

  * 高温自由生成
  * 非指令化探索式推理
  * 某些“无任务目标”的世界知识联想
  * 奇异 / 反直觉 prompt 下的鲁棒性

只是这些能力：

* **不在商业 KPI**
* **不在主流 benchmark**
* **被对齐层刻意抑制**

所以你“看不到”，并不等于“没有”。

---

## 二、为什么他们的遗忘 *看起来* 很小？

### 1. SFT 数据规模和覆盖面与你想象的完全不是一个量级

开源/学术 SFT 常见规模：

* 10⁴ – 10⁶ 条 instruction
* 强风格、强模板、强目标

头部厂商的 instruct SFT：

* **10⁷ – 10⁸+ 级别**
* 覆盖：

  * 多任务
  * 多风格
  * 多推理路径
  * 多语言
  * 多拒答边界
* 严格控制 **token-level 分布接近 pretrain**

关键点在于：

> **他们不是“精调一个子分布”，而是在“重塑整个条件分布的外壳”**

分布偏移被压得极小。

---

### 2. 实际流程是「Pretrain ⇄ SFT」反复交织，而不是线性两阶段

你脑中的流程可能是：

```
Pretrain → SFT → RLHF → Release
```

而真实流程更接近：

```
Pretrain
  ↕
SFT (v1)
  ↕
继续预训练（大量无标注 + 合成数据）
  ↕
SFT (v2)
  ↕
再预训练
  ↕
轻量 RL 对齐
```

这一步极其关键：

* **再预训练会“修复”SFT 带来的表征塌缩**
* SFT 更像是周期性施加的“行为约束”
* 表征主导权始终在 pretrain loss 手里

这在开源体系里几乎没人做，因为**贵得离谱**。

---

### 3. SFT 的“可塑性子空间”被严格限制

他们很少做：

* 全参数、无约束 SFT

而是组合使用：

* partial fine-tune
* LoRA / adapter（但规模很大）
* layer-wise LR decay
* 只动高层 / policy-relevant 层

等价于：

> **instruction-following 被限制在参数空间的一张“薄壳”上**

底层世界模型几乎不动，自然谈不上大规模遗忘。

---

## 三、一个常被忽略的点：**他们允许“能力存在但不表达”**

### 4. 厂商不追求“真实能力最大化”，而是“可控行为最优”

这点和你前面关于 RLVR 的判断完全一致。

商业 instruct 模型的目标函数本质是：

[
\max ; \text{usefulness} + \text{predictability} - \text{risk}
]

而不是：

[
\max ; \text{capability}
]

因此他们：

* 接受某些能力被 policy gate 掉
* 接受 base 模型更“聪明”，但不可控
* 接受评测外能力退化

你看到的是：

* 行为稳定
* 输出一致
* 指令服从极强

而不是：

* 表征完全无损

---

## 四、为什么开源模型一 SFT 就“明显变傻”？

可以直接对比根因：

| 维度                | 商业模型   | 开源常见         |
| ----------------- | ------ | ------------ |
| SFT 数据量           | 极大、全覆盖 | 小、强偏置        |
| 是否混 pretrain data | 是      | 很少           |
| 是否反复预训练           | 是      | 否            |
| 参数更新约束            | 极强     | 弱            |
| 能力评估维度            | 极宽     | 少量 benchmark |
| 是否容忍隐藏能力          | 是      | 不行（要可见）      |

一句话：

> **你在用“手术刀级别的数据”，对“整个大脑”做“不可逆手术”。**

---

## 五、一个更本质的理解方式（很重要）

### instruct following 并不是一种“新能力”

而是：

* 对已有能力的 **条件路由**
* 对输出分布的 **概率重加权**
* 对交互协议的 **显式化**

头部厂商真正做的是：

> **在几乎不改变世界模型的前提下，增加一个“指令接口层”**

而学术 / 开源 SFT 往往是在：

> **直接修改世界模型本身**

这就是副作用差异的根源。

---

## 六、总结成三句话

1. **他们也遗忘，只是你不在意、看不到、测不到。**
2. **他们用巨大数据、反复预训练和严格参数约束，把遗忘压到极低。**
3. **instruct following 是行为层，而不是能力层；商业模型始终让 pretrain 主导能力上限。**

如果你愿意，下一步我可以进一步拆解：

* 为什么“弱 SFT + 强 pretrain”正在成为主流
* instruct 能否通过 decoding / routing 层完全绕开 SFT
* 为什么未来可能不再需要传统意义的 SFT

这些都和你前面提到的 RLVR 能力边界问题是同一条逻辑线。
