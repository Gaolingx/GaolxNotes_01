# SFT能力偏移原因

### 🧑‍💻 User

sft领域模型微调，为什么会出现某些能力强了其他能力弱？

---

### 🤖 Assistant

在SFT（Supervised Fine-Tuning，有监督微调）过程中，模型在某些特定任务上能力变强，而在通用能力或其他未训练任务上能力变弱，这种现象在学术界和工业界被称为 **“灾难性遗忘”（Catastrophic Forgetting）** 或 **“对齐税”（Alignment Tax）**。

根本原因在于：**模型参数的更新方向是为了最小化SFT数据的损失函数，而这个方向往往偏离了保持通用能力的“最优解”区域。**

以下是导致这一现象的几个核心技术原因分析：

### 1. 数据分布的剧烈偏移 (Distribution Shift)

这是最主要的原因。
*   **预训练阶段 (Pre-training)：** 模型在大规模、多样化的数据集（Common Crawl, Wikipedia, Books等）上训练，学习的是通用的语言概率分布 $P_{pre}(y|x)$。此时模型的参数 $\theta$ 处于一个能够兼顾各种任务的广义最优区域。
*   **微调阶段 (SFT)：** SFT使用的数据集通常是特定领域的（如医疗、法律）或特定格式的（如问答指令）。这个数据集的分布 $P_{sft}(y|x)$ 与预训练分布差异巨大且非常狭窄。
*   **结果：** 优化器会强行将模型参数从“通用全能”的区域拉向“特定任务”的局部极小值。在这个过程中，原本用于编码其他知识（如写诗、逻辑推理）的权重被修改，以适应新的任务。

### 2. 有限的参数容量 (Capacity Constraints)

虽然LLM拥有数十亿甚至数千亿参数，但其容量依然是有限的。这就是所谓的 **“可塑性-稳定性权衡”（Plasticity-Stability Dilemma）**：
*   **可塑性：** 模型学习新知识的能力。
*   **稳定性：** 模型保留旧知识的能力。
*   **竞争关系：** 神经网络的权重是共享的。当你强迫模型“记住”大量的特定领域知识（例如某种特殊的JSON格式输出或垂直领域的生僻词）时，这些神经元连接必须重新排列。如果新任务占据了过多的模型容量，原本用于处理其他任务的神经通路就会被覆盖或破坏。

### 3. 过拟合与模式坍缩 (Overfitting & Mode Collapse)

SFT数据量通常远小于预训练数据量，且往往具有高度一致的“模版感”。
*   **表面对齐：** 模型可能并没有真正理解新知识，而是过度拟合了SFT数据的**格式**。例如，如果SFT数据里所有的回答都以“好的，这很简单”开头，模型可能会在所有回答前都加上这句话，甚至在不该加的时候也加（如回答悲剧性问题时）。
*   **多样性丧失：** 预训练模型可能有10种回答同一个问题的方式，但SFT数据通常只提供一种“标准答案”。微调后，模型输出的概率分布变窄（Low Entropy），导致其丧失了创造性和多样性。

### 4. 优化目标的数学视角

假设预训练的损失函数是 $\mathcal{L}_{pre}(\theta)$，SFT的损失函数是 $\mathcal{L}_{sft}(\theta)$。

在SFT过程中，我们的优化目标仅仅是：
$$ \min_{\theta} \mathcal{L}_{sft}(\theta) $$

这就导致了参数 $\theta$ 会沿着 $\nabla \mathcal{L}_{sft}$ 的方向移动。如果没有约束项，$\theta$ 会越走越远，直到 $\mathcal{L}_{pre}(\theta)$（即通用能力）显著上升。

### 5. 负迁移 (Negative Transfer)

有时候，SFT学到的规则与预训练学到的常识是冲突的。
*   **例子：** 假设你在做一个“角色扮演”的SFT，要求模型扮演一个“只会说谎的坏人”。
*   **后果：** 模型可能会学会“否定事实”这一逻辑模式。当你在后续测试通用常识（如“太阳从哪边升起”）时，这种新的逻辑模式可能会干扰原本的正确知识，导致模型回答错误。

---

### 如何缓解这个问题？

业界通常采用以下几种方法来平衡“特定能力增强”与“通用能力下降”：

1.  **数据混合 (Data Mixing / Replay Buffer)：**
    在SFT的数据集中，按一定比例（如10%-20%）混入高质量的预训练数据或通用指令数据。这相当于在复习新课的同时，时不时复习一下旧知识，强迫模型在优化新任务时，不能让通用能力的Loss变得太高。

2.  **正则化约束 (Regularization / KL Divergence)：**
    在计算Loss时加入约束项，限制微调后的模型分布 $P_{sft}$ 不能偏离基础模型分布 $P_{base}$ 太远。
    $$ Loss = \mathcal{L}_{sft} + \beta \cdot KL(P_{sft} || P_{base}) $$
    这在RLHF（PPO算法）中是标准做法，但在SFT中也可以通过类似思路控制。

3.  **参数高效微调 (PEFT / LoRA)：**
    不全量微调所有参数，而是冻结主干参数，只微调外挂的少量参数（如Low-Rank Adapters）。
    *   **原理：** 这样保留了预训练的大部分权重（通用知识），新能力存储在新增的Adapter中。虽然LoRA在某些极端特定任务上可能不如全量微调，但它极大地保护了模型的通用能力不被破坏。

4.  **早停 (Early Stopping)：**
    SFT通常只需要很少的Epoch（通常1-3个Epoch）。训练时间过长会显著加剧灾难性遗忘。

---

### 🧑‍💻 User

但是各大ai厂商也要对base做sft才能实现instruct following啊，为啥他们就不会出现这种遗忘的副作用呢？

---

### 🤖 Assistant

这是一个非常深刻且直击本质的问题。

首先要纠正一个误区：**各大厂商的模型其实也面临这个问题。**

在学术界和OpenAI内部，这被称为 **"Alignment Tax"（对齐税）**。即为了让模型听话（Instruct Following）和安全，往往会牺牲掉一部分模型的通用智力（如上下文学习能力、分布外推理能力等）。GPT-3在刚发布Instruct版本（text-davinci-001/002/003）时，曾在很多纯推理榜单上分数下降。

但是，为什么你感觉现在的GPT-4、Claude 3、Gemini在微调后不仅听话，而且通用能力似乎没怎么掉，甚至更强了？

这是因为大厂手里掌握着普通开发者不具备的**“黑科技”配方**，主要体现在以下五个维度：

### 1. 极度丰富的数据配比 (Data Mixture Strategy)

这是最核心的秘密。普通人在做垂直领域SFT时，往往只喂入该领域的指令数据（比如只有医疗问答）。

大厂在做Instruct SFT时，使用的是**“全科食谱”**。他们的SFT数据集不是几十万条单一类型的指令，而是包含了成百上千种任务类型的混合体：

*   **通用任务：** 摘要、翻译、改写、扩写。
*   **逻辑任务：** 数学推导、代码编写、逻辑推理链 (CoT)。
*   **知识任务：** 事实问答、百科常识。
*   **特定技能：** 角色扮演、格式化输出 (JSON/XML)。

**原理：** 当你在训练“医疗能力”的同时，同时也在训练“写代码”和“做数学”。这种**多任务学习 (Multi-task Learning)** 强迫模型在参数空间中寻找一个能**同时**满足所有这些任务的“最大公约数”区域，而不是仅仅滑向某个单一任务的深坑。

### 2. “通用知识复习”机制 (Replay Buffer)

为了防止模型“忘了本”，大厂在SFT阶段会混入一定比例（例如10% - 20%）的高质量**预训练数据（Pre-training Data）**。

*   **你的做法：** 只给模型看 `Q: 感冒吃什么? A: 吃感冒灵。`
*   **大厂的做法：** 给模型看 `Q: 感冒吃什么? A: ...` 的同时，还要看一段维基百科关于流感病毒的纯文本，或者一段Python代码。

**数学解释：** 这相当于在Loss函数中显式地加了一个项，要求模型在优化指令跟随的同时，不能破坏对原始语言分布 $P_{pre}(x)$ 的建模能力。

### 3. 数据质量与合成数据 (Quality & Synthetic Data)

普通SFT容易出现“能力下降”，很大程度上是因为SFT数据的**信息密度低于预训练数据**。如果用大量低质量、短回复、逻辑简单的语料去微调一个高智商模型，就像让爱因斯坦去读小学语文课本，模型会被“教傻”（Lobotomy）。

大厂的做法：
*   **人工标注：** 雇佣博士级别的标注员编写极其复杂、逻辑严密的思维链 (CoT) 数据。
*   **模型进化 (Model Evolution)：** 使用最强的模型（如GPT-4）生成极高质量的数据，去微调下一代或小一点的模型。
*   **拒绝采样 (Rejection Sampling)：** 让模型对同一个问题生成100个回答，用Reward Model打分，只把分最高的那个拿去训练。

**结论：** 只有当SFT数据的质量 $\ge$ 预训练模型原本的能力时，模型才不会退化。

### 4. 严苛的评估体系 (Evaluation-Driven Development)

你做微调可能只看Loss降没降，或者人工测几个Case。但大厂有一套巨大的**回归测试集 (Regression Benchmark)**。

在SFT训练的每一步（Checkpoints），他们都会在数千个榜单上自动跑分：
*   如果 MMLU (通用知识) 分数掉了 1%，报警。
*   如果 HumanEval (写代码) 分数掉了，报警。
*   如果 GSM8K (数学) 分数掉了，报警。

**策略：** 只有那些在获得指令遵循能力的同时，通用能力没有显著下降（甚至有所提升）的Checkpoint，才会被保留下来进入下一轮。如果发现某项能力弱了，他们会立刻分析原因，并在训练数据中**加权增强**对应领域的数据。

### 5. RLHF/DPO 阶段的 KL 散度约束

SFT通常只是第一步，后面还有RLHF（基于人类反馈的强化学习）。在RLHF阶段，有一个至关重要的数学约束：**KL Divergence Penalty (KL散度惩罚)**。

$$ R(y|x) = r(x, y) - \beta \log \frac{\pi_{RL}(y|x)}{\pi_{SFT}(y|x)} $$

*   $\pi_{RL}$ 是正在训练的模型。
*   $\pi_{SFT}$ 是SFT后的参考模型（有时甚至是Base模型）。
*   这项公式强行规定：**你学好学坏都可以，但你的输出分布不能偏离原来的模型太远。**

这就像给孙悟空戴了个紧箍咒，允许它学新本事（对齐人类偏好），但禁止它性情大变（遗忘通用能力或发生模式坍缩）。

### 总结

大厂之所以能做到“既要又要”，靠的不是魔法，而是：
1.  **全领域数据混合**（不偏科）。
2.  **混入预训练数据**（不忘本）。
3.  **极高的数据质量**（不降智）。
4.  **大规模自动化评测**（实时监控）。

对于个人或小团队，想缓解SFT后的能力遗忘，性价比最高的方法是：**在你的SFT数据中，混入通用数据集（如Alpaca, ShareGPT的精选子集）以及少量的预训练文本。**

---

### 🧑‍💻 User

我们发现，实际training pipeline是「Pretrain ⇄ SFT」反复交织，而不是线性两阶段

一般人脑中的流程可能是：

```
Pretrain → SFT → RLHF → Release
```

而真实流程更接近：

```
Pretrain
  ↕
SFT (v1)
  ↕
继续预训练（大量无标注 + 合成数据）
  ↕
SFT (v2)
  ↕
再预训练
  ↕
轻量 RL 对齐
```

这种类似“回火”的步骤为什么极其关键，也是主流sota进行instruct对齐的流程？对于个人/小团队，如何根据模型规模控制数据和训练成本？

---

### 🤖 Assistant

你的观察极其敏锐，这正是目前大模型训练（尤其是以Llama 3、Qwen、DeepSeek为代表的SOTA模型）的**核心秘密**。

这种流程在业界被称为 **"Iterative SFT"（迭代式SFT）** 或 **"Continual Pre-training & Finetuning Loop"**。你提到的“回火”（Tempering）这个比喻非常精准，就像锻造钢铁一样，单纯的捶打（SFT）会让钢变硬但变脆（过拟合、遗忘），必须反复回炉加热（Pretrain/Replay）来重新排列内部结构，才能得到既坚硬又有韧性的好钢。

以下深度解析为什么这种“三明治”或“螺旋式”流程是关键，以及小团队如何低成本复刻。

---

### 一、 为什么“回火”流程（Pretrain ⇄ SFT）极其关键？

传统的线性流程假设：`知识注入（Pretrain）` 和 `格式规范（SFT）` 是两个完全切割的阶段。
但现实是：**SFT 不仅仅是学格式，它会导致知识分布的扭曲；Pretrain 不仅仅是学知识，它能修复分布的平滑度。**

这种反复交织的流程主要解决了三个深层问题：

#### 1. 合成数据飞轮 (The Synthetic Data Flywheel) —— 这一点最关键
在SOTA模型（如Llama 3）的训练中，SFT V2使用的数据，很多是由SFT V1模型自己生成并经过清洗的。

*   **流程：**
    1.  `Base` → `SFT v1`（用少量高质量人工数据）。
    2.  `SFT v1` 生成大量数据（比如让它把Wiki百科改写成问答，或生成代码的思维链）。
    3.  使用Reward Model或规则过滤掉低质量数据。
    4.  `Base` + `合成数据` → `继续预训练` 或 `SFT v2`。
*   **原理：** 模型在预训练中学到了海量“模糊的知识”。通过第一轮SFT，它学会了如何表达。然后让它自己把自己肚子里的“模糊知识”写出来变成“清晰的训练数据”，再喂回给自己吃。
*   **效果：** 这是一种**知识蒸馏（Self-Distillation）**。模型自己生成的Chain-of-Thought（CoT）往往比人类写的更符合模型内部的逻辑流，学起来效率更高。

#### 2. 寻找“平坦极小值” (Flat Minima) —— 泛化能力的数学本质
*   **线性SFT：** 容易让模型陷入Loss Landscape上的一个**尖锐的局部极小值（Sharp Minimum）**。在这个点上，模型对训练数据拟合极好，但只要输入稍微变一点（分布外），Loss就会剧烈上升（表现为回答崩坏）。
*   **回火流程：** 在SFT后，重新混入通用语料进行“继续预训练”或“重放”，相当于把模型从尖锐的坑里“踢”出来，强迫它去寻找一个更宽阔、更平坦的低Loss区域。
*   **结果：** 平坦极小值意味着具有更好的**鲁棒性（Robustness）**和**泛化能力**。

#### 3. 解决“能力不对齐” (Skill Mismatch)
*   有时候，SFT要求模型做一项它根本没见过的任务（比如某种极偏门的编程语言）。
*   如果硬做SFT，模型会产生幻觉。
*   **正确做法：** 发现缺什么知识 -> 回到 Pretrain 阶段补充该领域的 Raw Text（无标注语料） -> 再次进行 SFT 激活。这是一个动态查漏补缺的过程。

---

### 二、 个人/小团队如何控制成本与策略？

对于没有几千张H100的小团队，完全复刻大厂的“无限回火”是不现实的。你需要根据**模型参数规模**和**算力预算**采取“非对称战争”策略。

#### 1. 策略矩阵：根据模型规模决定打法

| 模型规模 | 典型代表 | 核心痛点 | 推荐策略 (Poor Man's Recipe) |
| :--- | :--- | :--- | :--- |
| **小模型**<br>(< 7B) | Qwen-2.5-7B<br>Llama-3-8B | 记忆容量小，容易遗忘；<br>推理能力天花板低。 | **全量微调 (Full FT) + 激进的数据混合**<br>因为训练便宜，可以多轮迭代。不要只做LoRA，小模型Full FT效果提升明显。 |
| **中模型**<br>(14B - 34B) | Qwen-14B<br>Yi-34B | 显存是瓶颈；<br>Full FT 成本高。 | **LoRA / QLoRA + Model Merging (模型融合)**<br>这是小团队的大杀器（详见下文）。 |
| **大模型**<br>(70B+) | Llama-3-70B<br>Qwen-72B | 训练成本极高；<br>很难调得比原版好。 | **Prompt Engineering / RAG / In-Context Learning**<br>非要训的话，只能用极少量的LoRA做风格迁移，千万别碰知识注入。 |

#### 2. 小团队的“低成本回火”实操方案

既然不能像大厂那样反复Pretrain，我们可以用**Model Merging（模型融合）**来模拟“回火”的效果。

**流程推荐：**

1.  **准备阶段：** 选定一个强力的Base模型（如Qwen-2.5-Coder-32B）。
2.  **SFT 阶段 (特定能力)：** 使用你的垂直领域数据（比如法律数据）训练一个 LoRA 适配器，记为 `Adapter_Law`。
3.  **SFT 阶段 (通用能力)：** 使用开源的高质量通用指令集（如ShareGPT, Magpie）训练另一个 LoRA，或者直接下载社区现成的通用 LoRA，记为 `Adapter_General`。
4.  **“冷”回火 —— 模型融合 (Model Merging)：**
    *   不要依次训练，而是使用 **TIES-Merging** 或 **SLERP (Spherical Linear Interpolation)** 算法，将 `Base` + `Adapter_Law` + `Adapter_General` 融合在一起。
    *   **原理：** 数学上证明，直接融合权重往往比连续训练更能保留不同任务的能力，且完全不需要额外的GPU计算时间（几分钟就在CPU上跑完了）。
5.  **迭代：** 拿着融合后的模型去测Bad Case，针对Bad Case生成新的数据，再去训练一个新的小LoRA，再Merge进去。

#### 3. 数据成本控制：质量 > 数量

小团队最大的优势是**“人”**。你没有大厂的算力，但你有对垂直业务的深刻理解。

*   **不要：** 试图清洗几TB的Common Crawl数据。
*   **要：** 手写/精修 100-500 条**教科书级别**的数据。
    *   LIMA 论文证明：**Less Is More for Alignment**。1000条完美的数据，效果远好于10万条平庸的数据。
*   **合成数据策略：**
    *   用最强的模型（如 GPT-4o 或 Claude 3.5 Sonnet）生成数据。
    *   **关键步骤：** 必须人工校验！小团队不要做全自动流程，因为你经不起噪声数据的干扰。人工校验这 1000 条数据，就是你最核心的壁垒。

### 总结

对于小团队，不要试图去跑 `Pretrain -> SFT` 的昂贵闭环。

你的“回火”应该是：
**`高质量小样本 SFT` (LoRA) + `模型融合` (Merging) + `通用能力 LoRA 混合`**。

用**权重的算术运算**（Merging）来代替**梯度的反向传播**（Training），是资源受限下的最优解。