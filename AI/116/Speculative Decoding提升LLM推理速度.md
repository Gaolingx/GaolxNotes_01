# Speculative Decoding提升LLM推理速度

### 🧑‍💻 User

Speculative Decoding如何提高llm推理阶段的decode速度？

---

### 🤖 Assistant

Speculative Decoding（推测性解码）是一种用于**显著提升大型语言模型推理阶段解码速度**的技术，核心思想是**“用小模型探路，大模型快速验证”**，在不改变输出结果质量的前提下，大幅减少大模型的调用次数。

以下是其工作原理和如何提升速度的详细说明：

1.  **核心参与者：**
    *   **小模型（草稿模型/Drafter）：** 一个比目标大模型小得多、快得多的模型（例如，参数少5-10倍）。它负责快速生成一个候选词序列（草案）。
    *   **大模型（目标模型）：** 用户最终想要使用的大型语言模型。它负责验证小模型生成的草案是否“正确”。
    *   **验证机制：** 决定接受草案中的哪些词。

2.  **工作流程（一次迭代）：**
    *   **步骤 1：小模型生成草案：** 给定当前的上下文（已经生成的文本），小模型**自回归地**生成一个长度为 `K` 的候选词序列 `(xₜ₊₁, xₜ₊₂, ..., xₜ₊ₖ)`。这个过程很快，因为小模型计算量小。
    *   **步骤 2：大模型并行验证草案：** 将当前的上下文 **加上** 小模型生成的整个 `K` 个词的草案，**一次性输入给大模型**。大模型以**非自回归**的方式（并行地）计算这 `K+1` 个位置（当前位置 + 草案的 `K` 个位置）的输出概率分布 `Pₘₒdₑₗ(x | context)`。
    *   **步骤 3：验证与接受：**
        *   从草案的第一个词 `xₜ₊₁` 开始，将其与大模型在对应位置计算出的概率分布进行比较。
        *   **接受条件：** 如果大模型预测的下一个词恰好是 `xₜ₊₁`（即 `argmax Pₘₒdₑₗ(xₜ₊₁ | context)` 就是 `xₜ₊₁`），或者一个基于概率的随机采样接受了这个词，那么这个词就被接受。
        *   **接受：** 如果 `xₜ₊₁` 被接受，则移动到草案的下一个词 `xₜ₊₂`，将其与大模型在 `t+2` 位置的概率分布进行比较（注意此时的上下文已经包含了被接受的 `xₜ₊₁`）。重复此验证过程。
        *   **拒绝：** 如果某个草案词 `xₜ₊ᵣ` (1 <= r <= K) 被大模型拒绝（即大模型认为下一个词不是它），那么：
            *   所有 `r` 之前的草案词都被接受（`xₜ₊₁, ..., xₜ₊ᵣ₋₁`）。
            *   被拒绝的位置 `t+r`，其输出由**大模型自己生成**（根据它在该位置计算出的概率分布采样一个词 `x'ₜ₊ᵣ`，这个 `x'ₜ₊ᵣ` 通常不同于 `xₜ₊ᵣ`）。
            *   本次迭代结束。下一个迭代的上下文更新为到 `x'ₜ₊ᵣ` 为止。
        *   **全部接受：** 如果 `K` 个草案词全部被接受，那么本次迭代就生成了 `K` 个词。下一个迭代的上下文包含所有已接受词。

3.  **如何提升速度（加速原理）：**
    *   **减少大模型调用次数（核心）：** 这是最主要的加速来源。在标准的自回归解码中，生成 `N` 个词需要调用大模型 `N` 次（每次生成一个词）。在 Speculative Decoding 中：
        *   如果一次迭代生成了 `R` 个词（`R` 可能等于 `K` 或小于 `K` ），那么这次迭代**只调用了一次大模型**（并行验证了 `K` 个位置）。
        *   因此，平均生成一个词所需的大模型调用次数远小于 1（理想情况下接近 `1/K`）。例如，如果平均每次迭代接受 `K=5` 个词中的 4 个，那么生成 4 个词只用了 1 次大模型调用，相当于速度提升了接近 4 倍。
    *   **利用并行计算：** 大模型验证 `K` 个位置的计算是高度并行的（一次前向传播），这比执行 `K` 次顺序的自回归调用要高效得多，尤其是在硬件（GPU/TPU）上。
    *   **小模型的高效性：** 小模型生成 `K` 个草案词非常快，其计算成本远低于大模型的一次调用。这个成本被分摊到最终生成的多个词上。
    *   **维持输出分布：** 关键点在于，最终的输出序列在统计上与直接使用大模型自回归生成的序列**完全一致**。验证步骤确保了大模型始终拥有最终决定权，要么接受草案词（如果它本来就会这么选），要么自己生成一个词（当它不同意草案时）。没有任何词是由小模型单独决定的。

4.  **加速效果与关键因素：**
    *   **加速比：** 通常可以达到 **2倍到4倍甚至更高** 的端到端解码速度提升（Token 生成速率）。具体数值取决于：
        *   **小模型与大模型的速度比：** 小模型越快越好。
        *   **小模型与大模型的质量对齐度：** 小模型预测的分布与大模型越接近，草案被接受的概率越高，加速效果越好。如果小模型太差，会导致频繁拒绝，反而可能降低速度。
        *   **草案长度 `K`：** `K` 越大，单次迭代可能生成的词越多，但被拒绝的风险也越高（草案越长，出错概率累积），且大模型并行验证的计算量也增加。存在一个最优的 `K` 值（通常经验值为 4-8）。
        *   **文本内容：** 在文本连贯、可预测性强的部分，小模型容易猜对，加速效果好；在需要创造性、转折或专业知识的部分，拒绝率可能升高。

5.  **优点：**
    *   **显著加速：** 主要优势，效果非常明显。
    *   **无损输出质量：** 理论上保证输出分布与大模型自回归解码相同（前提是验证机制实现正确）。
    *   **通用性：** 适用于各种自回归生成模型（Decoder-only LLM）。
    *   **易于集成：** 可以相对独立地应用于已有的推理系统上。

6.  **挑战与缺点：**
    *   **需要额外的小模型：** 需要训练或选择一个与目标大模型在行为上尽可能对齐的优质小模型。这会增加一些开发和管理成本。
    *   **内存开销：** 需要同时加载大模型和小模型，增加了显存/内存占用。
    *   **拒绝开销：** 当草案被拒绝时，需要大模型额外生成一个词，这部分时间会略微抵消加速效果。优化拒绝处理很重要。
    *   **最优 `K` 的选择：** `K` 的选择需要根据模型对和具体任务进行调整。
    *   **首词延迟：** 对生成第一个词（Token 0）没有加速效果，因为此时没有上下文供小模型生成草案。

**总结：**

Speculative Decoding 通过让小模型“推测”未来多个词作为草案，然后让大模型**一次并行计算**来验证整个草案，大幅减少了昂贵的大模型调用次数。只要小模型能提供高质量的草案（被大模型接受的概率高），就能在**完全不改变输出内容质量**的前提下，将大型语言模型的推理解码速度提升数倍。它已成为当前加速 LLM 推理的最重要、最实用的技术之一，被广泛应用于各种推理框架和云服务中。