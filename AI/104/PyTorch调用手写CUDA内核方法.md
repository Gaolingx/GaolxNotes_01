# PyTorchè°ƒç”¨æ‰‹å†™CUDAå†…æ ¸æ–¹æ³•

### ğŸ§‘â€ğŸ’» User

pytorchä¸­å¦‚ä½•è°ƒç”¨æ‰‹å†™çš„cuda kernelï¼Ÿ

---

### ğŸ¤– Assistant

# åœ¨PyTorchä¸­è°ƒç”¨æ‰‹å†™çš„CUDAå†…æ ¸

åœ¨PyTorchä¸­è°ƒç”¨æ‰‹å†™çš„CUDAå†…æ ¸é€šå¸¸æœ‰å‡ ç§æ–¹æ³•ï¼Œä¸‹é¢æˆ‘å°†ä»‹ç»æœ€å¸¸ç”¨çš„å‡ ç§æ–¹å¼ï¼š

## æ–¹æ³•1ï¼šä½¿ç”¨PyTorchçš„CUDAæ‰©å±•API

PyTorchæä¾›äº†`torch.utils.cpp_extension`æ¨¡å—æ¥ç®€åŒ–CUDAå†…æ ¸çš„é›†æˆï¼š

```python
import torch
from torch.utils.cpp_extension import load

# ç›´æ¥åŠ è½½CUDAæ‰©å±•
cuda_module = load(
    name='custom_kernel',
    sources=['kernel.cu'],
    extra_cflags=['-O2'],
    verbose=True)

# è°ƒç”¨å†…æ ¸
def custom_function(input_tensor):
    return cuda_module.custom_operation(input_tensor)
```

å¯¹åº”çš„`kernel.cu`æ–‡ä»¶ç¤ºä¾‹ï¼š

```cpp
#include <torch/extension.h>

__global__ void custom_kernel(const float* input, float* output, int size) {
    const int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        output[idx] = input[idx] * 2.0f;  // ç¤ºä¾‹æ“ä½œ
    }
}

torch::Tensor custom_operation(torch::Tensor input) {
    auto output = torch::zeros_like(input);
    const int threads = 256;
    const int blocks = (input.numel() + threads - 1) / threads;
    
    custom_kernel<<<blocks, threads>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        input.numel());
    
    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("custom_operation", &custom_operation, "Custom CUDA operation");
}
```

## æ–¹æ³•2ï¼šä½¿ç”¨NVRTCå³æ—¶ç¼–è¯‘

å¯¹äºæ›´åŠ¨æ€çš„åœºæ™¯ï¼Œå¯ä»¥ä½¿ç”¨NVRTCè¿›è¡Œè¿è¡Œæ—¶ç¼–è¯‘ï¼š

```python
import torch
from torch.utils.cpp_extension import load_inline

# CUDAå†…æ ¸ä»£ç å­—ç¬¦ä¸²
cuda_src = '''
__global__ void custom_kernel(const float* input, float* output, int size) {
    const int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        output[idx] = input[idx] * 2.0f;
    }
}

torch::Tensor custom_operation(torch::Tensor input) {
    auto output = torch::zeros_like(input);
    const int threads = 256;
    const int blocks = (input.numel() + threads - 1) / threads;
    
    custom_kernel<<<blocks, threads>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        input.numel());
    
    return output;
}
'''

# å³æ—¶ç¼–è¯‘
module = load_inline(
    name='inline_kernel',
    cpp_sources='',
    cuda_sources=cuda_src,
    functions=['custom_operation'])

# è°ƒç”¨
output = module.custom_operation(input_tensor)
```

## æ–¹æ³•3ï¼šä½¿ç”¨pybind11å’Œsetuptoolsæ„å»ºæ‰©å±•

å¯¹äºæ›´å¤æ‚çš„é¡¹ç›®ï¼Œå¯ä»¥åˆ›å»ºå®Œæ•´çš„æ‰©å±•æ¨¡å—ï¼š

1. åˆ›å»º`setup.py`:

```python
from setuptools import setup
from torch.utils.cpp_extension import CUDAExtension, BuildExtension

setup(
    name='custom_cuda',
    ext_modules=[
        CUDAExtension('custom_cuda', [
            'custom_kernel.cu',
        ])
    ],
    cmdclass={
        'build_ext': BuildExtension
    })
```

2. å®‰è£…æ‰©å±•ï¼š
```bash
python setup.py install
```

3. åœ¨Pythonä¸­ä½¿ç”¨ï¼š
```python
import torch
import custom_cuda

output = custom_cuda.custom_operation(input_tensor)
```

## æ³¨æ„äº‹é¡¹

1. **å†…å­˜ç®¡ç†**ï¼šPyTorchå¼ é‡å·²ç»å¤„ç†äº†å†…å­˜ç®¡ç†ï¼Œé€šå¸¸ä¸éœ€è¦æ‰‹åŠ¨åˆ†é…/é‡Šæ”¾å†…å­˜
2. **åŒæ­¥/å¼‚æ­¥**ï¼šCUDAå†…æ ¸è°ƒç”¨é»˜è®¤æ˜¯å¼‚æ­¥çš„ï¼Œå¿…è¦æ—¶ä½¿ç”¨`cudaDeviceSynchronize()`
3. **é”™è¯¯æ£€æŸ¥**ï¼šå†…æ ¸å¯åŠ¨åæ£€æŸ¥é”™è¯¯`cudaError_t err = cudaGetLastError()`
4. **ç±»å‹å®‰å…¨**ï¼šç¡®ä¿å¼ é‡ç±»å‹ä¸å†…æ ¸æœŸæœ›çš„ç±»å‹åŒ¹é…
5. **æ€§èƒ½**ï¼šä¼˜åŒ–å—å¤§å°å’Œç½‘æ ¼å¤§å°ä»¥è·å¾—æœ€ä½³æ€§èƒ½

## ç¤ºä¾‹ï¼šå‘é‡åŠ æ³•

å®Œæ•´ç¤ºä¾‹ - å‘é‡åŠ æ³•ï¼š

`vector_add.cu`:
```cpp
#include <torch/extension.h>

__global__ void vector_add_kernel(
    const float* a, const float* b, float* c, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        c[idx] = a[idx] + b[idx];
    }
}

torch::Tensor vector_add(torch::Tensor a, torch::Tensor b) {
    CHECK_INPUT(a);
    CHECK_INPUT(b);
    auto c = torch::zeros_like(a);
    
    int threads = 256;
    int blocks = (a.numel() + threads - 1) / threads;
    
    vector_add_kernel<<<blocks, threads>>>(a.data_ptr<float>(),
                                         b.data_ptr<float>(),
                                         c.data_ptr<float>(),
                                         a.numel());
    
    return c;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("vector_add", &vector_add, "Vector add with CUDA");
}
```

Pythonç«¯ï¼š
```python
import torch
from torch.utils.cpp_extension import load

cuda_module = load(name='vector_add', sources=['vector_add.cu'])

a = torch.randn(1000, device='cuda')
b = torch.randn(1000, device='cuda')
c = cuda_module.vector_add(a, b)
```

ä»¥ä¸Šæ–¹æ³•éƒ½å¯ä»¥åœ¨PyTorchä¸­æˆåŠŸé›†æˆå’Œè°ƒç”¨æ‰‹å†™çš„CUDAå†…æ ¸ã€‚é€‰æ‹©å“ªç§æ–¹æ³•å–å†³äºé¡¹ç›®çš„å¤æ‚æ€§å’Œéœ€æ±‚ã€‚