# RoPEåœ¨LLaMA2ä¸­çš„åº”ç”¨

### ğŸ§‘â€ğŸ’» User

ä»¥llama archä¸ºä¾‹ï¼Œropeä½ç½®ç¼–ç æ˜¯å¦‚ä½•åµŒå…¥qkçš„ï¼Ÿï¼ˆç»“åˆhttps://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.pyä»£ç åˆ†æï¼‰

---

### ğŸ¤– Assistant

å¥½çš„ â€” ä¸‹é¢æˆ‘ç»“åˆ transformers ä¸­ LLaMA æ¶æ„ï¼Œä»¥åŠå…¶ `modeling_llama.py` å¯¹ Rotary Position Embedding (RoPE) çš„å®ç°ï¼Œè¯´æ˜ â€œRoPE æ˜¯æ€æ ·åµŒå…¥åˆ° Q/K (query/key) å‘é‡ä¸­â€ çš„ â€” å¹¶æŒ‡å‡ºå®ç°ç»†èŠ‚ + æ³¨æ„äº‹é¡¹ã€‚

---

## ğŸ” èƒŒæ™¯ï¼šä¸ºä»€ä¹ˆ LLaMA ç”¨ RoPE

* RoPE çš„åŸºæœ¬æ€æƒ³ä¸æ˜¯å°†ä½ç½®ç¼–ç å‘é‡ç›´æ¥â€œåŠ â€åˆ° token embedding ä¸Šï¼ˆåƒä¼ ç»Ÿçš„ç»å¯¹ positional embeddingï¼‰ï¼Œè€Œæ˜¯ä»¥**æ—‹è½¬ (rotation)** çš„æ–¹å¼æŠŠä½ç½®ä¿¡æ¯èå…¥åˆ° Q / K å‘é‡ä¸­ã€‚è¿™æ ·åšçš„å¥½å¤„æ˜¯ attention çš„ç‚¹ç§¯ `qÂ·k` å°†å¤©ç„¶åœ°åæ˜  **ç›¸å¯¹ä½ç½® (relative position)**ã€‚([LearnOpenCV][1])
* ç›¸æ¯”ä¼ ç»Ÿ absolute-embeddingï¼ŒRoPE æ”¯æŒæ›´å¥½çš„ç›¸å¯¹ä½ç½®æ¨ç†ï¼Œä¹Ÿæ›´å®¹æ˜“å¤–æ¨ (extrapolate) åˆ°æ¯”è®­ç»ƒæ—¶æ›´é•¿çš„ context é•¿åº¦ (é…åˆæŸäº› scaling / interpolation æŠ€æœ¯) ã€‚([LearnOpenCV][1])
* å› æ­¤ï¼ŒLLaMA åœ¨æ¯ä¸ª attention layer ä¸­ï¼Œå¯¹å…¶ Q å’Œ K éƒ½åº”ç”¨ RoPEï¼Œè€Œä¸æ˜¯åœ¨ embedding é˜¶æ®µåŠ ä½ç½®ç¼–ç ã€‚([Metric Coders][2])

---

## ğŸ“„ LLaMA ä¸­ RoPE çš„ä»£ç  â€” å¤§è‡´æµç¨‹ (åŸºäº modeling_llama.py)

åœ¨ transformers çš„å®ç° ( `modeling_llama.py`) ä¸­ï¼Œæœ‰ä¸€ä¸ªç±»å’Œå‡½æ•°è´Ÿè´£ RoPEï¼æ—‹è½¬ä½ç½®ç¼–ç  (rotary embedding)ï¼š

* `class LlamaRotaryEmbedding(nn.Module)`ï¼šè´Ÿè´£åˆå§‹åŒ–é¢‘ç‡ (inverse-frequency)ï¼Œä»¥åŠç¼“å­˜ cos / sin å€¼ã€‚([GitHub][3])
* `def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1)`ï¼šå°† cos / sin (ä»¥åŠ position_ids) åº”ç”¨åˆ° q å’Œ k ä¸Šã€‚è¿™ä¸ªå‡½æ•°å³æ˜¯å°†æ—‹è½¬ â€œæ–½åŠ  (apply)â€ åˆ° Q/K å‘é‡çš„åœ°æ–¹ã€‚([GitHub][4])

å¤§è‡´æµç¨‹åˆ†ä¸ºä»¥ä¸‹å‡ æ­¥ï¼š

1. **é¢‘ç‡ (inv_freq) å‡†å¤‡**

   * `inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2) / dim))` â€”â€” è¿™é‡Œ `dim` æ˜¯ head_dimï¼ˆæˆ– Q/K çš„æœ€åä¸€ç»´ç»´åº¦ï¼‰ï¼Œ`base` é»˜è®¤ä¸º 10000ã€‚è¿™ä¸ç»å…¸çš„ sinusoidal positional embedding çš„é¢‘ç‡ç”Ÿæˆæ–¹å¼ç±»ä¼¼ï¼Œåªä¸è¿‡è¿™é‡Œæ˜¯ä¸º â€œæ¯ä¸¤ä¸ªç»´åº¦ (even-odd pair)â€ ç”Ÿæˆä¸€ä¸ªé¢‘ç‡ã€‚([clvsit ä¸ªäººåšå®¢][5])
   * ä¹‹åä¸ºæœ€å¤§ position (ä¾‹å¦‚æœ€å¤§ context é•¿åº¦) ç”Ÿæˆ position â†” Î¸ (è§’åº¦) çš„è¡¨ (precompute cos å’Œ sin)ï¼Œå¹¶ç¼“å­˜åœ¨ `cos_cached`, `sin_cached`ã€‚([53AI][6])

2. **Q / K æŠ•å½± (linear)**

   * è¾“å…¥ hidden_states ç»çº¿æ€§å±‚åˆ†åˆ«æŠ•å½±å¾—åˆ° raw `q`, `k`, `v`ã€‚
   * åœ¨å¾—åˆ° `q` å’Œ `k` ä¹‹å (ä½†åœ¨ attention scoring ä¹‹å‰)ï¼Œè°ƒç”¨ `apply_rotary_pos_emb(q, k, cos, sin, position_ids)`ã€‚([CSDNåšå®¢][7])

3. **åº”ç”¨ RoPE â€” rotate half**

   * `apply_rotary_pos_emb` å†…éƒ¨ä½¿ç”¨ä¸€ç§ç§°ä¸º `rotate_half(x)` çš„æ“ä½œï¼ŒæŠŠæ¯ä¸ªå‘é‡ç»´åº¦çš„ä¸€åŠ (ååŠ) å’Œå‰åŠæ‹†æˆä¸¤éƒ¨åˆ† (x1, x2)ã€‚å…·ä½“ä»£ç å¤§è‡´å¦‚ä¸‹ (ä¼ªä»£ç )ï¼š

     ```text
     x1 = x[..., : x.shape[-1]//2]
     x2 = x[..., x.shape[-1]//2:]
     return concat(-x2, x1, dim=-1)
     ```

     è¿™æ˜¯ä¸€ç§æŠŠåä¸€åŠæ˜ å°„åˆ°å‰åŠ + å‰åŠæ˜ å°„åˆ°åä¸€åŠ (with sign flip) çš„æ“ä½œ â€”â€” ç›¸å½“äºåœ¨æ¯å¯¹å¶æ•°/å¥‡æ•°ç»´åº¦ä¸Šæ—‹è½¬ 90Â°ã€‚([GitHub][4])

   * ç„¶åï¼ŒçœŸæ­£çš„ `cos, sin` è§’åº¦ä¹Ÿä¼šè¢«åŠ å…¥ (ç‚¹ä¹˜ / åŠ æƒ) â€”â€” é€šå¸¸æ˜¯ç±»ä¼¼ä¸‹é¢ (æ•°å­¦) çš„æ“ä½œ (å¯¹æ¯ä¸€å¯¹ç»´åº¦)ï¼š

     [
     \begin{pmatrix} q_{2i} \ q_{2i+1} \end{pmatrix} \rightarrow
     \begin{pmatrix} \cos Î¸ & -\sin Î¸ \ \sin Î¸ & \cos Î¸ \end{pmatrix}
     \begin{pmatrix} q_{2i} \ q_{2i+1} \end{pmatrix}
     ]

     åŒç†å¯¹ K åšç›¸åŒæ—‹è½¬ (ä½ç½®è§’åº¦ä½¿ç”¨å¯¹åº” token çš„ position_id)ã€‚è¿™æ˜¯ç»å…¸å¤æ•° (complex) ä¹˜æ³•åœ¨å®ç»´ (real-dim) ä¸Šç­‰ä»·çš„å®ç°æ–¹å¼ã€‚([CSDNåšå®¢][7])

   * æœ€ç»ˆåˆå¹¶ (concatenate) å›åŸæ¥çš„ç»´åº¦é¡ºåº (real vectors) ä½œä¸ºå¸¦ä½ç½®ç¼–ç  (rotated) çš„ Q / Kã€‚([CSDNåšå®¢][7])

4. **Attention ç‚¹ç§¯**

   * ä½¿ç”¨æ—‹è½¬åçš„ `q`, `k` è¿›è¡Œ scaled-dot-product attentionã€‚ç”±äºå¯¹ q å’Œ k éƒ½åšäº† position-dependent çš„æ—‹è½¬ (cos/sin)ï¼Œæ‰€ä»¥æœ€ç»ˆ `q Â· k` çš„å€¼ä¸­ï¼Œä¼šéšå¼åœ°åŒ…å«ä¸¤ token çš„ **è§’åº¦å·® (Î¸_m â€“ Î¸_n)**ï¼Œä¹Ÿå°±æ˜¯å®ƒä»¬çš„ **ç›¸å¯¹ä½ç½® (m â€“ n)**ã€‚è¿™ä½¿å¾— attention æˆä¸º relative-position awareã€‚([LearnOpenCV][1])

---

## ğŸ§® æ•°å­¦ä¸ç›´è§‰è§£é‡Š

è®¾ token åœ¨åºåˆ—ä¸­ä½ç½®åˆ†åˆ«ä¸º m, nï¼›å¯¹ Q å‘é‡ (æ¥è‡ª token m) å’Œ K å‘é‡ (æ¥è‡ª token n)ï¼š

* Q_raw = W_q x_m,   K_raw = W_k x_n

ä½¿ç”¨ RoPE åï¼Œç›¸å½“äº

```
Q = R(Î¸_m) Â· Q_raw  
K = R(Î¸_n) Â· K_raw  
```

å…¶ä¸­ R(Î¸) æ˜¯å¯¹å¶æ•°/å¥‡æ•°ç»´åº¦åšäºŒç»´æ—‹è½¬ (cos, â€“sin; sin, cos)ã€‚

äºæ˜¯ attention logit

```
Q Â· K  = (R(Î¸_m) Q_raw) Â· (R(Î¸_n) K_raw)
       = Q_raw Â· (R(Î¸_m)áµ€ R(Î¸_n)) Â· K_raw
       = Q_raw Â· R(Î¸_n â€“ Î¸_m) Â· K_raw
```

ä¹Ÿå°±æ˜¯è¯´ï¼Œåªä¾èµ–äº (Î¸_n â€“ Î¸_m)ï¼Œä¹Ÿå°±æ˜¯ä¸¤ä¸ª token çš„ **ç›¸å¯¹ä½ç½®ä¿¡æ¯ (n â€“ m)** â€”â€” è¿™æ­£ä½“ç°äº† RoPE çš„ç›¸å¯¹ä½ç½® (relative position) ç‰¹æ€§ã€‚([CSDNåšå®¢][7])

å¯¹äºå¤šç»´ hidden dimension (head_dim)ï¼Œè¿™ç§æ—‹è½¬æ˜¯å¯¹æ¯ä¸€å¯¹ (even, odd) ç»´åº¦åš â€œå°æ—‹è½¬ (2D-rotation)â€ â€” æ€»ä½“ä¸Šç›¸å½“äºå¯¹é«˜ç»´ç©ºé—´åšå¤šä¸ª independent çš„æ—‹è½¬å­ç©ºé—´ (2D-plane) æ—‹è½¬ã€‚

---

## âš ï¸ æ³¨æ„äº‹é¡¹ / å®ç°ç»†èŠ‚ (å°¤å…¶ç»“åˆ transformers çš„å®ç°)

* åœ¨å®ç°é‡Œï¼Œ`apply_rotary_pos_emb` å…ˆåš `rotate_half` è¿™æ ·çš„ç»´åº¦æ‹†åˆ† (x1, x2)ï¼Œç„¶åç”¨ cached çš„ `cos`, `sin` (åŸºäº position_ids) åšåŠ æƒ / æ—‹è½¬ã€‚([GitHub][4])
* æœ‰äººæŒ‡å‡º (åœ¨ GH issue) transformers çš„å®ç°å’Œ â€œåŸå§‹ paper /å®˜æ–¹ LLaMA å®ç°â€ ä¹‹é—´æœ‰ç»†å¾®å·®åˆ« (rotary çš„å…·ä½“é¡ºåº /ç»´åº¦å¦‚ä½•æ‹†åˆ† /cos/sin çš„ unsqueeze /å¹¿æ’­) ã€‚([GitHub][4])
* å› ä¸ºæ˜¯å¯¹ Q å’Œ K åŒæ—¶åšæ—‹è½¬ (è€Œ V ä¸åš)ï¼Œæ‰€ä»¥ä½ç½®ç¼–ç åªé€šè¿‡ QÂ·K çš„ç›¸å¯¹è§’åº¦ä½“ç°ï¼Œä¸ä¼šæ”¹å˜ V çš„å†…å®¹ (value representation) æœ¬èº«ã€‚è¿™æ ·è®¾è®¡å¯ä»¥ä¿æŒ value çš„è¡¨ç¤ºä¸å˜ï¼Œä»…åœ¨ attention æƒé‡ (æ³¨æ„åŠ›è¯„åˆ†) ä¸­å¼•å…¥ä½ç½®ä¿¡æ¯ã€‚
* å¯¹äº â€œé•¿ä¸Šä¸‹æ–‡ (long context)â€ / extrapolationï¼ŒåŸºç¡€ RoPE æœ¬èº«æ”¯æŒæ— é™æ‰©å±• (å› ä¸º cos/sin æ˜¯å‘¨æœŸå‡½æ•°)ï¼Œä½†åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œæœ‰äº›å®ç°/é…ç½® (ä¾‹å¦‚ scaling factor, base, position_ids) å¯èƒ½é™åˆ¶æœ€å¤§ context é•¿åº¦ã€‚è‹¥æƒ³æ‰©å±•ï¼Œéœ€è¦é¢å¤–çš„ trick (å¦‚æŸäº›è®ºæ–‡æˆ–æ‰©å±•æ–¹æ³•) ã€‚([LearnOpenCV][1])

---

## âœ… æ€»ç»“ â€” LLaMA ä¸­ RoPE åµŒå…¥ q/k çš„æœºåˆ¶

1. LLaMA æ²¡æœ‰ç”¨ä¼ ç»Ÿçš„åŠ æ€§ positional embeddingï¼›è€Œæ˜¯é€šè¿‡ RoPEï¼Œå°†ä½ç½®ä¿¡æ¯ä»¥æ—‹è½¬ (rotation) çš„å½¢å¼æ³¨å…¥åˆ° Q å’Œ K å‘é‡ã€‚
2. åœ¨ `modeling_llama.py` ä¸­ï¼Œ`LlamaRotaryEmbedding` è´Ÿè´£é¢‘ç‡ (inv_freq) å‡†å¤‡ï¼Œå¹¶é¢„è®¡ç®— cos / sinï¼›`apply_rotary_pos_emb` å°† cos / sin + position_ids åº”ç”¨äº Q / Kã€‚
3. æ—‹è½¬å®é™…ä¸Šæ˜¯åœ¨æ¯å¯¹ç›¸é‚»ç»´åº¦ (even, odd) ä¸Šåš 2D æ—‹è½¬ (ç›¸å½“äºå¤æ•°ä¹˜ä»¥ (e^{i Î¸}))ã€‚
4. æœ€ç»ˆ attention çš„ dot-product (qÂ·k) å°±éšå¼åœ°åŒ…å«äº†ä¸¤ä¸ª token çš„ç›¸å¯¹ä½ç½® (ä½ç½®å·®) ä¿¡æ¯ â€”â€” å› æ­¤ attention å¯ä»¥æ˜¯ position-aware (relative) è€Œä¸æ˜¯åªèƒ½åŸºäº contentã€‚

---

å¦‚æœä½ åŒæ„çš„è¯ï¼Œæˆ‘å¯ä»¥**è´´å‡º (æˆ–ä¼ªä»£ç ) è§£é‡Šç‰ˆæœ¬** â€”â€” ç”¨ä½ æ›´ç†Ÿæ‚‰çš„æ•°å­¦ /ä»£ç é£æ ¼ (æ¯”å¦‚ä½ ä¹‹å‰åšçš„é‡åŒ– /ä¼˜åŒ–èƒŒæ™¯) æ¥æè¿° RoPE åœ¨ LLaMA ä¸­çš„ q/k æ³¨å…¥æµç¨‹ï¼Œè¿™æ ·ä½ åœ¨åš quantization / fused kernel çš„æ—¶å€™æ›´å¥½å¯¹é½ã€‚ä½ è¦æˆ‘å¸®ä½ ç”Ÿæˆå—ï¼Ÿ

[1]: https://learnopencv.com/rope-position-embeddings/?utm_source=chatgpt.com "Inside RoPE: Rotary Magic into Position Embeddings"
[2]: https://www.metriccoders.com/post/under-the-hood-of-llama-decoding-its-transformer-architecture?utm_source=chatgpt.com "Under the Hood of LLaMA: Decoding its Transformer Architecture"
[3]: https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py "transformers/src/transformers/models/llama/modeling_llama.py at main Â· huggingface/transformers Â· GitHub"
[4]: https://github.com/huggingface/transformers/issues/25199?utm_source=chatgpt.com "[LLaMA] Rotary positional embedding differs with official implementation Â· Issue #25199 Â· huggingface/transformers Â· GitHub"
[5]: https://clvsit.github.io/RoPE-%E7%9B%B8%E5%AF%B9%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E8%A7%A3%E8%AF%BB%E4%B8%8E%E5%A4%96%E6%8E%A8%E6%80%A7%E7%A0%94%E7%A9%B6/?utm_source=chatgpt.com "RoPE ç›¸å¯¹ä½ç½®ç¼–ç è§£è¯»ä¸å¤–æ¨æ€§ç ”ç©¶ | clvsit ä¸ªäººåšå®¢"
[6]: https://www.53ai.com/news/qianyanjishu/1291.html?utm_source=chatgpt.com "ä¸€æ–‡çœ‹æ‡‚ LLaMA ä¸­çš„æ—‹è½¬å¼ä½ç½®ç¼–ç ï¼ˆRotary Position Embeddingï¼‰ - 53AI-AIçŸ¥è¯†åº“|ä¼ä¸šAIçŸ¥è¯†åº“|å¤§æ¨¡å‹çŸ¥è¯†åº“|AIHub"
[7]: https://blog.csdn.net/angel_hben/article/details/132475879?utm_source=chatgpt.com "LLaMAä¸­ROPEä½ç½®ç¼–ç å®ç°æºç è§£æ-CSDNåšå®¢"
