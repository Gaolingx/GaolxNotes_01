# RoPEåœ¨LLaMA2ä¸­çš„åº”ç”¨

### ğŸ§‘â€ğŸ’» User

ä»¥llama archä¸ºä¾‹ï¼Œropeä½ç½®ç¼–ç æ˜¯å¦‚ä½•åµŒå…¥qkçš„ï¼Ÿï¼ˆç»“åˆhttps://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.pyä»£ç åˆ†æï¼‰

---

### ğŸ¤– Assistant

ç»“åˆ Hugging Face çš„ `transformers` åº“ä¸­ `modeling_llama.py` çš„æºç é€»è¾‘ä»¥åŠä½ æä¾›çš„ä»£ç ç‰‡æ®µï¼ŒLlama æ¶æ„ä¸­ RoPEï¼ˆRotary Positional Embeddingsï¼Œæ—‹è½¬ä½ç½®ç¼–ç ï¼‰åµŒå…¥ Qï¼ˆQueryï¼‰å’Œ Kï¼ˆKeyï¼‰çš„è¿‡ç¨‹å¯ä»¥åˆ†ä¸ºä¸‰ä¸ªæ ¸å¿ƒæ­¥éª¤ï¼š**åˆå§‹åŒ–é¢‘ç‡**ã€**è®¡ç®—æ—‹è½¬è¾…åŠ©å˜é‡**ã€**å¯¹ Q å’Œ K è¿›è¡Œæ—‹è½¬å˜æ¢**ã€‚

ä»¥ä¸‹æ˜¯è¯¦ç»†çš„ä»£ç é€»è¾‘åˆ†æï¼š

### 1. æ ¸å¿ƒåŸç†ç®€è¿°
RoPE ä¸åƒ BERT é‚£æ ·å°†ä½ç½®å‘é‡ç›´æ¥**åŠ **åœ¨ Embedding ä¸Šï¼Œè€Œæ˜¯é€šè¿‡å°† Query å’Œ Key å‘é‡åœ¨å¤æ•°åŸŸï¼ˆæˆ–è€…çœ‹ä½œä¸¤ä¸¤ä¸€ç»„ï¼‰è¿›è¡Œ**æ—‹è½¬**ï¼Œä»è€Œåœ¨åšç‚¹ç§¯ï¼ˆAttention Scoreï¼‰æ—¶è‡ªç„¶åœ°å¼•å…¥ç›¸å¯¹ä½ç½®ä¿¡æ¯ã€‚

æ•°å­¦å…¬å¼æ ¸å¿ƒä¸ºï¼š
$$
\text{RoPE}(x, m) = x \cdot \cos(m\theta) + \text{rotate\_half}(x) \cdot \sin(m\theta)
$$
å…¶ä¸­ $x$ æ˜¯ Q æˆ– K å‘é‡ï¼Œ$m$ æ˜¯ä½ç½®ç´¢å¼•ï¼Œ$\theta$ æ˜¯é¢‘ç‡ã€‚

---

### 2. ä»£ç å®ç°æµç¨‹åˆ†æ

åŸºäº Hugging Face çš„å®ç°é€»è¾‘ï¼Œæ•´ä¸ªè¿‡ç¨‹å‘ç”Ÿåœ¨ `LlamaAttention` æ¨¡å—ä¸­ã€‚

#### ç¬¬ä¸€æ­¥ï¼šåˆå§‹åŒ–é¢‘ç‡ (Init)
åœ¨æ¨¡å‹åˆå§‹åŒ–é˜¶æ®µï¼ˆå¦‚ä½ æä¾›çš„ Snippet 1 æ‰€ç¤ºï¼‰ï¼Œä¼šæ ¹æ®é…ç½®é€‰æ‹©ä¸åŒçš„ RoPE å®ç°ç±»ï¼ˆå¦‚ `LlamaRotaryEmbedding`ï¼‰ã€‚

```python
# å¯¹åº” Snippet 1 çš„é€»è¾‘
class LlamaRotaryEmbedding(nn.Module):
    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):
        super().__init__()
        # è®¡ç®—é€†é¢‘ç‡ inv_freq
        # å…¬å¼: theta_i = 1 / (base ^ (2i / d))
        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim))
        self.register_buffer("inv_freq", inv_freq, persistent=False)
      
        # é¢„è®¡ç®— cos å’Œ sin ç¼“å­˜ (cos_cached, sin_cached)
        self._set_cos_sin_cache(
            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()
        )
```
**å…³é”®ç‚¹**ï¼šè¿™é‡Œç”Ÿæˆäº†ä¸€ä¸ª `inv_freq` å‘é‡ï¼Œå†³å®šäº†æ—‹è½¬çš„è§’åº¦ç²’åº¦ã€‚å¦‚æœä½¿ç”¨äº† `rope_scaling`ï¼ˆçº¿æ€§æˆ–åŠ¨æ€NTKï¼‰ï¼Œä¼šåœ¨è¿™é‡Œè°ƒæ•´é¢‘ç‡çš„è®¡ç®—æ–¹å¼ï¼Œä½†åç»­åº”ç”¨é€»è¾‘ä¸å˜ã€‚

#### ç¬¬äºŒæ­¥ï¼šå‡†å¤‡æ—‹è½¬çŸ©é˜µ (Prepare Cos/Sin)
åœ¨ `LlamaAttention.forward` è¿‡ç¨‹ä¸­ï¼ŒQ å’Œ K ç»è¿‡çº¿æ€§å±‚æŠ•å½±å¹¶ reshape æˆ `[batch_size, num_heads, seq_len, head_dim]` åï¼Œéœ€è¦è·å–å½“å‰åºåˆ—å¯¹åº”çš„ `cos` å’Œ `sin` å€¼ã€‚

```python
# åœ¨ LlamaAttention.forward ä¸­
def forward(self, hidden_states, ...):
    # 1. çº¿æ€§æŠ•å½±å¾—åˆ° Q, K, V
    query_states = self.q_proj(hidden_states)
    key_states = self.k_proj(hidden_states)
  
    # ... Reshape & Transpose å˜æˆ [bsz, heads, seq_len, head_dim] ...

    # 2. è·å–å½“å‰åºåˆ—é•¿åº¦å¯¹åº”çš„ cos å’Œ sin
    # rot_emb å®é™…ä¸Šè°ƒç”¨ LlamaRotaryEmbedding.forward
    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)
```

#### ç¬¬ä¸‰æ­¥ï¼šæ‰§è¡Œæ—‹è½¬å˜æ¢ (Apply RoPE)
è¿™æ˜¯é—®é¢˜çš„æ ¸å¿ƒï¼ŒRoPE æ˜¯å¦‚ä½•â€œåµŒå…¥â€è¿›å»çš„ã€‚HF å®šä¹‰äº†ä¸€ä¸ªè¾…åŠ©å‡½æ•° `rotate_half` å’Œä¸»å‡½æ•° `apply_rotary_pos_emb`ã€‚

**è¾…åŠ©å‡½æ•°ï¼šrotate_half**
å°†å‘é‡çš„ä¸€åŠå–è´Ÿå¹¶äº¤æ¢ä½ç½®ï¼Œæ¨¡æ‹Ÿå¤æ•°æ—‹è½¬ä¸­çš„ $i(x+iy) = -y+ix$ æ“ä½œã€‚
```python
def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    # å‡è®¾ x çš„æœ€åç»´åº¦æ˜¯ head_dim
    x1 = x[..., : x.shape[-1] // 2] # å‰åŠéƒ¨åˆ†
    x2 = x[..., x.shape[-1] // 2 :] # ååŠéƒ¨åˆ†
    # ç»“æœæ‹¼æ¥ï¼š[-x2, x1]
    return torch.cat((-x2, x1), dim=-1)
```

**ä¸»å‡½æ•°ï¼šapply_rotary_pos_emb**
è¿™é‡Œå°† Q å’Œ K ä¸ä½ç½®ç¼–ç èåˆã€‚
```python
def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):
    """
    q, k: [batch_size, num_heads, seq_len, head_dim]
    cos, sin: [seq_len, head_dim] (é¢„è®¡ç®—å¥½çš„)
    position_ids: [batch_size, seq_len] (æ¯ä¸ªtokençš„ä½ç½®ç´¢å¼•)
    """
  
    # 1. æ ¹æ® position_ids æå–å¯¹åº”çš„ cos å’Œ sin
    #    ç»“æœå½¢çŠ¶å˜ä¸º [batch_size, 1, seq_len, head_dim] ä»¥ä¾¿å¹¿æ’­
    cos = cos[position_ids].unsqueeze(unsqueeze_dim)
    sin = sin[position_ids].unsqueeze(unsqueeze_dim)
  
    # 2. æ‰§è¡Œæ—‹è½¬å…¬å¼
    # q_embed = (q * cos) + (rotate_half(q) * sin)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
  
    return q_embed, k_embed
```

---

### 3. æ€»ç»“ï¼šåµŒå…¥ QK çš„å…·ä½“åŠ¨ä½œ

åœ¨ Hugging Face çš„ Llama å®ç°ä¸­ï¼ŒRoPE çš„åµŒå…¥å‘ç”Ÿåœ¨**Attention æ ¸å¿ƒè®¡ç®—ï¼ˆDot Productï¼‰ä¹‹å‰**ï¼š

1.  **è¾“å…¥**ï¼šæŠ•å½±åçš„ `query_states` å’Œ `key_states`ã€‚
2.  **å˜æ¢**ï¼š
    *   ä¿ç•™åŸå‘é‡ `q` ä¹˜ä»¥ $\cos$ã€‚
    *   å°†åŸå‘é‡ `q` åˆ‡åˆ†ã€ç¿»è½¬ã€éƒ¨åˆ†å–è´Ÿå¾—åˆ° `rotate_half(q)`ï¼Œç„¶åä¹˜ä»¥ $\sin$ã€‚
    *   å°†ä¸Šè¿°ä¸¤éƒ¨åˆ†ç›¸åŠ ã€‚
3.  **å…¬å¼åŒ–è¡¨è¾¾**ï¼š
    $$ Q_{\text{rotated}} = Q \otimes \cos + \text{Rotate}(Q) \otimes \sin $$
    $$ K_{\text{rotated}} = K \otimes \cos + \text{Rotate}(K) \otimes \sin $$
4.  **åç»­**ï¼šä½¿ç”¨æ—‹è½¬åçš„ $Q_{\text{rotated}}$ å’Œ $K_{\text{rotated}}$ è¿›è¡ŒçŸ©é˜µä¹˜æ³•è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼š
    $$ \text{AttnScore} = \text{Softmax}\left(\frac{Q_{\text{rotated}} \cdot K_{\text{rotated}}^T}{\sqrt{d}}\right) $$

é€šè¿‡è¿™ç§æ–¹å¼ï¼Œä½ç½®ä¿¡æ¯è¢«æ³¨å…¥åˆ°äº† Q å’Œ K çš„å‘é‡æ–¹å‘ä¸­ï¼Œä½¿å¾—æ³¨æ„åŠ›æœºåˆ¶èƒ½å¤Ÿé€šè¿‡å‘é‡å¤¹è§’æ„ŸçŸ¥ token ä¹‹é—´çš„ç›¸å¯¹è·ç¦»ã€‚