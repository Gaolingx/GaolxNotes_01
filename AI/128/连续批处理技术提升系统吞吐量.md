# 连续批处理技术提升系统吞吐量

### 🧑‍💻 User

vllm等主流推理框架中的continuous batching是如何实现的，他是如何提高系统并发吞吐量的？

---

### 🤖 Assistant

vLLM、TGI（Text Generation Inference）等主流推理框架中的 **Continuous Batching**（也称为 **Iteration-Level Batching** 或 **Dynamic Batching**）是一种**动态批处理技术**，它通过在模型推理的**每次迭代（iteration）** 中动态调整批次大小和组成，显著提高GPU利用率和系统吞吐量。其核心思想是**打破请求间的计算屏障，允许不同请求在不同时间加入或离开批次**。

---

### 一、Continuous Batching 的实现原理
以下是其关键实现机制：

#### 1. **请求队列与调度器**
   - 框架维护一个**待处理请求队列**（Pending Request Queue）。
   - 调度器在每个迭代步骤开始前，动态选择一批**可执行的请求**加入当前批次。选择策略包括：
     - **已缓存的请求**（KV Cache已存在，无需重新计算）。
     - **新到达的请求**（需要初始化KV Cache）。
     - **优先处理延迟敏感的请求**（可选策略）。

#### 2. **迭代级别的动态批处理**
   - **传统静态批处理**：所有请求必须同时开始/结束（如下图左）。
   - **Continuous Batching**：每个请求独立执行，每次迭代只处理当前批次中**所有请求的下一个token**（如下图右）：
     ```
     迭代1: [Req1-t1, Req2-t1, Req3-t1]  
     迭代2: [Req1-t2, Req2-t2, Req4-t1]  // Req3完成退出，Req4加入
     迭代3: [Req1-t3, Req2-t3, Req4-t2]  // 持续动态调整
     ```

#### 3. **KV Cache 的精细管理**
   - **核心挑战**：不同请求的KV Cache长度不同，需高效管理。
   - **解决方案**：
     - **PagedAttention（vLLM的核心）**：将KV Cache划分为固定大小的块（类似内存分页），通过页表动态映射到请求。
     - **内存复用**：完成请求的KV Cache块立即释放，供新请求使用。

#### 4. **异步执行与流水线**
   - **计算与调度解耦**：
     - GPU计算当前批次时，CPU异步准备下一批次的请求（数据加载、预处理等）。
   - **非阻塞调度**：已完成请求的结果返回与下一批次的调度并行执行。

---

### 二、Continuous Batching 如何提升吞吐量
#### 1. **提高 GPU 利用率**
   - **消除空闲等待**：传统批处理中GPU需等待整个批次完成才能处理新请求（尾部空闲）。Continuous Batching 让GPU持续工作，空闲时间趋近于0。
   - **填充计算间隙**：短请求完成后立即释放资源，新请求可无缝加入。

#### 2. **降低平均延迟**
   - **短请求快速退出**：生成长度短的请求无需等待长请求完成。
   - **高优先级调度**：可结合SLA策略优先处理延迟敏感请求。

#### 3. **高效内存利用**
   - **PagedAttention 的优势**（vLLM）：
     - 减少KV Cache碎片，提升内存利用率。
     - 支持比最大序列长度更大的并发请求量。
   - **按需分配**：只为当前激活的请求保留KV Cache。

#### 4. **自适应批次大小**
   - **动态调整**：根据请求到达速率和GPU负载，自动扩展/收缩批次大小。
   - **处理突发流量**：突然涌入的请求可快速加入正在运行的批次。

---

### 三、与传统静态批处理的对比
| **特性**               | **静态批处理**                     | **Continuous Batching**          |
|------------------------|----------------------------------|----------------------------------|
| **批次边界**           | 请求必须同时开始/结束              | 每次迭代动态调整批次              |
| **GPU利用率**          | 尾部空闲严重                      | 接近100%持续计算                 |
| **内存效率**           | 预分配固定内存，可能浪费           | 动态分配/释放（如PagedAttention）|
| **长尾延迟**           | 高（受最长请求阻塞）               | 低（短请求可提前退出）            |
| **适用场景**           | 离线批量推理                      | 在线高并发服务                   |

---

### 四、性能提升实例（vLLM）
- **吞吐量**：在LLaMA-7B上，比HuggingFace Transformers提升**24倍**。
- **并发能力**：支持比静态批处理高**5-10倍**的并发请求量。
- **延迟稳定性**：P99延迟下降50%以上（尤其在高负载时）。

---

### 五、技术挑战与优化方向
1. **调度算法**：如何平衡吞吐量/公平性/优先级（如SLA感知调度）。
2. **内存碎片**：尽管PagedAttention缓解，极端场景仍需优化。
3. **异构请求**：混合不同模型/参数的请求（e.g., 多LoRA适配器）。
4. **硬件适配**：针对NVLink/InfiniBand等优化跨GPU KV Cache交换。

---

### 总结
**Continuous Batching** 通过**迭代级动态批处理** + **KV Cache精细管理**（如vLLM的PagedAttention）彻底解决了传统批处理的资源闲置问题。其本质是**将批处理粒度从“请求级”细化到“Token级”**，使GPU的计算能力持续饱和，从而在在线推理场景中实现吞吐量与延迟的双重优化。这一技术已成为大模型高并发服务的**标配解决方案**。

---

### 🧑‍💻 User

在PagedAttention以前，kv cache一般基于output_length预分配的，output_lenght往往很大，导致显存占用很大，比如Bloom 175B output_lenght=1024时显存占用3.9G。但实际生成的output_lenght往往很小，导致显存的巨大浪费。显存的浪费导致，导致无法组很大的batch, 吞吐量上不去。那PagedAttention是如何高效管理kv cache，从而可以配合continuous batch提高batch数，提升吞吐量的？

---

### 🤖 Assistant

PagedAttention 的核心突破在于将操作系统的**虚拟内存分页机制**引入到 KV Cache 管理，通过 **物理内存块（Block）的动态分配/释放** 和 **逻辑地址映射** 解决了传统预分配方式的显存浪费问题。以下是其关键技术实现及如何提升吞吐量的详细分析：

---

### 一、传统 KV Cache 管理的痛点（以 Bloom 175B 为例）
- **固定预分配**：每个请求预先分配 `[output_length, hidden_size]` 的连续显存。
- **显存浪费**： 
  - 示例：`hidden_size=12288, output_length=1024, dtype=float16`  
    单条 KV Cache 大小 = `2层 x 1024 x 12288 x 2字节 ≈ 49.15MB`  
    显存占用 = `49.15MB x batch_size`
  - **问题**：实际生成长度可能仅 20~100 token，但预分配显存无法释放，导致：
    - 单条请求浪费 90%+ 显存
    - 最大 batch_size 被严重限制（如从 100 降至 10）

---

### 二、PagedAttention 的解决方案
#### 1. **KV Cache 分块存储**
   - **物理块（Block）**：
     - 将显存划分为 **固定大小的物理块**（如 16/128个token容量）。
     - 块大小需权衡：太小增加管理开销，太大降低灵活性。
   - **逻辑序列映射**：
     - 每个请求的 KV Cache 被拆分为多个**逻辑块**，通过**页表**映射到物理块。
     - 类似虚拟内存：序列的连续逻辑地址 → 离散物理块。

#### 2. **动态按需分配**
   - **分配时机**：
     - 当序列生成新 token 时，检查当前块剩余空间。
     - 若空间不足，**从空闲池申请新物理块**（无需预分配全程显存）。
   - **释放机制**：
     - 请求完成后，其占用的所有物理块**立即归还空闲池**。
     - 新请求可复用这些块。

#### 3. **分页注意力计算**
   - **Attention 计算适配**：
     - 传统 Attention 要求输入连续内存，而物理块离散存储。
     - PagedAttention 修改计算内核：
       1. 根据页表**收集离散块**中的 K/V 数据。
       2. 重组为**连续内存视图**（不拷贝数据，仅修改指针）。
       3. 执行分块矩阵运算。
   - **零显存复制**：通过指针重定向避免数据搬运开销。

---

### 三、如何提升 Continuous Batching 的吞吐量
#### 1. **显存利用率质变**
   | **场景**              | 传统方式（预分配） | PagedAttention     |
   |----------------------|------------------|-------------------|
   | **单请求峰值显存**   | 49.15MB         | **3.07MB** (16块x 16token) |
   | **实际使用率**       | ≈10%            | **>90%**          |
   - **效果**：同等显存下，**batch_size 提升 5-10倍**（e.g. 从 10 → 100）。

#### 2. **消除外部碎片**
   - **传统痛点**：预分配长序列导致显存碎片化，无法容纳新请求。
   - **PagedAttention 策略**：
     - 物理块大小固定，**空闲池统一管理**。
     - 新请求按需获取块，**碎片仅存在于最后一个块内**（内部碎片 < 块大小）。

#### 3. **高效处理变长序列**
   - **动态扩展**：长序列自动申请新块，短序列仅占用少量块。
   - **混合负载支持**：同一批次内可混合处理：
     - 刚启动的短序列（1个块）
     - 生成长文本的请求（数十个块）

#### 4. **显存复用加速调度**
   - **瞬时释放**：请求完成后，其物理块**秒级回归空闲池**。
   - **Continuous Batching 配合**：
     - 调度器立即将新请求加入批次。
     - **批处理不中断**：GPU 持续满载计算。

---

### 四、性能提升实例分析
假设 **A100 80G GPU**，模型为 **Bloom 175B**：
- **传统方式**：  
  单请求峰值显存 49.15MB → 最大 batch_size ≈ **1,600**  
  实际平均生成长度 50 token → **有效利用率仅 5%** → 有效吞吐 ≈ 80 请求/秒

- **PagedAttention**：  
  单请求平均占 4个块（16token/块） ≈ 12.3MB  
  最大 batch_size ≈ **6,500**  
  显存利用率 >85% → 有效吞吐 ≈ **3,250 请求/秒**  
  **>>> 吞吐量提升 40倍！**

---

### 五、工程实现关键点
1. **块大小选择**：
   - 权衡：16token/块（灵活性高） vs 128token/块（管理开销低）。
   - vLLM 默认：**16 token/块**（适合中短文本场景）。

2. **页表管理优化**：
   - 使用 GPU 加速的**哈希表**存储逻辑→物理映射。
   - 支持 $O(1)$ 复杂度的块查询。

3. **空洞块处理**：
   - 序列中断生成时（e.g. 用户取消），立即回收未使用块。

4. **跨块注意力计算**：
   - 当序列跨多个块时，高效合并相邻块的 K/V 矩阵。

---

### 总结
**PagedAttention 通过“显存分页化”实现了：**
1. **按需分配** → 显存占用从 $O(max\_length)$ 降为 $O(actual\_length)$  
2. **零浪费回收** → 显存利用率从 <10% 提升至 >90%  
3. **物理块复用** → 支持秒级加入新请求  

**与 Continuous Batching 协同效应：**  
- **显存高效 → Batch Size 激增 → GPU 计算饱和 → 吞吐量飞跃**  
- **动态调度 + 即时回收 → 尾部延迟降低 50%+**  

这一机制使 vLLM 在同等硬件条件下，吞吐量达到传统方案的 **10-24倍**，成为大模型高并发推理的基石技术。