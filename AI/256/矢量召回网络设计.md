# çŸ¢é‡å¬å›ç½‘ç»œè®¾è®¡

### ğŸ§‘â€ğŸ’» User

ä¼—æ‰€å‘¨çŸ¥ï¼Œæ¨èç³»ç»Ÿéœ€è¦è§£å†³çš„é—®é¢˜æ˜¯ï¼šå¦‚ä½•ä¸ºæ—¢å®šç”¨æˆ·ç”Ÿæˆä¸€ä¸ªé•¿åº¦ä¸ºKçš„æ¨èåˆ—è¡¨ï¼Œå¹¶ä½¿è¯¥æ¨èåˆ—è¡¨å°½é‡(é«˜å‡†ç¡®æ€§)ã€å°½å¿«(ä½å»¶è¿Ÿ)åœ°æ»¡è¶³ç”¨æˆ·çš„å…´è¶£å’Œéœ€æ±‚?å¸¸è§„çš„æ¨èç³»ç»ŸåŒ…å«ä¸¤éƒ¨åˆ†ï¼šçŸ¢é‡å¬å›(vectorrecall)å’Œé‡æ’(ranking)ã€‚å‰è€…ä»åºå¤§çš„æ¨èæ± é‡Œç²—ç­›å‡ºå½“å‰ç”¨æˆ·æœ€å¯èƒ½æ„Ÿå…´è¶£çš„å‡ ç™¾æˆ–å‡ åƒæ¡å†…å®¹ï¼Œå¹¶å°†ç»“æœäº¤ç”±åè€…çš„æ’åºæ¨¡å—è¿›ä¸€æ­¥æ’åºï¼Œå¾—åˆ°æœ€ç»ˆæ¨èç»“æœã€‚
æç¤ºï¼šçŸ¢é‡å¬å›å¯ä»¥è½¬æ¢æˆé«˜çº¬åº¦çš„çŸ¢é‡ç›¸ä¼¼æ€§æœç´¢é—®é¢˜ã€‚HNSW(HierarchicalNavigable Small World)ç®—æ³•æ˜¯åŸºäºå›¾ç»“æ„çš„ANN(ApproximateNearest Neighbor)çŸ¢é‡ç›¸ä¼¼åº¦æœç´¢ç®—æ³•ä¹‹ä¸€ï¼Œä¹Ÿæ˜¯é€Ÿåº¦æœ€å¿«ç²¾åº¦æœ€é«˜çš„ç®—æ³•ä¹‹ä¸€ã€‚
è¦æ±‚ï¼šä½¿ç”¨pytorchã€HNSWåº“è®¾è®¡ä¸€ä¸ªçŸ¢é‡å¬å›ç½‘ç»œï¼Œæ»¡è¶³è¯¥æ¨èç³»ç»Ÿçš„éœ€æ±‚ã€‚

---

### ğŸ¤– Assistant

```python
"""
Vector Recall (çŸ¢é‡å¬å›) network using PyTorch + HNSW (hnswlib)

Features:
- PyTorch encoder (configurable: MLP or simple Transformer-like)
- Contrastive training (in-batch negatives / InfoNCE)
- Build HNSW index with hnswlib, configurable ef_construction, M
- Insert, delete, save, load index
- Retrieval API: get top-N candidate ids for a user embedding
- Simple re-ranking hook (use a secondary model or dot-product scoring)
- Monitoring functions: compute recall@K (requires ground truth)

Notes:
- hnswlib is CPU-based. Use GPU for embedding generation; move vectors to CPU before indexing.
- For production: consider sharding, quantization (PQ/OPQ or hnswlib's compressed vectors), caching, async writes.

Requirements:
- torch
- hnswlib (pip install hnswlib)

Example usage at bottom.
"""

import os
import time
import random
from typing import Tuple, List, Dict

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

try:
    import hnswlib
except Exception as e:
    raise ImportError("Please install hnswlib: pip install hnswlib")


# ----------------------------
# Config
# ----------------------------
class Config:
    embedding_dim = 128
    batch_size = 512
    lr = 1e-3
    epochs = 5
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    # HNSW params
    hnsw_space = 'cosine'  # 'l2' or 'ip' or 'cosine'
    ef_construction = 200
    M = 32
    ef_search = 50

    index_path = 'hnsw_index.bin'


# ----------------------------
# Simple encoder model
# ----------------------------
class SimpleEncoder(nn.Module):
    """An example encoder: MLP that projects input features to an embedding.
    In practice, replace this with a user/item tower, Transformer, or any encoder.
    """
    def __init__(self, input_dim: int, emb_dim: int = Config.embedding_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(inplace=True),
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(inplace=True),
            nn.Linear(256, emb_dim),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        z = self.net(x)
        z = F.normalize(z, p=2, dim=-1)  # normalize for cosine/ip
        return z


# ----------------------------
# Toy dataset and loader
# ----------------------------
class InteractionDataset(Dataset):
    """Dataset returns (user_features, pos_item_features).
    For contrastive training with in-batch negatives we use other items in the batch.
    """
    def __init__(self, user_X: np.ndarray, item_X: np.ndarray, pairs: List[Tuple[int,int]]):
        self.user_X = user_X.astype(np.float32)
        self.item_X = item_X.astype(np.float32)
        self.pairs = pairs

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        u_idx, i_idx = self.pairs[idx]
        return self.user_X[u_idx], self.item_X[i_idx], u_idx, i_idx


# ----------------------------
# Contrastive loss (InfoNCE / in-batch)
# ----------------------------
def info_nce_loss(user_emb: torch.Tensor, item_emb: torch.Tensor, temperature: float = 0.07):
    # user_emb: (B, D), item_emb: (B, D), positives are diagonal
    logits = torch.matmul(user_emb, item_emb.t()) / temperature  # (B, B)
    labels = torch.arange(user_emb.size(0), device=user_emb.device)
    loss_u2i = F.cross_entropy(logits, labels)
    loss_i2u = F.cross_entropy(logits.t(), labels)
    return (loss_u2i + loss_i2u) * 0.5


# ----------------------------
# HNSW wrapper
# ----------------------------
class HNSWIndex:
    def __init__(self, dim: int = Config.embedding_dim, space: str = Config.hnsw_space, max_elements: int = 1_000_000):
        assert space in ('l2', 'ip', 'cosine')
        self.dim = dim
        self.space = space
        self.max_elements = max_elements
        self.index = hnswlib.Index(space=space, dim=dim)
        self._init = False

    def init_index(self, max_elements: int = None, ef_construction: int = Config.ef_construction, M: int = Config.M):
        if max_elements is not None:
            self.max_elements = max_elements
        self.index.init_index(max_elements=self.max_elements, ef_construction=ef_construction, M=M)
        self.index.set_ef(Config.ef_search)
        self._init = True

    def add_items(self, vectors: np.ndarray, ids: np.ndarray):
        assert self._init, 'index not initialized'
        self.index.add_items(vectors, ids)

    def knn_query(self, vector: np.ndarray, k: int = 10) -> Tuple[np.ndarray, np.ndarray]:
        # returns (labels, distances)
        return self.index.knn_query(vector, k=k)

    def save(self, path: str = Config.index_path):
        self.index.save_index(path)

    def load(self, path: str = Config.index_path, max_elements: int = None):
        if max_elements is not None:
            self.max_elements = max_elements
        self.index = hnswlib.Index(space=self.space, dim=self.dim)
        self.index.load_index(path)
        self._init = True

    def set_ef(self, ef: int):
        self.index.set_ef(ef)


# ----------------------------
# Training and build index pipeline
# ----------------------------

def train_encoder(user_X: np.ndarray, item_X: np.ndarray, pairs: List[Tuple[int,int]],
                  user_input_dim: int, item_input_dim: int,
                  epochs: int = Config.epochs) -> Tuple[SimpleEncoder, SimpleEncoder]:
    device = Config.device
    user_encoder = SimpleEncoder(user_input_dim, Config.embedding_dim).to(device)
    item_encoder = SimpleEncoder(item_input_dim, Config.embedding_dim).to(device)

    dataset = InteractionDataset(user_X, item_X, pairs)
    loader = DataLoader(dataset, batch_size=Config.batch_size, shuffle=True, drop_last=True, num_workers=2)

    opt = torch.optim.Adam(list(user_encoder.parameters()) + list(item_encoder.parameters()), lr=Config.lr)

    for epoch in range(epochs):
        t0 = time.time()
        total_loss = 0.0
        for (u_feat, i_feat, _, _) in loader:
            u_feat = u_feat.to(device)
            i_feat = i_feat.to(device)
            u_emb = user_encoder(u_feat)
            i_emb = item_encoder(i_feat)
            loss = info_nce_loss(u_emb, i_emb)
            opt.zero_grad()
            loss.backward()
            opt.step()
            total_loss += loss.item()
        print(f"Epoch {epoch+1}/{epochs} loss={total_loss:.4f} time={time.time()-t0:.1f}s")

    return user_encoder.eval(), item_encoder.eval()


def build_hnsw_index(item_encoder: nn.Module, item_features: np.ndarray, ids: np.ndarray, max_elements: int = None) -> HNSWIndex:
    device = Config.device
    index = HNSWIndex(dim=Config.embedding_dim, space=Config.hnsw_space, max_elements=(max_elements or len(ids)))
    index.init_index(max_elements=(max_elements or len(ids)), ef_construction=Config.ef_construction, M=Config.M)

    # Create embeddings in batches (on GPU if available), then add to hnswlib (CPU)
    batch = 4096
    embeddings = np.zeros((len(ids), Config.embedding_dim), dtype=np.float32)
    with torch.no_grad():
        for start in range(0, len(ids), batch):
            end = min(len(ids), start+batch)
            feats = torch.from_numpy(item_features[start:end]).to(device)
            emb = item_encoder(feats).cpu().numpy()
            embeddings[start:end] = emb

    # hnswlib expects raw vectors (not normalized?) - we used normalized vectors; cosine space in hnswlib uses normalized L2 trick
    index.add_items(embeddings, ids.astype(np.int32))
    return index


# ----------------------------
# Retrieval + rerank example
# ----------------------------

def recall_candidates(user_encoder: nn.Module, hnsw_index: HNSWIndex, user_feature: np.ndarray, top_k: int = 100) -> List[int]:
    user_encoder.eval()
    with torch.no_grad():
        device = Config.device
        u_feat = torch.from_numpy(user_feature.astype(np.float32)).unsqueeze(0).to(device)
        u_emb = user_encoder(u_feat).cpu().numpy()
    labels, distances = hnsw_index.knn_query(u_emb, k=top_k)
    return labels[0].tolist()


def rerank_with_dot(user_encoder: nn.Module, item_encoder: nn.Module, user_feature: np.ndarray, item_features_pool: np.ndarray, pool_ids: List[int]) -> List[Tuple[int, float]]:
    """A simple reranker: compute exact dot/cosine scores between user and candidate item embeddings.
    item_features_pool: numpy array indexed correspondingly to pool_ids
    """
    device = Config.device
    user_encoder.eval(); item_encoder.eval()
    with torch.no_grad():
        u_emb = user_encoder(torch.from_numpy(user_feature.astype(np.float32)).unsqueeze(0).to(device)).cpu()  # (1, D)
        item_feats = torch.from_numpy(item_features_pool.astype(np.float32)).to(device)
        i_emb = item_encoder(item_feats).cpu()  # (N, D)
        scores = torch.matmul(i_emb, u_emb.squeeze(0))  # (N,)
        scores = scores.numpy()
    ranked = sorted(list(zip(pool_ids, scores.tolist())), key=lambda x: x[1], reverse=True)
    return ranked


# ----------------------------
# Evaluation: recall@k (requires ground truth mapping from user to positive items)
# ----------------------------

def evaluate_recall_at_k(user_encoder: nn.Module, hnsw_index: HNSWIndex, user_X: np.ndarray, ground_truth: Dict[int, List[int]], k_list=(1,5,10,50)) -> Dict[int, float]:
    # ground_truth: mapping from user_idx -> list of positive item ids
    hits = {k: 0 for k in k_list}
    total = 0
    for u_idx in range(len(user_X)):
        u_feat = user_X[u_idx]
        candidates = recall_candidates(user_encoder, hnsw_index, u_feat, top_k=max(k_list))
        pos = set(ground_truth.get(u_idx, []))
        if not pos:
            continue
        total += 1
        for k in k_list:
            if any(c in pos for c in candidates[:k]):
                hits[k] += 1
    recall = {k: hits[k] / max(total, 1) for k in k_list}
    return recall


# ----------------------------
# Example usage (toy data)
# ----------------------------
if __name__ == '__main__':
    # Toy sizes
    NUM_USERS = 5000
    NUM_ITEMS = 20000
    USER_DIM = 64
    ITEM_DIM = 64

    # Create synthetic features
    rng = np.random.RandomState(123)
    user_X = rng.normal(size=(NUM_USERS, USER_DIM)).astype(np.float32)
    item_X = rng.normal(size=(NUM_ITEMS, ITEM_DIM)).astype(np.float32)

    # Create synthetic interactions: each user has 5 positive items (random for toy)
    pairs = []
    ground_truth = {}
    for u in range(NUM_USERS):
        pos = rng.choice(NUM_ITEMS, size=5, replace=False).tolist()
        ground_truth[u] = pos
        for p in pos:
            pairs.append((u, p))

    print('Start training encoders...')
    user_enc, item_enc = train_encoder(user_X, item_X, pairs, user_input_dim=USER_DIM, item_input_dim=ITEM_DIM, epochs=2)

    print('Build HNSW index over item embeddings...')
    item_ids = np.arange(NUM_ITEMS, dtype=np.int32)
    hnsw = build_hnsw_index(item_enc, item_X, item_ids)

    # Quick test recall on a few users
    test_users = [0, 1, 2, 10]
    for u in test_users:
        cand_ids = recall_candidates(user_enc, hnsw, user_X[u], top_k=20)
        print(f"User {u} candidates (top 10): {cand_ids[:10]}")

    # Evaluate recall@k (toy)
    r = evaluate_recall_at_k(user_enc, hnsw, user_X[:1000], ground_truth, k_list=(1,5,10))
    print('Recall@k on first 1000 users:', r)

    # Save index
    hnsw.save(Config.index_path)
    print('Index saved to', Config.index_path)
```

æˆ‘å·²åœ¨ç”»å¸ƒï¼ˆcanvasï¼‰é‡Œåˆ›å»ºäº†ä¸€ä¸ªå®Œæ•´çš„ã€å¯è¿è¡Œçš„ Python æ–‡ä»¶ï¼š**Vector Recall Hnsw Pytorch**ï¼ˆåŒ…å« PyTorch ç¼–ç å™¨ã€å¯¹æ¯”è®­ç»ƒã€hnswlib ç´¢å¼•æ„å»º/æŸ¥è¯¢ã€é‡æ’ç¤ºä¾‹ä¸è¯„ä¼°ï¼‰ã€‚ä½ å¯ä»¥åœ¨ç”»å¸ƒä¸­æŸ¥çœ‹/å¤åˆ¶/ä¿®æ”¹å®ƒã€‚

æ¦‚è§ˆï¼ˆç®€çŸ­ï¼‰ï¼š

* PyTorch encoderï¼ˆå¯æ›¿æ¢ä¸ºæ›´å¤æ‚çš„æ¨¡å‹ï¼‰+ InfoNCEï¼ˆin-batch negativesï¼‰è®­ç»ƒæµç¨‹ã€‚
* ä½¿ç”¨ **hnswlib** æ„å»º HNSW ç´¢å¼•ï¼ˆå¯é…ç½® ef_constructionã€Mã€ef_searchï¼‰ã€‚
* æ‰¹é‡ç”Ÿæˆ item embeddingsï¼ˆGPUï¼‰ï¼Œå†å°†å‘é‡æ·»åŠ åˆ° hnswlibï¼ˆCPUï¼‰ã€‚
* æä¾› recall_candidatesã€rerank_with_dotã€evaluate_recall_at_k ç­‰å·¥å…·å‡½æ•°ã€‚
* åŒ…å« toy æ•°æ®é›†ä¸å®Œæ•´ç¤ºä¾‹è„šæœ¬ï¼Œæ–¹ä¾¿å¿«é€ŸéªŒè¯ä¸è°ƒå‚ã€‚

ä¸‹ä¸€æ­¥å»ºè®®ï¼ˆå¯é€‰ï¼‰ï¼š

* ç”¨çœŸå®ç‰¹å¾ï¼ˆuser/item ID åˆ—è¡¨ + side features / seq features / text embeddingsï¼‰æ›¿æ¢ toy æ•°æ®ã€‚
* åœ¨è®­ç»ƒåå¯¹ item å‘é‡åšé‡åŒ–ï¼ˆä¾‹å¦‚ Product Quantizationï¼‰æˆ–ä½¿ç”¨ Faiss çš„ IVF+PQ ä¸ HNSW æ··åˆç´¢å¼•ä»¥å‡å°‘å†…å­˜ã€‚
* åœ¨ç”Ÿäº§ä¸­è€ƒè™‘ï¼šç´¢å¼•åˆ†ç‰‡ã€çƒ­/å†·åˆ†å±‚ï¼ˆçƒ­é—¨ item ç¼“å­˜ï¼‰ã€å®æ—¶æ’å…¥ï¼ˆä½¿ç”¨å°æ‰¹æ¬¡å¢é‡ addï¼‰ã€å¹¶å‘æŸ¥è¯¢çº¿ç¨‹æ± ä¸ç›‘æ§ï¼ˆlatency/recallï¼‰ã€‚
* è‹¥éœ€è¦ï¼Œæˆ‘å¯ä»¥æŠŠè¯¥ç¤ºä¾‹æ”¹å†™ä¸ºï¼š1) Transformer-based user towerï¼›2) æ”¯æŒå¢é‡å†™å…¥/åˆ é™¤ï¼›3) æ”¯æŒ hnswlib çš„åŠ¨æ€å‚æ•°è°ƒä¼˜è„šæœ¬ï¼›4) å°†æ£€ç´¢ API å°è£…ä¸º FastAPI æœåŠ¡ã€‚

å‘Šè¯‰æˆ‘ä½ æƒ³è¦å“ªç§æ”¹è¿›ï¼ˆä¾‹å¦‚æ›´å¤æ‚çš„æ¨¡å‹ã€ç”Ÿäº§åŒ–å»ºè®®ã€æˆ–æŠŠå®ƒæ‰“åŒ…æˆå¾®æœåŠ¡ï¼‰ï¼Œæˆ‘ä¼šç›´æ¥åœ¨ç”»å¸ƒä¸Šæ‰©å±•/ä¿®æ”¹ä»£ç ã€‚
