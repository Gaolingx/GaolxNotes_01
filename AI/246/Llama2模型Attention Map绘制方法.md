# Llama2æ¨¡å‹Attention Mapç»˜åˆ¶æ–¹æ³•

### ğŸ§‘â€ğŸ’» User

ä»¥llama2æ¶æ„ä¸ºä¾‹ï¼Œå¦‚ä½•ç»˜åˆ¶decoder-only transformeræ¨¡å‹ä¸åŒlayerä»¥åŠä¸åŒheadçš„attentionmapï¼Ÿ

---

### ğŸ¤– Assistant

å¥½çš„ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸æ·±å…¥ä¸”å®è·µæ€§å¾ˆå¼ºçš„é—®é¢˜ã€‚æˆ‘ä»¬å°†ä»¥ Llama 2 æ¶æ„ä¸ºä¾‹ï¼Œè¯¦ç»†è®²è§£å¦‚ä½•ç»˜åˆ¶ Decoder-Only Transformer æ¨¡å‹ä¸åŒå±‚ï¼ˆLayerï¼‰å’Œä¸åŒå¤´ï¼ˆHeadï¼‰çš„ Attention Mapã€‚

### æ ¸å¿ƒæ¦‚å¿µç†è§£

1.  **Decoder-Only Transformer**ï¼š åƒ Llama 2ã€GPT ç³»åˆ—è¿™æ ·çš„æ¨¡å‹å±äºæ­¤ç±»ã€‚å®ƒä»¬ä½¿ç”¨**å› æœæ©ç ï¼ˆCausal Maskï¼‰**ï¼Œç¡®ä¿æ¯ä¸ªä½ç½®åªèƒ½å…³æ³¨åˆ°å®ƒä¹‹å‰ï¼ˆåŒ…æ‹¬è‡ªèº«ï¼‰çš„ä½ç½®ï¼Œè€Œä¸èƒ½â€œçª¥è§æœªæ¥â€ã€‚è¿™åæ˜ åœ¨ Attention Map ä¸Šå°±æ˜¯ä¸€ä¸ªä¸Šä¸‰è§’çŸ©é˜µï¼ˆå³ä¸Šè§’ä¸º `-inf` æˆ–éå¸¸å°çš„å€¼ï¼Œç»è¿‡ softmax åå˜ä¸º 0ï¼‰ã€‚
2.  **Attention Map**ï¼š æœ¬è´¨ä¸Šå°±æ˜¯ç»è¿‡ Softmax å½’ä¸€åŒ–åçš„ Attention Weight çŸ©é˜µã€‚å¯¹äºä¸€ä¸ªç»™å®šçš„å¤´ï¼Œå…¶å¤§å°ä¸º `[target_seq_len, source_seq_len]`ã€‚åœ¨è‡ªå›å½’è§£ç ä¸­ï¼Œ`source` å’Œ `target` æ˜¯ç›¸åŒçš„åºåˆ—ï¼Œæ‰€ä»¥çŸ©é˜µæ˜¯æ–¹é˜µã€‚
3.  **è·å–æ–¹å¼**ï¼š éœ€è¦åœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œä»æ¨¡å‹çš„ç‰¹å®šå±‚å’Œç‰¹å®šå¤´ä¸­â€œé’©å–â€ï¼ˆhookï¼‰æˆ–ç›´æ¥è¿”å›è¿™ä¸ªæƒé‡çŸ©é˜µã€‚

---

### æ­¥éª¤è¯¦è§£

æˆ‘ä»¬å°†è¿‡ç¨‹åˆ†ä¸ºä¸‰ä¸ªä¸»è¦æ­¥éª¤ï¼š**æ¨¡å‹å‡†å¤‡**ã€**æ•°æ®å‰å‘ä¼ æ’­ä¸æƒé‡æŠ“å–**ã€**å¯è§†åŒ–ç»˜å›¾**ã€‚

#### ç¬¬ 1 æ­¥ï¼šæ¨¡å‹å‡†å¤‡

é¦–å…ˆï¼Œä½ éœ€è¦åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚ç”±äºåŸå§‹ Llama 2 æ¨¡å‹çš„å‰å‘ä¼ æ’­é»˜è®¤ä¸ä¼šè¿”å› Attention Weightsï¼Œæˆ‘ä»¬éœ€è¦é‡‡å–ä¸€äº›æ–¹æ³•å°†å…¶â€œé’©â€å‡ºæ¥ã€‚

**æ–¹æ³•ä¸€ï¼šä½¿ç”¨æ¨¡å‹çš„ `output_attentions=True` å‚æ•°ï¼ˆæ¨èä¸”ç®€å•ï¼‰**

Hugging Face Transformers åº“ä¸­çš„ Llama 2 å®ç°å·²ç»æ”¯æŒåœ¨è°ƒç”¨æ¨¡å‹æ—¶ç›´æ¥è¿”å›æ³¨æ„åŠ›æƒé‡ã€‚

```python
import torch
from transformers import LlamaForCausalLM, LlamaTokenizer
import matplotlib.pyplot as plt

# è®¾ç½®è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model_name = "meta-llama/Llama-2-7b-chat-hf" # ä»¥ 7B-chat ç‰ˆæœ¬ä¸ºä¾‹
tokenizer = LlamaTokenizer.from_pretrained(model_name)
model = LlamaForCausalLM.from_pretrained(model_name, 
                                         torch_dtype=torch.float16, # åŠç²¾åº¦ä»¥èŠ‚çœæ˜¾å­˜
                                         device_map="auto")

# éå¸¸é‡è¦ï¼šå¦‚æœtokenizeræ²¡æœ‰pad_tokenï¼Œå°†å…¶è®¾ç½®ä¸ºeos_token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model.eval() # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
```

**æ–¹æ³•äºŒï¼šä½¿ç”¨ PyTorch Hookï¼ˆæ›´åº•å±‚ï¼Œæ§åˆ¶æ›´çµæ´»ï¼‰**

å¦‚æœå‡ºäºæŸç§åŸå› ä½ éœ€è¦æ›´åº•å±‚çš„æ§åˆ¶ï¼Œå¯ä»¥ä½¿ç”¨ Hookã€‚è¿™ç§æ–¹æ³•æ›´å¤æ‚ï¼Œä½†å¯ä»¥è®¿é—®åˆ°ä¸­é—´å±‚çš„æ‰€æœ‰å˜é‡ã€‚

```python
# å®šä¹‰ä¸€ä¸ªå­—å…¸æ¥å­˜å‚¨æŠ“å–åˆ°çš„attention maps
attention_maps = {}

# å®šä¹‰hookå‡½æ•°
def get_attention_hook(layer_idx, head_idx):
    # è¿™ä¸ªhookä¼šåœ¨è¯¥æ¨¡å—çš„å‰å‘ä¼ æ’­å®Œæˆåè¢«è°ƒç”¨
    # module: è¯¥å±‚çš„Attentionæ¨¡å—
    # input: è¾“å…¥å…ƒç»„
    # output: è¾“å‡ºå…ƒç»„ (é€šå¸¸åŒ…å«hidden_states, attention_weights, ...)
    def hook(module, input, output):
        # output é€šå¸¸æ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œç¬¬äºŒä¸ªå…ƒç´ æ˜¯attention weights
        # attention_weights çš„å½¢çŠ¶: [batch_size, num_heads, seq_len, seq_len]
        attn_weights = output[1]
        # æˆ‘ä»¬å–batchä¸­çš„ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼Œå’ŒæŒ‡å®šçš„head
        # ä½¿ç”¨detach()å°†å…¶ä»è®¡ç®—å›¾ä¸­åˆ†ç¦»å¹¶ç§»åˆ°CPU
        attention_maps[f"layer_{layer_idx}_head_{head_idx}"] = attn_weights[0, head_idx].detach().cpu()
    return hook

# æ³¨å†Œhookã€‚æˆ‘ä»¬éœ€è¦éå†æ¨¡å‹æ‰¾åˆ°æ‰€æœ‰çš„attentionå±‚ã€‚
# LlamaModelçš„ç»“æ„æ˜¯ model.model -> LlamaModel
# ç„¶å model.model.layers æ˜¯ä¸€ä¸ªç”± LlamaDecoderLayer ç»„æˆçš„ModuleList
for layer_idx, layer in enumerate(model.model.layers):
    # æ¯ä¸€å±‚çš„self-attentionæ¨¡å—æ˜¯ layer.self_attn
    # æˆ‘ä»¬éœ€è¦é’©ä½å®ƒçš„è¾“å‡º
    hook = get_attention_hook(layer_idx, head_idx=0) # è¿™é‡Œä»¥æŠ“å–æ¯ä¸ªå±‚çš„ç¬¬0å¤´ä¸ºä¾‹
    handle = layer.self_attn.register_forward_hook(hook)
    # å¦‚æœä½ æƒ³ä¹‹åç§»é™¤hookï¼Œå¯ä»¥ä¿å­˜handle
```

#### ç¬¬ 2 æ­¥ï¼šæ•°æ®å‰å‘ä¼ æ’­ä¸æƒé‡æŠ“å–

ç°åœ¨æˆ‘ä»¬å¤„ç†è¾“å…¥æ–‡æœ¬ï¼Œå¹¶å°†å…¶é¦ˆé€åˆ°æ¨¡å‹ä¸­ã€‚

**ä½¿ç”¨æ–¹æ³•ä¸€ (`output_attentions`)ï¼š**

```python
# å‡†å¤‡è¾“å…¥
text = "The capital of France is"
inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
inputs = {k: v.to(device) for k, v in inputs.items()}

# å‰å‘ä¼ æ’­ï¼Œå…³é”®å‚æ•°ï¼šoutput_attentions=True
with torch.no_grad():
    outputs = model(**inputs, output_attentions=True)

# outputsç°åœ¨åŒ…å«ä¸€ä¸ªå…ƒç»„ â€˜attentionsâ€™
# attentions æ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œé•¿åº¦ä¸º num_hidden_layers
# æ¯ä¸ªå…ƒç´ çš„å½¢çŠ¶: [batch_size, num_heads, seq_len, seq_len]
all_attentions = outputs.attentions

# è·å–ç¬¬0å±‚çš„æ‰€æœ‰å¤´çš„attention map
# all_attentions[0] çš„å½¢çŠ¶: [1, 32, seq_len, seq_len] (å¯¹äºLlama-2-7Bï¼Œæœ‰32ä¸ªå¤´)
layer_0_attentions = all_attentions[0].squeeze(0) # ç§»é™¤batchç»´åº¦ -> [32, seq_len, seq_len]

# è·å–ç¬¬5å±‚ï¼Œç¬¬10ä¸ªå¤´çš„attention map
layer_idx = 5
head_idx = 10
attention_map_5_10 = all_attentions[layer_idx][0, head_idx].cpu().numpy()
```

**ä½¿ç”¨æ–¹æ³•äºŒ (Hook)ï¼š**
åœ¨è¿è¡Œå‰å‘ä¼ æ’­ä¹‹å‰ï¼Œç¡®ä¿ä½ å·²ç»æ³¨å†Œå¥½äº† Hookã€‚

```python
# æ¸…ç©ºä¹‹å‰å­˜å‚¨çš„attention maps
attention_maps.clear()

# å‡†å¤‡è¾“å…¥ï¼ˆåŒä¸Šï¼‰
text = "The capital of France is"
inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
inputs = {k: v.to(device) for k, v in inputs.items()}

# å‰å‘ä¼ æ’­
with torch.no_grad():
    outputs = model(**inputs) # ä¸éœ€è¦output_attentions=True

# ç°åœ¨ï¼Œattention_maps å­—å…¸ä¸­å·²ç»å­˜å‚¨äº†æ‰€æœ‰æ³¨å†Œhookçš„å±‚çš„attentionæ•°æ®
# ä¾‹å¦‚ï¼Œè®¿é—®ç¬¬0å±‚ç¬¬0å¤´çš„map
attention_map_0_0 = attention_maps[â€œlayer_0_head_0â€].numpy()
```

#### ç¬¬ 3 æ­¥ï¼šå¯è§†åŒ–ç»˜å›¾

ä½¿ç”¨ `matplotlib` çš„ `imshow` å‡½æ•°æ¥ç»˜åˆ¶çƒ­åŠ›å›¾ã€‚

```python
def plot_attention_map(attention_matrix, layer_idx, head_idx, tokens=None):
    """
    ç»˜åˆ¶æŒ‡å®šå¤´å’Œå±‚çš„attention mapã€‚
    
    å‚æ•°:
        attention_matrix: 2D numpyæ•°ç»„ [seq_len, seq_len]
        layer_idx: å±‚ç´¢å¼•ï¼Œç”¨äºæ ‡é¢˜
        head_idx: å¤´ç´¢å¼•ï¼Œç”¨äºæ ‡é¢˜
        tokens: å¯é€‰çš„tokenåˆ—è¡¨ï¼Œä½œä¸ºxå’Œyè½´çš„åˆ»åº¦æ ‡ç­¾
    """
    plt.figure(figsize=(10, 8))
    # ä½¿ç”¨imshowæ˜¾ç¤ºçŸ©é˜µï¼Œé¢œè‰²æ˜ å°„ç”¨viridisçœ‹èµ·æ¥ä¸é”™
    cax = plt.imshow(attention_matrix, cmap='viridis', interpolation='nearest')
    plt.colorbar(cax) # æ·»åŠ é¢œè‰²æ¡

    # è®¾ç½®æ ‡é¢˜å’Œè½´æ ‡ç­¾
    plt.title(f"Attention Map - Layer {layer_idx}, Head {head_idx}")
    plt.xlabel("Source Token Position")
    plt.ylabel("Target Token Position")

    # å¦‚æœæä¾›äº†tokensï¼Œè®¾ç½®åˆ»åº¦æ ‡ç­¾
    if tokens is not None:
        plt.xticks(range(len(tokens)), tokens, rotation=90)
        plt.yticks(range(len(tokens)), tokens)

    plt.tight_layout()
    plt.show()

# è·å–tokenizedçš„æ–‡æœ¬ï¼Œç”¨äºè½´æ ‡ç­¾
tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])

# ç»˜åˆ¶æˆ‘ä»¬ä¹‹å‰æå–çš„ç¬¬5å±‚ç¬¬10å¤´çš„å›¾
plot_attention_map(attention_map_5_10, layer_idx=5, head_idx=10, tokens=tokens)
```

**ç»“æœè§£è¯»ï¼š**
ç”Ÿæˆçš„å›¾åƒæ˜¯ä¸€ä¸ªæ–¹é˜µã€‚Y è½´æ˜¯**ç›®æ ‡ token**ï¼ˆæ­£åœ¨è®¡ç®—æ³¨æ„åŠ›çš„ tokenï¼‰ï¼ŒX è½´æ˜¯**æº token**ï¼ˆè¢«å…³æ³¨çš„ tokenï¼‰ã€‚ç”±äºå› æœæ©ç ï¼Œä½ ä¼šå‘ç°çŸ©é˜µçš„å³ä¸Šä¸‰è§’éƒ¨åˆ†ï¼ˆå¯¹åº”â€œå±•æœ›æœªæ¥â€ï¼‰çš„å€¼å‡ ä¹ä¸º 0ï¼ˆæ·±è‰²ï¼‰ã€‚å¯¹è§’çº¿åŠå…¶å·¦ä¸‹éƒ¨åˆ†åˆ™æ˜¾ç¤ºå‡ºä¸åŒçš„å…³æ³¨æ¨¡å¼ã€‚æ˜äº®çš„é»„è‰²ç‚¹è¡¨ç¤ºè¯¥ç›®æ ‡ token å¯¹æº token æœ‰å¾ˆå¼ºçš„å…³æ³¨ã€‚

---

### å®Œæ•´ä»£ç ç¤ºä¾‹ï¼ˆä½¿ç”¨æ–¹æ³•ä¸€ï¼‰

```python
import torch
from transformers import LlamaForCausalLM, LlamaTokenizer
import matplotlib.pyplot as plt

# 1. æ¨¡å‹å‡†å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_name = "meta-llama/Llama-2-7b-chat-hf"
tokenizer = LlamaTokenizer.from_pretrained(model_name)
model = LlamaForCausalLM.from_pretrained(model_name, 
                                         torch_dtype=torch.float16,
                                         device_map="auto")
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
model.eval()

# 2. æ•°æ®å¤„ç†ä¸å‰å‘ä¼ æ’­
text = "The capital of France is"
inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
inputs = {k: v.to(device) for k, v in inputs.items()}
tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])

with torch.no_grad():
    outputs = model(**inputs, output_attentions=True)

all_attentions = outputs.attentions # Tuple of layers

# 3. å¯è§†åŒ–ç‰¹å®šå±‚å’Œå¤´
layer_to_plot = 0  # æŸ¥çœ‹ç¬¬ä¸€å±‚
head_to_plot = 0   # æŸ¥çœ‹ç¬¬ä¸€ä¸ªå¤´

# æå–attention matrix
# all_attentions[layer_to_plot].shape = [1, num_heads, seq_len, seq_len]
attn_matrix = all_attentions[layer_to_plot][0, head_to_plot].cpu().numpy()

# 4. ç»˜å›¾
plt.figure(figsize=(10, 8))
plt.imshow(attn_matrix, cmap='viridis', interpolation='nearest')
plt.colorbar()
plt.title(f"Layer {layer_to_plot}, Head {head_to_plot}")
plt.xlabel("Source Token")
plt.ylabel("Target Token")
plt.xticks(range(len(tokens)), tokens, rotation=90)
plt.yticks(range(len(tokens)), tokens)
plt.tight_layout()
plt.show()
```

### è¿›é˜¶æŠ€å·§

*   **æ‰¹é‡ç»˜åˆ¶å¤šä¸ªå¤´/å±‚**ï¼š ä½ å¯ä»¥ä½¿ç”¨ `plt.subplots` åœ¨ä¸€ä¸ªå¤§å›¾ä¸­åˆ›å»ºå¤šä¸ªå­å›¾ï¼ˆsubplotï¼‰ï¼Œæ¥åŒæ—¶æ¯”è¾ƒåŒä¸€å±‚çš„ä¸åŒå¤´ï¼Œæˆ–è€…ä¸åŒå±‚çš„åŒä¸€ä¸ªå¤´ã€‚
*   **å¹³å‡æ³¨æ„åŠ›**ï¼š ä½ å¯ä»¥è®¡ç®—æŸä¸€å±‚æ‰€æœ‰å¤´çš„å¹³å‡æ³¨æ„åŠ› `torch.stack(all_attentions[layer_idx]).mean(dim=1)`ï¼Œæ¥è§‚å¯Ÿè¯¥å±‚çš„æ•´ä½“å…³æ³¨æ¨¡å¼ã€‚
*   **åˆ†æç‰¹å®šæ¨¡å¼**ï¼š è§‚å¯Ÿæ˜¯å¦æœ‰å¤´ä¸“é—¨å…³æ³¨[CLS] tokenã€ä¸‹ä¸€ä¸ªtokenã€æ ‡ç‚¹ç¬¦å·æˆ–é•¿è·ç¦»ä¾èµ–ã€‚Decoder-only æ¨¡å‹ä¸­çš„å¤´é€šå¸¸æœ‰æ›´ä¸“é—¨çš„è§’è‰²ï¼Œå¦‚â€œå‰ç¼€å¤´â€ã€â€œé‡å¤å¤´â€ç­‰ã€‚

é€šè¿‡ä»¥ä¸Šæ­¥éª¤ï¼Œä½ å°±å¯ä»¥æœ‰æ•ˆåœ°å¯è§†åŒ–å’Œåˆ†æ Llama 2 è¿™ç±» Decoder-Only æ¨¡å‹å†…éƒ¨çš„ç¥ç§˜ Attention æœºåˆ¶äº†ã€‚