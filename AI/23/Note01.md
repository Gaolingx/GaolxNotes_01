# Hugging Face Tokenizer åŠŸèƒ½ä¸ä½¿ç”¨ä»‹ç»

## Question 1

èƒ½å¦ä»‹ç»ä¸‹huggingface tokenizerï¼Ÿ

## Answer 1

Hugging Face çš„ **Tokenizer** æ˜¯ [Transformers åº“](https://github.com/huggingface/transformers) ä¸­ä¸€ä¸ªæ ¸å¿ƒç»„ä»¶ï¼Œç”¨äºå°†è‡ªç„¶è¯­è¨€æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥å¤„ç†çš„æ•°å­—å½¢å¼ï¼ˆå¦‚ token IDsï¼‰ã€‚å®ƒåœ¨ NLP ä»»åŠ¡ä¸­èµ·åˆ°å…³é”®ä½œç”¨ï¼Œæ”¯æŒå¤šç§é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚ BERTã€GPTã€T5 ç­‰ï¼‰ï¼Œå¹¶æä¾›äº†çµæ´»çš„åˆ†è¯ï¼ˆtokenizationï¼‰å’Œç¼–ç ï¼ˆencodingï¼‰åŠŸèƒ½ã€‚

---

### **Tokenizer çš„æ ¸å¿ƒåŠŸèƒ½**
1. **åˆ†è¯ï¼ˆTokenizationï¼‰**  
   å°†æ–‡æœ¬æ‹†åˆ†ä¸ºè¯ã€å­è¯æˆ–ç¬¦å·ï¼ˆtokenï¼‰ã€‚ä¾‹å¦‚ï¼š
   - **å•è¯çº§åˆ†è¯**ï¼š`"Hello world"` â†’ `["Hello", "world"]`
   - **å­è¯çº§åˆ†è¯**ï¼ˆå¦‚ BPEã€WordPieceï¼‰: `"unhappy"` â†’ `["un", "##happy"]`
   - **å­—ç¬¦çº§åˆ†è¯**ï¼š`"hello"` â†’ `["h", "e", "l", "l", "o"]`

2. **ç¼–ç ï¼ˆEncodingï¼‰**  
   å°† token è½¬æ¢ä¸ºæ¨¡å‹éœ€è¦çš„è¾“å…¥å½¢å¼ï¼ŒåŒ…æ‹¬ï¼š
   - Token IDsï¼ˆæ•´æ•°è¡¨ç¤ºï¼‰
   - Attention masksï¼ˆåŒºåˆ†æœ‰æ•ˆå†…å®¹ä¸å¡«å……ï¼‰
   - ç‰¹æ®Šæ ‡è®°ï¼ˆå¦‚ `[CLS]`ã€`[SEP]`ã€`<s>` ç­‰ï¼‰

3. **æ”¯æŒå¤šç§é¢„è®­ç»ƒæ¨¡å‹**  
   ä¸åŒæ¨¡å‹ä½¿ç”¨ä¸åŒçš„åˆ†è¯ç­–ç•¥ï¼Œä¾‹å¦‚ï¼š
   - **BERT**ï¼šWordPiece
   - **GPT-2**ï¼šByte-Pair Encoding (BPE)
   - **T5**ï¼šSentencePiece
   - **RoBERTa**ï¼šBPE

---

### **Tokenizer çš„ç»„æˆéƒ¨åˆ†**
1. **é¢„å¤„ç†å™¨ï¼ˆPre-tokenizationï¼‰**  
   - æŒ‰ç©ºæ ¼æˆ–æ ‡ç‚¹æ‹†åˆ†æ–‡æœ¬ã€‚
   - ä¾‹å¦‚ï¼š`"Don't stop!"` â†’ `["Don", "'", "t", "stop", "!"]`

2. **åˆ†è¯æ¨¡å‹ï¼ˆTokenizer Modelï¼‰**  
   - åº”ç”¨ç‰¹å®šç®—æ³•ï¼ˆå¦‚ BPEã€WordPieceï¼‰ç”Ÿæˆå­è¯ã€‚
   - ä¾‹å¦‚ï¼š`"embeddings"` â†’ `["em", "##bed", "##ding", "##s"]`

3. **åå¤„ç†å™¨ï¼ˆPost-processingï¼‰**  
   - æ·»åŠ ç‰¹æ®Šæ ‡è®°ï¼ˆå¦‚ `[CLS]`ã€`[SEP]`ï¼‰ã€‚
   - å¤„ç†å¥å­å¯¹è¾“å…¥ã€‚

---

### **ä½¿ç”¨ç¤ºä¾‹**
#### 1. åŠ è½½ Tokenizer
```python
from transformers import AutoTokenizer

# è‡ªåŠ¨æ ¹æ®æ¨¡å‹åç§°åŠ è½½å¯¹åº”çš„åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
```

#### 2. åŸºæœ¬åˆ†è¯ä¸ç¼–ç 
```python
text = "Hello, how are you?"

# åˆ†è¯
tokens = tokenizer.tokenize(text)
print(tokens)  # ["hello", ",", "how", "are", "you", "?"]

# ç¼–ç ä¸ºæ¨¡å‹è¾“å…¥
encoded_input = tokenizer(text, return_tensors="pt")
print(encoded_input)
# è¾“å‡º:
# {
#   'input_ids': tensor([[101, 7592, 1010, 2129, 2024, 2017, 1029, 102]]),
#   'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])
# }
```

#### 3. å¤„ç†å¥å­å¯¹
```python
text1 = "I love NLP."
text2 = "It's fascinating."

encoded_input = tokenizer(text1, text2, padding=True, truncation=True, return_tensors="pt")
```

---

### **å…³é”®ç‰¹æ€§**
1. **å¤šè¯­è¨€æ”¯æŒ**  
   æ”¯æŒ 100+ ç§è¯­è¨€çš„åˆ†è¯ï¼Œé€‚ç”¨äºè·¨è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ XLM-Rã€mBERTï¼‰ã€‚

2. **é«˜æ•ˆå¤„ç†**  
   åŸºäº Rust å®ç°çš„åº•å±‚åº“ [ğŸ¤— Tokenizers](https://github.com/huggingface/tokenizers)ï¼Œæ”¯æŒå¿«é€Ÿå¤„ç†å¤§è§„æ¨¡æ–‡æœ¬ã€‚

3. **è‡ªå®šä¹‰èƒ½åŠ›**  
   - å¯è®­ç»ƒè‡ªå·±çš„åˆ†è¯å™¨ï¼ˆå¦‚ä½¿ç”¨ `train_new_from_iterator`ï¼‰ã€‚
   - çµæ´»é…ç½®å¡«å……ï¼ˆ`padding`ï¼‰ã€æˆªæ–­ï¼ˆ`truncation`ï¼‰ã€æœ€å¤§é•¿åº¦ï¼ˆ`max_length`ï¼‰ã€‚

4. **å…¼å®¹æ€§**  
   ä¸ Hugging Face çš„ `Datasets`ã€`Trainer` ç­‰å·¥å…·æ— ç¼é›†æˆã€‚

---

### **å¸¸è§æ“ä½œ**
- **ä¿å­˜ä¸åŠ è½½åˆ†è¯å™¨**ï¼š
  ```python
  tokenizer.save_pretrained("./my_tokenizer")
  tokenizer = AutoTokenizer.from_pretrained("./my_tokenizer")
  ```

- **å¤„ç†ç‰¹æ®Šæ ‡è®°**ï¼š
  ```python
  tokenizer.add_special_tokens({"additional_special_tokens": ["[NEW_TOKEN]"]})
  ```

- **è§£ç ï¼ˆå°† IDs è½¬å›æ–‡æœ¬ï¼‰**ï¼š
  ```python
  decoded_text = tokenizer.decode(encoded_input["input_ids"][0])
  ```

---

### **åº•å±‚åº“ï¼šğŸ¤— Tokenizers**
Hugging Face çš„ [Tokenizers åº“](https://github.com/huggingface/tokenizers) æä¾›äº†é«˜æ€§èƒ½çš„åˆ†è¯å®ç°ï¼Œæ”¯æŒï¼š
- **BPEï¼ˆByte-Pair Encodingï¼‰**
- **WordPiece**
- **Unigram**
- **SentencePiece**

å¯ç›´æ¥ç”¨äºè®­ç»ƒè‡ªå®šä¹‰åˆ†è¯å™¨ï¼š
```python
from tokenizers import Tokenizer, models, trainers

tokenizer = Tokenizer(models.BPE())
trainer = trainers.BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]"])
tokenizer.train(files=["text.txt"], trainer=trainer)
```

---

### **æ€»ç»“**
Hugging Face çš„ Tokenizer æ˜¯ NLP æµç¨‹ä¸­çš„å…³é”®å·¥å…·ï¼Œé€šè¿‡ç»Ÿä¸€çš„æ¥å£æ”¯æŒå¤šç§é¢„è®­ç»ƒæ¨¡å‹ï¼Œç®€åŒ–äº†æ–‡æœ¬é¢„å¤„ç†æ­¥éª¤ã€‚æ— è®ºæ˜¯å¿«é€Ÿå®éªŒè¿˜æ˜¯ç”Ÿäº§éƒ¨ç½²ï¼Œå®ƒéƒ½èƒ½é«˜æ•ˆå¤„ç†æ–‡æœ¬æ•°æ®ï¼Œå¹¶ä¸ Transformers ç”Ÿæ€å®Œç¾é›†æˆã€‚