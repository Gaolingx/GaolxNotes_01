# LLM预训练优化器显存占用分析

### 🧑‍💻 User

llm预训练阶段载入的优化器需要多少显存？

---

### 🤖 Assistant

在大型语言模型（LLM）的预训练阶段，**优化器状态（Optimizer States）是显存占用的主要部分之一**。其显存消耗主要取决于三个关键因素：

### 📍 1. **优化器类型**
   - **Adam / AdamW**（最常用）：
     - 每个参数需要存储：
       - **参数本身的副本**（通常为FP32，4字节/参数）
       - **一阶动量（m）**（FP32，4字节/参数）
       - **二阶动量（v）**（FP32，4字节/参数）
     - **总计：每个参数约需 12 字节**。
   - **AdaFactor**（显存优化型）：
     - 压缩二阶动量（v），不存完整矩阵。
     - **每个参数约需 4~8 字节**（取决于实现）。
   - **SGD with Momentum**：
     - 需存储参数副本和动量（各4字节）。
     - **总计：每个参数约需 8 字节**。

### 📍 2. **模型参数量（N）**
   - 以 **Adam** 为例：优化器显存占用 ≈ `12 × N` 字节。
   - **经典模型示例**：
     - **GPT-3 175B 参数**：
       - 优化器显存 = `175 × 10^9 × 12 字节` ≈ **2.1 TB**（纯Adam状态）。
     - **Llama 2 70B**：
       - 优化器显存 = `70 × 10^9 × 12 字节` ≈ **840 GB**。

### 📍 3. **混合精度训练（AMP）**
   - **常见场景**：模型参数用FP16（2字节/参数），但**优化器状态仍需FP32**（避免数值不稳定）。
   - 此时显存分布：
     - **模型参数**：`2 × N` 字节（FP16）
     - **梯度**：`2 × N` 字节（FP16）
     - **优化器状态**：`12 × N` 字节（Adam FP32状态）
   - **显存放大系数**：仅优化器状态就是参数的 **6 倍**（FP16模型）❗

---

### 🧮 显存计算公式（以Adam为例）
```text
总优化器显存 (GB) = [参数量 (N) × 12 字节] / (1024^3)
```

---

### 🚀 优化技术（显著降低显存）
1. **ZeRO（Zero Redundancy Optimizer）**：
   - **Stage 1**：切分优化器状态到多卡 → **显存降至 ≈ `12 × N / GPU数`**。
   - **Stage 2** + **梯度分区**：额外切分梯度 → **显存降至 ≈ (12 + 2) × N / GPU数**（FP16模型）。
   - **Stage 3**：参数分区 → 显存进一步降低（但通信开销增大）。

2. **混合精度 + 卸载**：
   - 如 **DeepSpeed** 的 `CPU Offload` 将优化器状态移至CPU内存（牺牲速度换显存）。

3. **低显存优化器**：
   - 使用 **Adafactor** 或 **8-bit Adam**（如`bitsandbytes`库）可减少50%+优化器显存。

---

### 💡 实际示例（Llama 2 70B 训练配置）
| 配置                | 单卡优化器显存 | 技术说明                     |
|---------------------|----------------|-----------------------------|
| 原始 Adam           | ~840 GB        | 不可行                      |
| ZeRO-Stage2 + 8卡   | ~105 GB/GPU    | (12+2)×70B / 8 = 122.5GB/GPU |
| ZeRO-3 + CPU Offload| < 50 GB/GPU    | 状态/梯度卸载至CPU           |
| 8-bit Adam          | ~420 GB        | 显存减半，但需兼容性         |

---

### ✅ 关键结论
1. **优化器显存 ≈ 参数量 × 系数**：
   - **Adam系数 = 12**，**SGD+Momentum系数 = 8**，**Adafactor系数 ≈ 4~8**。
2. **千亿级模型必须依赖显存优化**：
   - 如无 ZeRO，仅Adam状态就需要 **数TB显存**（远超单卡容量）。
3. **实际占用公式**：
   ```text
   总显存 ≈ 模型参数 + 梯度 + 优化器状态 + 激活值 + 临时缓冲区
   ```
   **其中优化器状态常占 30%~50%**（尤其混合精度下）。

建议使用 **DeepSpeed** 或 **FSDP（PyTorch）** 的ZeRO策略，结合混合精度训练，是高效预训练LLM的工业标准方案。