# LLM训练优化器显存占用分析

### 🧑‍💻 User

为什么llm训练过程中，优化器的显存占用非常大？

---

### 🤖 Assistant

在大型语言模型 (LLM) 训练过程中，**优化器状态**是导致显存占用巨大的主要因素之一，尤其是在使用像 Adam 或 AdamW 这样高效但“重”的优化器时。主要原因可以归结为以下几点：

1.  **优化器状态本身的大小：**
    *   **Adam 的状态：** Adam 及其变种（AdamW）是目前训练 LLM 最常用的优化器。它需要为模型中的**每个可训练参数**维护两个额外的状态：
        *   **一阶矩估计：** 类似于动量，跟踪梯度的指数移动平均。
        *   **二阶矩估计：** 跟踪梯度平方的指数移动平均（用于自适应学习率）。
    *   **状态大小：** 这两个状态变量通常与参数本身**具有相同的维度**。这意味着，对于模型中的每个参数，Adam 需要存储：
        *   参数本身 (`param`)
        *   梯度 (`grad`) - 在反向传播后计算得到。
        *   一阶矩 (`m`)
        *   二阶矩 (`v`)
    *   **显存开销：** 因此，**仅优化器状态 (`m` 和 `v`) 的显存占用就至少是模型参数显存占用的 2 倍**。

2.  **梯度的显存占用：**
    *   在反向传播过程中，计算得到的梯度 (`grad`) 也需要存储在显存中，直到优化器执行 `step()` 更新参数。
    *   梯度的大小与参数本身完全相同。所以，梯度又占用了**1倍**于参数大小的显存。

3.  **混合精度训练：**
    *   为了加速训练和节省显存，LLM 训练广泛使用混合精度训练。
    *   在这种模式下，模型参数、激活值和梯度通常使用 **16 位浮点数**存储和计算。
    *   **然而，优化器状态通常需要保持 32 位精度 (`fp32`) 以保证数值稳定性**和更新的准确性。
    *   这意味着：
        *   `param` 在计算时是 `fp16`，但通常有一个 `fp32` 的副本用于更新（称为 `master params`）。
        *   `grad` 在计算时是 `fp16`，但在更新前会转换成 `fp32` 精度（或者直接累积为 `fp32`）。
        *   `m` 和 `v` 状态**总是 `fp32`**。
    *   **显存开销：** 因此，在混合精度训练下：
        *   `fp16 param`: 0.5 倍参数量显存（按字节算，假设原始参数是 `fp32`）。
        *   `fp32 master param`: 1 倍参数量显存。
        *   `fp16 grad`: 0.5 倍参数量显存（临时存在，更新后释放）。
        *   `fp32 m`: 1 倍参数量显存。
        *   `fp32 v`: 1 倍参数量显存。
    *   峰值显存占用（在反向传播后，优化器更新前）至少包含：
        *   `fp16 param` + `fp32 master param` + `fp16 grad` + `fp32 m` + `fp32 v` = 0.5 + 1 + 0.5 + 1 + 1 = **4 倍参数量显存** (按 `fp32` 参数大小计算)。这是非常巨大的开销！

4.  **模型规模巨大：**
    *   LLM 的核心特点就是参数规模庞大（数亿、数十亿甚至万亿级别）。即使单个参数的状态占用很小，乘以巨大的参数数量后，总量就变得极其惊人。
    *   **例子：** 假设一个模型有 1750 亿参数 (`175B`)。一个 `fp32` 值占用 4 字节。
        *   仅 `fp32` 参数显存： `175B * 4 bytes = 700 GB`。
        *   Adam 优化器状态 (`m` + `v`): `175B * 2 states * 4 bytes/state = 1.4 TB`。
        *   梯度 (`fp32`): `175B * 4 bytes = 700 GB`。
        *   峰值显存 (仅参数、梯度、优化器状态)： `700GB + 700GB + 1400GB = 2.8 TB`！这还不包括前向传播的激活值（也非常大）、各种临时缓冲区、框架开销等。
    *   即使使用混合精度：
        *   `fp16 param`: `175B * 2 bytes = 350 GB`
        *   `fp32 master param`: `175B * 4 bytes = 700 GB`
        *   `fp16 grad`: `175B * 2 bytes = 350 GB` (临时)
        *   `fp32 m`: `175B * 4 bytes = 700 GB`
        *   `fp32 v`: `175B * 4 bytes = 700 GB`
        *   峰值显存 (参数/梯度/状态): `350GB + 700GB + 350GB + 700GB + 700GB = 2.8 TB`。虽然 `fp16 param/grad` 体积减半，但 `master param` 和 `fp32 m/v` 的巨大开销依然存在。

5.  **分布式训练策略：**
    *   为了训练如此巨大的模型，必须使用数据并行、模型并行（如张量并行、流水线并行）或混合并行。
    *   在**数据并行**中，每个 GPU 上都有一份完整的模型副本。这意味着**每个 GPU 上都需要存储完整的优化器状态 (`m`, `v`)**！这是朴素的 `DataParallel` 或分布式数据并行 的主要显存瓶颈。
    *   **解决方案 - ZeRO：** 像 DeepSpeed 的 ZeRO 和 PyTorch 的 `FullyShardedDataParallel` 等优化技术就是为了解决这个问题而设计的。它们通过在不同 GPU 间**切分优化器状态、梯度和甚至模型参数**来显著降低单个 GPU 上的显存占用。例如：
        *   **ZeRO Stage 1:** 切分优化器状态 (`m`, `v`)。每个 GPU 只存储自己负责的那部分参数的完整状态。
        *   **ZeRO Stage 2:** 在 Stage 1 基础上切分梯度。每个 GPU 只存储自己负责的那部分参数的梯度。
        *   **ZeRO Stage 3:** 在 Stage 2 基础上切分模型参数。每个 GPU 只存储自己负责的那部分模型参数。
    *   **显存节省：** ZeRO Stage 3 理论上可以将每个 GPU 上存储的优化器状态、梯度和参数减少到原始大小的 `1/N`（`N` 是数据并行度）。这大大缓解了优化器状态带来的显存压力，使得训练超大模型成为可能。

**总结:**

LLM 训练中优化器显存占用巨大的根本原因在于：**Adam 等高效优化器需要为每个参数维护两个与参数本身同维度的 `fp32` 状态 (`m`, `v`)，加上梯度 (`grad`) 和可能的 `fp32 master param`，使得与优化直接相关的显存开销（参数、梯度、优化器状态）达到了模型 `fp32` 参数显存的 3-4 倍。当模型参数规模达到数十亿或万亿级别时，这个开销就变得天文数字般巨大。** 分布式训练策略（尤其是 ZeRO）通过切分这些状态来分摊显存负担，是解决这一问题的关键技术。

---

### 🧑‍💻 User

为什么lora微调不需要存储这么大的优化器和模型的梯度？

---

### 🤖 Assistant

LoRA（Low-Rank Adaptation）微调之所以能**显著减少优化器和梯度的显存占用**，核心在于它采用了 **“参数冻结+低秩增量更新”** 的设计策略。这种设计从根本上改变了需要存储和更新的参数量，从而大幅降低了显存需求。以下是详细原因：

---

### 关键原因分析
1.  **冻结原始大模型参数**：
    *   LoRA **不更新原始预训练模型的参数**。这些参数在微调过程中被固定（冻结），仅作为静态的基础模型使用。
    *   **显存节省：** 由于原始参数不更新，因此：
        *   **不需要存储它们的梯度 (`grad`)**
        *   **不需要为它们维护优化器状态 (`m`, `v`)**。这是省显存的最大来源！

2.  **仅引入并更新少量低秩适配器参数**：
    *   LoRA 只在模型的特定层（通常是注意力层的 `query`, `key`, `value`, `output` 投影矩阵）旁路添加**两个小的低秩矩阵** `A` 和 `B`。
    *   矩阵维度：
        *   `A`: 形状为 `[原始维度, r]` (通常 `r` 很小，如 4, 8, 16, 64)
        *   `B`: 形状为 `[r, 原始维度]`
        *   **关键：秩 `r` << 原始维度 (例如 768, 1024, 4096)**。例如 `r=8` 时，新增参数量仅为原始矩阵的 `(768*8 + 8*768) / (768*768) ≈ 2.08%`。
    *   **实际更新的参数：** 只有 `A` 和 `B` 这两个小矩阵是可训练的。原始大矩阵 `W` 保持不变。

3.  **优化的对象仅限于适配器参数**：
    *   前向传播：实际计算变为 `h = Wx + (BA)x`。其中 `Wx` 是冻结的原始计算，`(BA)x` 是低秩适配器引入的增量。
    *   反向传播：**梯度只流向低秩矩阵 `A` 和 `B`**，不会流向原始参数 `W`。
    *   优化器更新：优化器 (如 AdamW) **只存储和更新 `A` 和 `B` 的梯度、以及它们对应的优化器状态 (`m_A`, `v_A`, `m_B`, `v_B`)**。

---

### 显存占用对比 (以 AdamW 优化器 + 混合精度训练为例)

假设：
*   原始大模型参数量：`P`
*   LoRA 新增的可训练参数量：`P_lora` (通常 `P_lora << P`，可能只有 `P` 的 0.1%~1%)
*   混合精度训练 (fp16 模型参数/梯度，fp32 优化器状态/master weights)

| 组件                      | 全量微调 (Full Fine-tuning)      | LoRA 微调                          |
| :------------------------ | :------------------------------- | :--------------------------------- |
| **模型参数 (fp16)**       | `≈ 2P` 字节 (fp16 参数)         | `≈ 2P` 字节 (**冻结的** fp16 参数) |
| **Master Params (fp32)**  | `≈ 4P` 字节                     | **0** (冻结参数不需要 master copy) |
| **梯度 (fp16)**           | `≈ 2P` 字节 (临时)              | `≈ 2 * P_lora` 字节 (**仅适配器梯度**) |
| **优化器状态 (AdamW)**    | `≈ 8P` 字节 (`m`, `v` 各 4P)    | `≈ 8 * P_lora` 字节 (`m_A`, `v_A`, `m_B`, `v_B`) |
| **峰值显存 (估算)**       | `≈ 2P + 4P + 2P + 8P = 16P` 字节 | `≈ 2P + 0 + 2P_lora + 8P_lora = 2P + 10P_lora` 字节 |

**关键结论：**
1.  **`P_lora` 极小：** `P_lora` 通常只有原始参数总量 `P` 的 **千分之一到百分之一** (0.1% - 1%)。例如对于一个 7B 参数模型 (`P = 7e9`)，`r=8` 的 LoRA 适配器参数量 `P_lora` 可能只有 **几百万 (1e6 ~ 1e7)**，远小于 `P`。
2.  **显存主导项变化：**
    *   全量微调：显存占用由 `P` 主导，峰值显存约 `16P` 字节。
    *   LoRA 微调：显存占用由**冻结的原始模型参数 (`2P` 字节)** 主导。新增的适配器相关开销 (`10P_lora` 字节) 因为 `P_lora` 太小，几乎可以忽略不计。
3.  **节省幅度巨大：** LoRA 的峰值显存占用 **≈ 2P** (主要是加载原始模型的 fp16 权重)，相比于全量微调的 **≈ 16P**，**显存需求降低到约 1/8 甚至更低！** 这使得在消费级显卡 (如 24GB) 上微调超大模型 (如 7B, 13B) 成为可能。

---

### 总结：LoRA 省显存的本质

*   **不存大梯度：** 只计算和存储**极少量适配器参数 (`A`, `B`)** 的梯度，不存储海量原始参数的梯度。
*   **不存大优化器状态：** 只为**极少量适配器参数**维护优化器状态 (如 Adam 的 `m`, `v`)，不为原始参数维护。
*   **无需 master weights：** 冻结的原始参数不需要 fp32 master copy。
*   **低秩的本质：** 新增的可训练参数 `P_lora` 本身规模极小 (低秩分解)，使得其梯度和优化器状态的开销变得微不足道。

因此，LoRA 通过**冻结绝大部分参数 + 仅优化极小规模的增量参数**，巧妙地规避了全量微调中由优化器状态和梯度带来的巨额显存开销，实现了高效的参数高效微调 (Parameter-Efficient Fine-Tuning, PEFT)。