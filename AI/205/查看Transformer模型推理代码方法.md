# æŸ¥çœ‹Transformeræ¨¡å‹æ¨ç†ä»£ç æ–¹æ³•

### ğŸ§‘â€ğŸ’» User

transformeråº“å¦‚ä½•çœ‹åˆ°å„ä¸ªarchæ¨¡å‹ï¼ˆllamaï¼Œqwenç­‰ï¼‰çš„æ¨ç†ä»£ç ï¼Ÿ

---

### ğŸ¤– Assistant

å½“ç„¶å¯ä»¥ã€‚åœ¨ `transformers` åº“ä¸­æŸ¥çœ‹å„ä¸ªæ¨¡å‹ï¼ˆå¦‚ Llamaã€Qwenï¼‰çš„æ¨ç†ä»£ç æ˜¯ä¸€ä¸ªéå¸¸ç›´æ¥çš„è¿‡ç¨‹ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯æ‰¾åˆ°å¯¹åº”æ¨¡å‹çš„ "modeling" æ–‡ä»¶ï¼Œå…¶ä¸­å®šä¹‰äº†æ¨¡å‹çš„ç½‘ç»œç»“æ„å’Œå‰å‘ä¼ æ’­ï¼ˆ`forward`ï¼‰é€»è¾‘ã€‚

### æ ¸å¿ƒå®šä½ï¼š`modeling_<model_arch>.py` æ–‡ä»¶

å¯¹äº `transformers` åº“æ”¯æŒçš„æ¯ä¸€ç§æ¨¡å‹æ¶æ„ï¼ˆArchitectureï¼‰ï¼Œå…¶æ ¸å¿ƒå®ç°éƒ½åœ¨ä¸€ä¸ªåä¸º `modeling_<model_arch>.py` çš„æ–‡ä»¶ä¸­ã€‚ä¾‹å¦‚ï¼š

  * Llama ç³»åˆ—æ¨¡å‹çš„å®ç°åœ¨ `modeling_llama.py`ã€‚
  * Qwen2 ç³»åˆ—æ¨¡å‹çš„å®ç°åœ¨ `modeling_qwen2.py`ã€‚
  * BERT æ¨¡å‹çš„å®ç°åœ¨ `modeling_bert.py`ã€‚

åœ¨è¿™ä¸ªæ–‡ä»¶ä¸­ï¼Œ**æ¨ç†çš„æ ¸å¿ƒä»£ç **æ˜¯æ¨¡å‹ç±»ï¼ˆå¦‚ `LlamaForCausalLM`ï¼‰çš„ `forward` æ–¹æ³•ã€‚è¿™ä¸ªæ–¹æ³•å®šä¹‰äº†å½“è¾“å…¥æ•°æ®æµè¿‡æ¨¡å‹æ—¶ï¼Œå¦‚ä½•è®¡ç®—å‡ºä¸‹ä¸€ä¸ªè¯çš„ logitsï¼ˆå³æœªç» softmax çš„æ¦‚ç‡åˆ†å¸ƒï¼‰ã€‚

-----

### å¦‚ä½•æ‰¾åˆ°å¹¶æŸ¥çœ‹è¿™äº›ä»£ç ï¼Ÿ

ä¸»è¦æœ‰ä¸¤ç§æ–¹æ³•ï¼Œæ¨èä½¿ç”¨ç¬¬ä¸€ç§ï¼Œå› ä¸ºå®ƒæ›´ç›´è§‚ã€‚

#### æ–¹æ³•ä¸€ï¼šé€šè¿‡ Hugging Face Hub ç½‘ç«™ï¼ˆæœ€ç®€å•ï¼‰

è¿™æ˜¯æœ€æ–¹ä¾¿ã€æœ€ç›´æ¥çš„æ–¹æ³•ï¼Œä½ ç”šè‡³ä¸éœ€è¦åœ¨æœ¬åœ°ä¸‹è½½ä»»ä½•ä¸œè¥¿ã€‚

1.  **æ‰“å¼€æ¨¡å‹é¡µé¢**ï¼šåœ¨ Hugging Face ç½‘ç«™ä¸Šæœç´¢ä½ æ„Ÿå…´è¶£çš„æ¨¡å‹ï¼Œä¾‹å¦‚ [Qwen/Qwen2-7B-Instruct](https://huggingface.co/Qwen/Qwen2-7B-Instruct)ã€‚

2.  **è¿›å…¥ "Files and versions" æ ‡ç­¾é¡µ**ï¼šåœ¨è¿™ä¸ªé¡µé¢ï¼Œä½ å¯ä»¥çœ‹åˆ°æ„æˆè¯¥æ¨¡å‹ä»“åº“çš„æ‰€æœ‰æ–‡ä»¶ã€‚

3.  **æ‰¾åˆ°å¹¶ç‚¹å‡» `modeling_...py` æ–‡ä»¶**ï¼š

      * å¯¹äº Llama æ¨¡å‹ï¼Œä½ éœ€è¦æ‰¾åˆ° `modeling_llama.py`ã€‚
      * å¯¹äº Qwen2 æ¨¡å‹ï¼Œä½ éœ€è¦æ‰¾åˆ° `modeling_qwen2.py`ã€‚
      * **æ³¨æ„**ï¼šé€šå¸¸æ¨¡å‹ä»“åº“ä¸ä¼šç›´æ¥åŒ…å«è¿™ä¸ª `.py` æ–‡ä»¶ï¼Œå› ä¸ºå®ƒå±äº `transformers` åº“çš„ä¸€éƒ¨åˆ†ã€‚ä½†ä½ å¯ä»¥é€šè¿‡æ¨¡å‹é¡µé¢å³ä¾§çš„ "Use in Transformers" ä¸‹çš„é“¾æ¥ç›´æ¥è·³è½¬åˆ°å…¶æ–‡æ¡£ï¼Œæ–‡æ¡£ä¸­é€šå¸¸ä¼šé“¾æ¥åˆ°æºä»£ç ã€‚

    ä¸€ä¸ªæ›´ç›´æ¥çš„æ–¹å¼æ˜¯ç›´æ¥åœ¨ `transformers` çš„ GitHub æºç ä¸­æŸ¥çœ‹ã€‚

#### æ–¹æ³•äºŒï¼šé€šè¿‡ `transformers` çš„ GitHub ä»“åº“ï¼ˆæ¨èï¼‰

è¿™ç§æ–¹æ³•å¯ä»¥è®©ä½ çœ‹åˆ°æœ€åŸå§‹ã€æœ€å®Œæ•´çš„ä»£ç ï¼Œå¹¶ç†è§£å…¶ä¸Šä¸‹æ–‡ã€‚

1.  **è®¿é—® `transformers` GitHub ä»“åº“**ï¼š[https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)
2.  **å¯¼èˆªåˆ°æ¨¡å‹ç›®å½•**ï¼šæ‰€æœ‰æ¨¡å‹çš„æºä»£ç éƒ½å­˜æ”¾åœ¨ `src/transformers/models/` ç›®å½•ä¸‹ã€‚
3.  **æ‰¾åˆ°å…·ä½“æ¨¡å‹çš„æ–‡ä»¶å¤¹**ï¼šæ¯ä¸ªæ¨¡å‹æ¶æ„éƒ½æœ‰ä¸€ä¸ªä¸“å±çš„æ–‡ä»¶å¤¹ã€‚
      * **Llama**: `src/transformers/models/llama/`
      * **Qwen2**: `src/transformers/models/qwen2/`
4.  **æ‰“å¼€ `modeling_<model_arch>.py` æ–‡ä»¶**ï¼šåœ¨è¿™ä¸ªæ–‡ä»¶å¤¹é‡Œï¼Œä½ å°±èƒ½æ‰¾åˆ°æ ¸å¿ƒçš„å®ç°æ–‡ä»¶ã€‚

-----

### ç¤ºä¾‹ï¼šæŸ¥çœ‹ Llama å’Œ Qwen2 çš„æ¨ç†ä»£ç 

**1. Llama 3**

  * **æ–‡ä»¶è·¯å¾„**: [`transformers/src/transformers/models/llama/modeling_llama.py`](https://www.google.com/search?q=%5Bhttps://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py%5D\(https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py\))
  * **å…³é”®ç±»**: `LlamaForCausalLM`
  * **å…³é”®æ–¹æ³•**: åœ¨ `LlamaForCausalLM` ç±»ä¸­æ‰¾åˆ° `forward` æ–¹æ³•ã€‚è¿™ä¸ªæ–¹æ³•æ¥æ”¶ `input_ids` ç­‰è¾“å…¥ï¼Œç„¶åå°†å…¶ä¼ é€’ç»™åº•å±‚çš„ `LlamaModel`ï¼Œæœ€åé€šè¿‡ä¸€ä¸ªçº¿æ€§å±‚ï¼ˆ`lm_head`ï¼‰è®¡ç®—å‡º logitsã€‚

<!-- end list -->

```python
# åœ¨ modeling_llama.py ä¸­
class LlamaForCausalLM(LlamaPreTrainedModel):
    # ... åˆå§‹åŒ–ç­‰æ–¹æ³• ...

    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        # ... å…¶ä»–å‚æ•° ...
        labels: Optional[torch.LongTensor] = None,
        # ...
    ) -> Union[Tuple, CausalLMOutputWithPast]:
        
        # 1. è°ƒç”¨ LlamaModel è·å–æœ€åä¸€å±‚çš„ hidden states
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            # ...
        )
        hidden_states = outputs[0]

        # 2. é€šè¿‡ lm_headï¼ˆä¸€ä¸ªçº¿æ€§å±‚ï¼‰è®¡ç®— logits
        logits = self.lm_head(hidden_states)
        logits = logits.float()

        # ... è®¡ç®—æŸå¤±ç­‰ ...
        
        return CausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )
```

**2. Qwen2**

  * **æ–‡ä»¶è·¯å¾„**: [`transformers/src/transformers/models/qwen2/modeling_qwen2.py`](https://www.google.com/search?q=%5Bhttps://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2/modeling_qwen2.py%5D\(https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2/modeling_qwen2.py\))
  * **å…³é”®ç±»**: `Qwen2ForCausalLM`
  * **å…³é”®æ–¹æ³•**: åŒæ ·æ˜¯ `forward` æ–¹æ³•ï¼Œå…¶ç»“æ„å’Œ `LlamaForCausalLM` éå¸¸ç›¸ä¼¼ã€‚

-----

### æ¨ç†çš„ä¸¤ä¸ªå±‚é¢ï¼š`forward` vs `generate`

å½“ä½ æŸ¥çœ‹ä»£ç æ—¶ï¼Œéœ€è¦åŒºåˆ†ä¸¤ä¸ªé‡è¦çš„å‡½æ•°ï¼š

1.  **`forward`**: è¿™æ˜¯æ¨¡å‹æœ€åº•å±‚çš„å•æ¬¡æ¨ç†æ­¥éª¤ã€‚å®ƒæ¥æ”¶ä¸€æ‰¹ token IDsï¼Œè®¡ç®—å‡º**ä¸‹ä¸€ä¸ª** token çš„ logitsã€‚è¿™æ˜¯ç†è§£æ¨¡å‹å†…éƒ¨å·¥ä½œåŸç†ï¼ˆå¦‚ Attentionã€MLPï¼‰çš„å…³é”®ã€‚

2.  **`generate`**: è¿™æ˜¯ä¸€ä¸ªæ›´é«˜å±‚æ¬¡çš„å°è£…ï¼Œæ˜¯ç”¨æˆ·é€šå¸¸ç”¨æ¥ç”Ÿæˆå®Œæ•´æ–‡æœ¬çš„å…¥å£å‡½æ•°ï¼ˆä¾‹å¦‚ `model.generate(...)`ï¼‰ã€‚`generate` æ–¹æ³•å†…éƒ¨ä¼š**å¾ªç¯è°ƒç”¨ `forward`**ã€‚å®ƒåŒ…å«äº†ä¸€æ•´å¥—æ–‡æœ¬ç”Ÿæˆçš„ç­–ç•¥ï¼Œä¾‹å¦‚ï¼š

      * **Token é€‰æ‹©**ï¼šGreedy search, beam search, top-k/top-p sampling ç­‰ã€‚
      * **KV Caching**: ç¼“å­˜å·²ç»è®¡ç®—è¿‡çš„é”®ï¼ˆKeyï¼‰å’Œå€¼ï¼ˆValueï¼‰ï¼Œé¿å…é‡å¤è®¡ç®—ï¼Œä»è€Œæå¤§åœ°åŠ é€Ÿç”Ÿæˆè¿‡ç¨‹ã€‚
      * **æ§åˆ¶åœæ­¢æ¡ä»¶**ï¼šå½“ç”Ÿæˆåˆ° `eos_token` (end-of-sentence) æˆ–è¾¾åˆ° `max_length` æ—¶åœæ­¢ã€‚

`generate` æ–¹æ³•çš„æºä»£ç å®šä¹‰åœ¨ [`transformers/src/transformers/generation/utils.py`](https://www.google.com/search?q=%5Bhttps://github.com/huggingface/transformers/blob/main/src/transformers/generation/utils.py%5D\(https://github.com/huggingface/transformers/blob/main/src/transformers/generation/utils.py\)) ä¸­ï¼Œå®ƒè¢«æ··å…¥ï¼ˆmixinï¼‰åˆ°æ‰€æœ‰æ”¯æŒç”Ÿæˆçš„æ¨¡å‹ç±»ä¸­ã€‚

### `AutoModel` çš„å¹•åå·¥ä½œ

æœ€åï¼Œå½“ä½ ä½¿ç”¨ `AutoModelForCausalLM.from_pretrained(...)` æ—¶ï¼Œ`transformers` åº“ä¼šè‡ªåŠ¨è¯»å–æ¨¡å‹ä»“åº“ä¸­çš„ `config.json` æ–‡ä»¶ï¼Œæ‰¾åˆ° `"model_type"` å­—æ®µï¼ˆä¾‹å¦‚ `"llama"` æˆ– `"qwen2"`ï¼‰ï¼Œç„¶ååŠ¨æ€åœ°ä» `modeling_<model_type>.py` æ–‡ä»¶ä¸­åŠ è½½æ­£ç¡®çš„æ¨¡å‹ç±»ï¼ˆå¦‚ `LlamaForCausalLM`ï¼‰ã€‚è¿™å°±æ˜¯ `AutoModel` "è‡ªåŠ¨" å·¥ä½œçš„åŸç†ã€‚

### æ€»ç»“

  * è¦çœ‹æ¨¡å‹çš„æ ¸å¿ƒç½‘ç»œç»“æ„å’Œå•æ­¥æ¨ç†ï¼Œè¯·æŸ¥çœ‹ `src/transformers/models/<æ¶æ„å>/modeling_<æ¶æ„å>.py` æ–‡ä»¶ä¸­çš„ `forward` æ–¹æ³•ã€‚
  * è¦ç†è§£å®Œæ•´çš„æ–‡æœ¬ç”Ÿæˆæµç¨‹ï¼ˆå¦‚é‡‡æ ·ã€KV Cacheï¼‰ï¼Œè¯·æŸ¥çœ‹ `generation/utils.py` ä¸­çš„ `generate` æ–¹æ³•ã€‚
  * æœ€ç®€å•çš„æ–¹å¼æ˜¯é€šè¿‡ `transformers` çš„ GitHub ä»“åº“ç›´æ¥å¯¼èˆªåˆ°ç›¸åº”æ–‡ä»¶ã€‚