# LLM PTQ量化精度下降原因分析

### 🧑‍💻 User

为什么llm的ptq量化对于激活分布复杂或者极值较多且分布不均的时候精度下降更为明显？

---

### 🤖 Assistant

LLM 的激活值在 PTQ（Post-Training Quantization）量化中精度下降更明显，尤其是在激活分布复杂、存在大量离群值且分布不均的情况下，主要是由以下几个相互关联的原因造成的：

1.  **量化本质：信息压缩与离散化带来的固有损失**
    *   量化是将连续的浮点数值映射到有限的离散整数上的过程。这本身就会丢失信息（精度损失）。
    *   量化步长（`scale`）决定了每个整数区间所代表的浮点值范围。步长越大，表示的数值范围越广，但区间内的数值分辨率就越低（更粗糙）。

2.  **离群值/极值对量化范围的影响**
    *   **主导缩放因子：** 激活值中即使只有少量几个绝对值非常大的离群值，也会迫使量化器选择非常大的动态范围（`min` 到 `max`）来覆盖这些值。这直接导致量化步长变得非常大。
    *   **牺牲大部分数据的精度：** 当量化步长被极值拉大后，用于表示占数据主体（例如 99.9%）的“正常”值范围（通常在 0 附近）的可用量化区间数量就变得非常有限。想象一下，整个数轴被拉得很长，但可用的“刻度”（量化 bin）数量是固定的。结果就是，原本在密集区域有细微差别的数值，现在都被迫挤进了同一个或相邻的几个 bin 里，导致这些值的区分度丧失，量化误差显著增大。**这是精度下降最核心的原因之一。**

3.  **复杂/非均匀分布对量化效率的影响**
    *   **均匀量化的不匹配：** 标准的 PTQ（如对称量化）通常使用均匀量化，即每个量化 bin 覆盖的浮点值范围是相等的。这对于接近均匀分布的数据很有效。
    *   **信息密度不均：** 复杂分布（如双峰、长尾、重尾）意味着数值在不同区域的信息密度不同。大部分信息可能集中在某个狭窄区域（主峰），而其他区域（尾部）数值稀疏但可能包含重要信息（如离群值）。
    *   **资源分配不当：** 均匀量化在这种分布上效率低下。它将宝贵的量化 bin 均匀地分配在整个（被离群值拉宽的）动态范围上。导致在信息密集的主区域，可用的 bin 数量不足，分辨率太低；而在信息稀疏的尾部区域，又浪费了大量的 bin。非对称量化可以稍微缓解（通过偏移零点），但面对复杂的多峰或极端长尾分布，其改善有限。

4.  **激活的动态性与校准挑战**
    *   **输入依赖：** 与权重（静态）不同，激活值是高度动态的，依赖于具体的输入数据。不同的输入会导致激活分布发生变化。
    *   **校准数据代表性：** PTQ 需要一小部分校准数据来估计激活值的范围（`min`/`max`）或分布（用于更高级的方法）。如果校准数据未能充分捕获所有可能的输入模式（特别是那些会激发出强烈离群值的输入），那么基于此计算出的量化参数在实际推理遇到这些“未见模式”时就会失效，要么发生严重的饱和（值超出量化范围被裁剪），要么分辨率严重不足（动态范围过大）。
    *   **复杂分布/离群值的不稳定性：** 包含复杂分布或离群值的激活，其统计特性（如最大值、直方图）对输入数据更为敏感，使得找到一个能稳定覆盖所有情况的量化参数（尤其是范围）更加困难。

5.  **误差累积与放大**
    *   **层间传播：** LLM 由数十甚至数百层组成。每一层的量化误差（特别是激活量化误差）会作为输入传递给下一层。
    *   **非线性放大：** 神经网络中的非线性激活函数（如 GeLU, SiLU, ReLU）会放大输入误差。如果某一层的输入激活因为量化（尤其是在密集区域分辨率不足）已经存在较大误差，经过非线性函数后，这个误差可能会被不成比例地放大。
    *   **注意力机制敏感性：** Transformer 的核心是注意力机制。注意力分数（softmax 的输入）通常由 `Q` 和 `K` 的点积计算得来。这些点积值的范围对输入非常敏感，容易出现极端值（正或负的大数）。量化误差会直接影响注意力分数的计算，进而扭曲注意力权重，对模型最终输出产生巨大影响。Softmax 函数本身对输入的绝对尺度和相对差异极其敏感，量化引入的微小误差在 softmax 后可能被显著放大。

6.  **模型对低精度表示的敏感性**
    *   LLM 经过训练在浮点精度下工作，其内部表示和计算流程对数值精度有一定的敏感性。尤其是在进行 softmax、LayerNorm 等操作时，输入的细微差异可能导致输出结果的较大变化。量化，尤其是在信息密集区域分辨率不足时，破坏了这些细微但关键的数值差异。

**总结来说：**

*   **离群值/极值** 是“罪魁祸首”：它们迫使量化器使用过大的动态范围，导致用于表示主体数据的量化分辨率严重不足。
*   **复杂/非均匀分布** 是“帮凶”：它使得均匀量化这种简单高效的方法在资源（量化 bin）分配上极不合理，浪费资源在稀疏区，饿死密集区。
*   **激活的动态性** 增加了校准的难度，使得量化参数难以稳定覆盖所有输入情况。
*   **深层结构** 和 **敏感操作**（注意力、softmax）将每一层的量化误差累积并放大，最终导致显著的输出精度下降。

因此，当遇到激活分布复杂、离群值多且分布不均的情况时，标准的均匀 PTQ 量化方案会同时面临“范围太大导致主体分辨率低”和“分布不均导致资源分配不合理”的双重困境，再加上误差在深层网络中被放大，精度损失就会表现得尤为明显。这也是为什么针对 LLM 激活量化，研究者们提出了许多更高级的技术，如混合精度量化（对不同层或不同张量使用不同位宽）、基于分组的量化（对通道分组使用不同的量化参数）、离群值抑制或隔离技术等，都是为了更好地应对这些挑战。

---

### 🧑‍💻 User

非对称量化是如何通过偏移零点尽可能提高量化质量的？

---

### 🤖 Assistant

非对称量化通过引入一个**零点偏移（Zero Point）** 来显著提升量化质量，尤其对**非均匀分布**或**包含负值**的数据（如激活值）效果明显。其核心在于更高效地利用有限的整数动态范围，减少信息损失。以下是其工作原理和优势详解：

---

### 一、对称量化的局限性（对比基准）
在对称量化中：
1.  **量化范围对称**：例如 `[-max_abs, max_abs]`，其中 `max_abs = max(|min|, |max|)`。
2.  **零点固定为0**：浮点0精确映射到整数0。
3.  **问题**：
    *   **浪费范围**：若数据分布不对称（如大部分值 >0，少量值 <0），负数区间可能只覆盖极少数据点，导致大量整数 bin 被浪费。
    *   **分辨率不足**：若数据分布偏向一侧（如全为正），则量化范围被“无用”的负半区间占据，导致实际有效区域的分辨率降低。

---

### 二、非对称量化的核心：零点偏移（Zero Point）
非对称量化的关键公式：
```math
\begin{align*}
scale &= \frac{float\_max - float\_min}{quant\_max - quant\_min} \\
zero\_point &= quant\_min - \frac{float\_min}{scale} \quad \text{（或反之）}
\end{align*}
```
*   **`float_min`, `float_max`**：原始浮点数据的最小/最大值（或基于校准的截断值）。
*   **`quant_min`, `quant_max`**：量化后的整数范围（如 `[0, 255]` for int8）。
*   **`zero_point`**：一个**整数**值，表示浮点 `0.0` 在量化后对应的整数位置。

---

### 三、零点偏移如何提升量化质量？
#### 1. **动态匹配数据实际范围**
*   **覆盖真实最小/最大值**：直接使用 `float_min` 和 `float_max` 确定范围，而非对称的 `max_abs`。
*   **消除浪费区间**：量化范围 `[quant_min, quant_max]` 完全对应 `[float_min, float_max]`，不再为不存在的数据保留空间。
*   **效果**：在相同位宽下，**有效动态范围利用率最大化**，可用于表示真实数据的整数 bin 数量增加。

#### 2. **优化非均匀/偏斜分布的分辨率**
*   **案例1：全正激活（如 ReLU 后）**：
    *   对称量化：范围 `[-max, max]` → 一半 bin (`[-max, 0]`) 被浪费。
    *   非对称量化：范围 `[0, max]` → 所有 bin (`[0, 255]`) 都用于表示正值，分辨率翻倍。
*   **案例2：含少量负值的长尾分布**：
    *   对称量化：范围 `[-large_tail, large_tail]` → 主分布区域（如 `[-small, large]`）可用的 bin 很少。
    *   非对称量化：范围 `[float_min, float_max]` ≈ `[-small, large_tail]` → 将更多 bin 分配给密集的主分布区域（尤其是正值部分），少量 bin 覆盖负值和长尾。

#### 3. **精确表示零点（Zero Preservation）**
*   **重要性**：深度学习中的零值具有特殊意义（如 ReLU 的边界、Padding、稀疏性）。
*   **对称量化**：浮点 `0.0` → 整数 `0`，天然精确。
*   **非对称量化**：通过 `zero_point` 精确保证 `float(0.0) = dequantize(quantize(0.0))`。
*   **效果**：避免在零点附近引入系统性偏差，对依赖零值的算子（如 ReLU、Add）的数值稳定性至关重要。

#### 4. **更好处理长尾和离群值**
*   虽然非对称量化不能消除离群值的影响，但相比对称量化：
    *   若离群值仅在单侧，可减少对另一侧密集区域的挤压。
    *   结合**截断（Clipping）策略**时，可独立优化 `float_min`/`float_max`，更灵活地平衡离群值和主分布的分辨率损失。

---

### 四、非对称量化 vs. 对称量化（实例对比）
| **数据分布**          | **对称量化 (int8)**       | **非对称量化 (int8)**       | **优势**                     |
|-----------------------|--------------------------|----------------------------|----------------------------|
| **ReLU 激活 (全 ≥0)** | 范围：`[-127, 127]`      | 范围：`[0, 255]`           | **分辨率翻倍** (255 vs 127) |
| **偏斜分布 (e.g., -1~100)** | 范围：`[-100, 100]`      | 范围：`[-1, 100]`          | **负区浪费减少**，主区 bin 增加 |
| **含精确零需求**      | 0 天然精确               | 通过 `zero_point` 保证精确 | **同等精确**               |

---

### 五、非对称量化的代价
1.  **计算开销增加**：
    *   推理时需额外处理 `zero_point` 的整数偏移：
        ```math
        // 矩阵乘示例 (A, B 为量化张量)
        int32_t result = (A_int - zp_A) * (B_int - zp_B); // 额外减法！
        ```
    *   相比对称量化（直接 `A_int * B_int`），增加两次减法/加法操作。
2.  **硬件支持**：部分低功耗硬件对非对称量化优化不足，可能降低峰值算力。

---

### 总结：零点偏移如何提高质量？
非对称量化通过 **`zero_point` 动态对齐浮点范围与整数范围**，实现：
1.  **消除无效区间浪费** → **提升有效分辨率**（尤其在数据偏斜时）。
2.  **精确保留零点** → 保证关键操作的数值稳定性。
3.  **更贴合真实分布** → 在相同位宽下，**显著降低主数据区域的量化误差**。

尽管引入额外计算成本，但其在**激活量化**（通常非对称、含零）中带来的精度收益远超开销，使其成为 PTQ 中处理复杂激活分布的首选方案。而权重量化因分布更对称且静态，常使用对称量化以追求极致推理速度。

---

### 🧑‍💻 User

目前有哪些主流的量化技术使用了非对称量化（例如gguf k-quants等）？

---

### 🤖 Assistant

目前主流的大模型量化技术中，**非对称量化**因其在处理非均匀分布数据（尤其是激活值）时的优势，被广泛应用于多种方案。以下是采用或支持非对称量化的代表性技术：

---

### **一、通用量化框架与运行时**
1. **TensorRT / TensorRT-LLM**  
   - **支持模式**：显式量化（Explicit Quantization）  
   - **细节**：  
     - 激活值（Activations）默认使用 **非对称量化**（`int8`），通过校准过程动态确定 `scale` 和 `zero_point`。  
     - 权重（Weights）通常使用对称量化以减少计算开销。  
   - **优势**：适配 ReLU 输出（全正分布）、长尾分布等常见激活模式。

2. **ONNX Runtime**  
   - **支持模式**：静态量化（Static Quantization）  
   - **细节**：  
     - 激活值支持非对称量化，通过校准数据计算 `zero_point`。  
     - 提供 `QuantizeLinear` 和 `DequantizeLinear` 算子直接支持非对称量化张量。

3. **PyTorch (FBGEMM / QNNPACK 后端)**  
   - **支持模式**：`torch.ao.quantization`  
   - **细节**：  
     - 激活值默认使用 **非对称 `uint8` 量化**（范围 `[0, 255]`），充分利用全正分布特性。  
     - 可通过 `qconfig` 配置为对称量化（如用于权重）。

---

### **二、大模型专用量化方案**
#### **1. GGUF (GGML 格式演进)**
- **核心机制**：  
  GGUF 是 [Llama.cpp](https://github.com/ggerganov/llama.cpp) 使用的二进制格式，支持多种量化类型，其中：  
  - **`Q4_K`/`Q5_K`/`Q6_K` (K-quants)**：  
    - **分组量化（Group-wise Quantization）**：将权重张量切分为小分组（如 64-128 元素/组）。  
    - **组内非对称量化**：每组独立计算 `scale` 和 `zero_point`，适应组内分布特性。  
    - **高灵活性**：对离群值多的组分配更大范围，减少精度损失。  
  - **其他类型**：如 `Q8_0`（每块对称量化）也支持非对称变体。

#### **2. GPTQ (训练后权重量化)**
- **原生设计**：权重量化通常为**对称量化**（因分布更均匀）。  
- **扩展支持**：  
  - 部分实现（如 [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)）允许激活值使用**非对称量化**增强推理精度。  
  - 可与 SmoothQuant 等技术结合，将离群值迁移至权重后对激活做非对称量化。

#### **3. SmoothQuant**  
- **核心思想**：通过数学变换将激活的离群值“迁移”到权重中。  
- **量化策略**：  
  - 迁移后的**激活值分布更平缓**，适合使用**非对称量化**（如 `int8`）进一步压缩。  
  - 权重仍使用对称量化保持高效计算。  

#### **4. AWQ (Activation-aware Weight Quantization)**  
- **核心思想**：基于激活分布重要性，保护权重中“关键通道”。  
- **量化支持**：  
  - 可配置为对**激活值使用非对称量化**（如 `int8`），尤其适用于非均匀分布场景。  
  - 开源实现（如 [AWQ](https://github.com/mit-han-lab/awq)）支持非对称模式。

---

### **三、硬件厂商方案**
1. **NVIDIA TensorRT-LLM**  
   - 如前所述，激活值默认非对称量化（`int8`），适配 Hopper GPU 的 FP8 非对称支持。

2. **Qualcomm AI Engine Direct**  
   - 支持激活值非对称量化（`int8`/`uint8`），通过 `zero_point` 优化移动端能耗。

3. **Intel Neural Compressor**  
   - 提供 `PostTrainingQuantConfig` 配置非对称量化策略，适配 CPU/GPU。

---

### **四、学术前沿方法**
1. **ZeroQuant-V2**  
   - **分层优化**：对某些层的激活值采用非对称量化（如 `int4`），通过 `zero_point` 压缩动态范围。  
2. **LLM-QAT** (量化感知训练)  
   - 在训练中学习 `scale` 和 `zero_point`，使非对称量化适应任务分布。  

---

### **关键技术总结表**
| **技术**       | **非对称量化应用场景**       | **优势**                                  |
|----------------|------------------------------|------------------------------------------|
| **GGUF K-quants** | 权重分组量化（组内非对称）   | 灵活处理组内离群值，4-bit 高压缩仍保精度 |
| **TensorRT**   | 激活值默认非对称（`int8`）   | 适配动态输入，减少 ReLU 激活范围浪费     |
| **SmoothQuant**| 迁移后激活值非对称量化       | 解决激活离群值问题，支持 8W8A 部署      |
| **AWQ**        | 可选激活非对称量化           | 保护关键通道，提升低比特量化鲁棒性       |
| **PyTorch**    | 静态量化中激活值用 `uint8`   | 简单高效处理全正分布（如 CNN 激活）      |

---

### **为什么这些技术选择非对称量化？**
1. **激活值特性**：  
   - 动态范围大、分布偏斜（如 ReLU 后全正）、存在离群值 → **非对称更好匹配真实分布**。  
2. **零点保留**：  
   - 对 Padding、门控机制（如 SwiGLU）中的零值敏感 → **`zero_point` 确保精确表示零**。  
3. **硬件兼容性**：  
   - 现代加速器（如 NPU、GPU）已优化非对称计算指令（如 `I8mm`），开销可控。  

> **注**：权重量化因分布对称且静态，仍以**对称量化**为主（如 GPTQ），但混合策略（如 K-quants 的分组非对称）正成为主流。

---

### **演进趋势**
1. **混合精度**：对敏感层激活保留高精度（如 `fp16`），其他层用非对称 `int8`/`int4`。  
2. **动态量化**：根据输入实时调整 `zero_point`（如 Dynamic TensorRT）。  
3. **非对称低位宽**：如 `int4` + 非对称分组（GGUF Q4_K）挑战极限压缩。  

如需实践建议（如 GGUF 中如何选择量化类型），可进一步探讨！