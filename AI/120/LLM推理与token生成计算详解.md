# LLM推理与token生成计算详解

### 🧑‍💻 User

为什么llm推理过程中会涉及到前向传播，decode每生成一个token都进行了哪些计算？

---

### 🤖 Assistant

LLM（大语言模型）推理过程中的**前向传播**是其生成文本的核心计算机制。理解为什么需要它以及每个`decode`步骤的具体计算，关键在于理解其**自回归**的生成方式。

### 为什么需要前向传播？
1.  **模型本质：** LLM 本质上是一个极其复杂的函数。给定一个输入序列（包含历史生成的 tokens 和提示词），它的核心任务是根据这个输入序列，计算输出词汇表上所有可能的下一个 token 的概率分布（`logits`）。执行这个计算的过程就是**前向传播**。
2.  **自回归生成：** LLM 一次只生成一个 token。要生成第 `t` 个 token：
    *   输入是序列 `[token_0, token_1, ..., token_{t-1}]`（通常是这些 token 的嵌入向量，加上位置编码）。
    *   模型对这个输入序列执行一次完整的前向传播。
    *   模型输出是序列中**最后一个位置**（对应 `token_{t-1}` 之后的位置）的 `logits` 向量。
    *   从这个 `logits` 向量采样或选择，得到第 `t` 个 token (`token_t`)。
3.  **循环依赖：** 要生成下一个 token (`token_{t+1}`)，必须把刚刚生成的 `token_t` 加入到输入序列中，形成新的输入 `[token_0, token_1, ..., token_t]`，然后再次执行前向传播，计算新的最后一个位置的 `logits`。这个过程循环往复。
4.  **计算图的必然性：** 前向传播是神经网络根据输入计算输出的标准流程。LLM 的架构（Transformer）决定了其输出依赖于整个输入序列的上下文信息，这必须通过完整的前向传播（遍历所有层：嵌入、注意力、FFN、层归一化等）来捕获。

**总结：每次预测下一个 token 都需要模型根据当前的完整输入序列（历史 tokens）计算其“理解”和“预测”，这个计算过程就是前向传播。没有前向传播，模型就无法进行预测。**

---

### `decode` 每生成一个 token 进行了哪些计算？
当我们说“decode一个token”时，指的是模型执行一次完整的前向传播，生成该token对应的`logits`，并从中选择/采样出该token的过程。具体步骤和计算如下：

1.  **输入准备：**
    *   **拼接新 token：** 将上一次生成的 token (`token_{t-1}`) 加入到输入序列的末尾。序列变为 `[token_0, token_1, ..., token_{t-1}]`。
    *   **嵌入查找：** 将序列中的每个 token ID 转换为其对应的**嵌入向量**。`input_vectors = EmbeddingLayer(token_ids)`。
    *   **添加位置编码：** 为每个嵌入向量添加**位置编码**信息，使模型知道 token 的顺序。`positional_vectors = PositionalEncoding(input_vectors)`。
    *   **更新 K/V 缓存：** 对于 Transformer 模型（特别是 Decoder-only 架构如 GPT），为了高效计算，会缓存先前所有 token 在注意力层计算出的 **Key (`K`)** 和 **Value (`V`)** 矩阵。这一步将读取缓存中 `[0]` 到 `[t-2]` 的 K/V，并为新加入的 `token_{t-1}` 计算其对应的 K 和 V（但*不*执行完整的注意力计算）。这是推理优化的关键。

2.  **前向传播计算：**
    *   准备好的输入向量（包含位置信息）被送入模型的多个 Transformer 层（Block）。每个层内部执行以下核心计算：
        *   **自注意力：**
            *   **计算 Query/Key/Value：** `Q = x * W_q`, `K = x * W_k`, `V = x * W_v`（`x` 是当前层的输入向量）。`K` 和 `V` 通常直接使用或更新缓存中的值。
            *   **注意力分数：** `Attention Scores = (Q * K^T) / sqrt(d_k)`。这里 `Q` 通常是新 token (`token_{t-1}`) 的向量（或当前层处理后的表示），`K` 是缓存中所有历史 tokens (`[0]` 到 `[t-1]`) 的向量。
            *   **注意力权重：** `Attention Weights = softmax(Attention Scores)`。
            *   **注意力输出：** `Attn_Output = Attention Weights * V`（`V` 也是缓存中 `[0]` 到 `[t-1]` 的向量）。
        *   **残差连接与层归一化：** `x = LayerNorm(Attn_Output + x)`。
        *   **前馈神经网络：** `FFN_Output = Activation(x * W_1 + b_1) * W_2 + b_2`。
        *   **残差连接与层归一化：** `x = LayerNorm(FFN_Output + x)`。
    *   这个过程在每一层重复。每一层的输出是下一层的输入。

3.  **输出层计算：**
    *   经过所有 Transformer 层后，得到序列中每个位置的最终表示向量。
    *   **关键：** 我们只关心**最后一个位置**（即 `token_{t-1}` 对应的位置）的输出向量，因为它代表了模型对**下一个 token** (`token_t`) 的预测。
    *   **线性投影：** 将最后一个位置的向量通过一个线性层（`LM Head`），投影到词汇表大小的维度。`logits = FinalLayer(x_last)`。`logits` 是一个长度等于词汇表大小的向量，每个元素代表对应 token 的未归一化得分。

4.  **Token 采样/选择：**
    *   **Logits 转换：** 通常对 `logits` 应用 `softmax` 函数，将其转换为概率分布：`probabilities = softmax(logits)`。有时会直接在 `logits` 上进行采样或选择（如 `top-k` 采样）。
    *   **采样策略：** 根据设定的解码策略，从这个概率分布中选择下一个 token (`token_t`)：
        *   **贪婪搜索：** 直接选择概率最高的 token (`argmax(probabilities)`）。
        *   **Top-k / Top-p：** 从概率最高的 `k` 个 token 中，或从累积概率超过 `p` 的最小 token 集合中采样。
        *   **温度采样：** 调整 `softmax` 的温度参数控制分布的平滑度后再采样。
        *   **Beam Search：** 维护多个候选序列（Beam），此步骤涉及为每个候选序列选择扩展 token 并更新分数（涉及多次前向传播或并行计算）。

5.  **更新与循环：**
    *   将新生成的 `token_t` 添加到序列末尾。
    *   更新 K/V 缓存：将 `token_t` 在每一层计算出的 K 和 V 添加到对应层的缓存中，为下一次前向传播（预测 `token_{t+1}`）做准备。
    *   如果 `token_t` 不是结束符（如 `<eos>`）且未达到最大长度限制，则跳回步骤 1，开始生成 `token_{t+1}`。

### 关键点总结

*   **核心计算是前向传播：** 每个生成的 token 都对应一次完整的模型前向传播计算。
*   **关注最后一个位置：** 每次前向传播的目标是获取输入序列**最后一个位置**的输出 `logits`，它预测的是**下一个 token**。
*   **K/V 缓存是优化关键：** 缓存机制避免了为每个新 token 重新计算整个历史序列的 K/V，大大提高了推理效率。计算量主要集中在为新 token 计算 Q 以及注意力权重与 V 的加权和上。
*   **计算量增长：** 虽然 K/V 缓存优化了注意力计算，但随着序列长度增长，注意力权重计算 (`Q * K^T`) 和加权和 (`Weights * V`) 的计算量（主要是访存和计算）仍会线性增加（相对于序列长度）。其他层（FFN）的计算量则相对固定（只处理新 token 的表示）。
*   **采样策略影响：** 采样/选择步骤本身计算量很小，但策略（如 Beam Search）可能会增加所需的前向传播次数。

理解这个“前向传播 -> 取最后一个位置 logits -> 采样 -> 更新输入和缓存 -> 重复”的循环，是理解 LLM 如何逐词生成文本的基础。