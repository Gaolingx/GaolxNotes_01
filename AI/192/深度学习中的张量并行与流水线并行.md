# 深度学习中的张量并行与流水线并行

### 🧑‍💻 User

如何理解深度学习中的张量并行和流水线（pipeline）并行？

---

### 🤖 Assistant

理解深度学习中的张量并行和流水线并行，需要从它们解决的核心问题出发：**如何将超大模型（参数规模巨大或层数极深）高效地分布到多个计算设备（如GPU）上进行训练？**

它们都属于**模型并行**的范畴，区别于数据并行（将批量数据拆分到不同设备上计算相同的模型）。这两种并行技术通常是**互补**的，可以结合使用。

---

### **1. 张量并行**
*   **核心思想：** 将一个**单一层的计算**（通常是计算量巨大的操作，如大型矩阵乘法）**水平拆分**到多个设备上执行。
*   **如何工作：**
    *   **拆分维度：** 对于像 `Y = X * W` 这样的矩阵乘法，可以将巨大的权重矩阵 `W` 沿着它的行或列（或两者）进行切分。
    *   **计算分布：** 每个设备持有 `W` 的一部分（例如 `W_i`）和输入 `X` 的对应部分（或全部 `X`，取决于拆分方式）。
    *   **局部计算：** 每个设备独立计算 `Y_i = X * W_i`（或 `Y_i = X_i * W_i`）。
    *   **通信聚合：** 需要将各个设备计算出的部分结果 `Y_i` 通过通信（如All-Reduce, All-Gather）聚合起来，得到完整的输出 `Y`。这通常发生在层内计算的中间或结束阶段。
*   **类比：** 想象一群工人共同计算一个巨大无比的乘法表。每个工人负责计算表格的一部分格子，最后大家把各自的结果拼在一起。
*   **典型应用场景：**
    *   Transformer 模型中巨大的 **Feed-Forward Network** 层（参数主要在中间的两个大矩阵上）。
    *   Transformer 模型中巨大的 **注意力头** 或 **注意力矩阵计算**（虽然通常用序列并行更高效）。
    *   任何包含**超大矩阵乘法**的层。
*   **优点：**
    *   能有效解决**单层参数过大**无法放入单个设备显存的问题。
    *   对于计算密集型的超大层，并行效率可以很高。
*   **缺点：**
    *   **通信开销大：** 层内计算频繁需要聚合部分结果，设备间通信量大且频繁，对设备间互联带宽要求非常高（如NVLink/InfiniBand）。通信可能成为瓶颈。
    *   **设备利用率：** 如果拆分不均匀或通信等待时间长，部分设备可能空闲。
*   **代表实现：** `Megatron-LM` 是张量并行（特别是针对Transformer FFN和注意力层）的经典实现。

---

### **2. 流水线并行**
*   **核心思想：** 将**整个模型的网络层按顺序垂直拆分**成多个“阶段”，每个阶段包含一组连续的层。每个阶段被放置在不同的设备上。数据（微批次）像流水线一样依次流经各个阶段。
*   **如何工作：**
    *   **模型切分：** 假设模型有 L 层，将其切分成 K 个阶段（Stage），每个阶段包含 L/K 层（通常不均匀）。每个阶段放置在一个设备上。
    *   **流水线调度：**
        *   **前向传播：** 设备1完成Stage1的前向计算后，将中间激活值传递给设备2；设备2开始Stage2的计算，同时设备1可以开始处理下一个微批次（Microbatch）的数据。如此流水推进。
        *   **反向传播：** 梯度计算按相反顺序进行流水。设备K计算完StageK的梯度后传递给设备K-1；设备K-1计算StageK-1的梯度（结合收到的上游梯度），同时设备K可以开始计算下一个微批次的反向传播。如此流水推进。
    *   **微批次：** 为了填充流水线，提高设备利用率，将一个大的数据批次拆分成多个更小的微批次。微批次是流水线调度的基本单位。
*   **类比：** 汽车装配流水线。每个工作站（设备）负责特定的装配任务（一组层）。底盘（数据）依次经过车身、喷漆、内饰、总装等工作站。不同底盘在不同工作站上同时被处理。
*   **典型应用场景：**
    *   层数极深的模型（如百层、千层Transformer）。
    *   模型整体参数非常大，但单个层的大小尚能放入单个设备显存。
*   **优点：**
    *   有效解决了**模型深度过大**（层数太多）导致无法放入单个设备显存的问题（因为每个设备只需保存自己阶段的参数和中间激活）。
    *   通信主要发生在相邻阶段之间，通信量相对张量并行较小（传递激活值和梯度），对设备间互联带宽要求相对低一些。
*   **缺点：**
    *   **流水线气泡：** 流水线启动（填充）和排空时，部分设备处于空闲状态，造成计算资源浪费（称为“气泡”）。微批次数量越多，气泡比例相对越小，但总批大小固定时微批次太小可能影响收敛。
    *   **设备负载均衡：** 需要精心划分阶段，使每个阶段的计算量尽可能均衡，否则最慢的阶段会成为瓶颈。
    *   **内存开销：** 为了支持反向传播，设备需要缓存多个微批次的中间激活值（直到其反向传播完成），可能消耗较多显存。
*   **代表实现：** `GPipe`, `PipeDream`, `DeepSpeed Pipeline Parallelism`。

---

### **关键对比总结**

| 特性         | 张量并行                     | 流水线并行                     |
| :----------- | :--------------------------------------- | :--------------------------------------- |
| **拆分维度** | **水平拆分单层**（层内并行）             | **垂直拆分模型**（层间并行）             |
| **通信模式** | **层内聚合** (All-Reduce/All-Gather)，**通信密集** | **相邻阶段传递** (激活/梯度)，**通信量相对较小** |
| **解决核心问题** | 单层参数/计算量过大（宽度）              | 模型总层数过多（深度）或总参数量过大     |
| **通信瓶颈** | 设备间互联带宽                           | 流水线气泡（启动/排空开销）              |
| **负载均衡** | 通常较易（拆分矩阵运算）                 | 需要精心划分阶段（计算量均衡）           |
| **内存开销** | 主要存储部分参数和部分激活               | 存储阶段参数和**多个微批次的激活**       |
| **典型应用** | Transformer FFN层, 大注意力矩阵          | 极深模型（如千层Transformer）            |

---

### **为什么需要结合使用？**

现实中的超大模型（如GPT-3, T5, Megatron-Turing NLG）通常**既宽又深**：
1.  **宽度问题：** 单个层（如FFN）的参数规模可能已经超过单个GPU的显存容量，需要张量并行来拆分单层。
2.  **深度问题：** 模型总层数（深度）巨大，导致总参数量和中间激活远超单GPU显存，需要流水线并行将不同的层组分配到不同的设备。

因此，**3D并行**成为训练超大模型的标配：
1.  **数据并行：** 复制相同的模型副本到多个设备组，处理不同的数据子集。**解决数据量大的问题。**
2.  **流水线并行：** 在数据并行的每个副本内部，将模型垂直切分成多个阶段分布在设备组内。**解决模型深度/总参数量问题。**
3.  **张量并行：** 在流水线并行的每个阶段内部，如果该阶段包含超大层，则进一步使用张量并行将该层的计算水平拆分到该阶段内的多个设备上。**解决单层宽度/计算量问题。**

通过这种分层组合，可以充分利用大规模计算集群的资源来训练以前无法想象的庞大模型。理解张量并行和流水线并行的原理和优缺点，是设计和优化大规模分布式深度学习训练的关键。

---

### 🧑‍💻 User

llm推理中，为什么流水线并行会出现气泡？

---

### 🤖 Assistant

在LLM推理中，流水线并行（Pipeline Parallelism）出现**气泡（Bubble）** 的根本原因是**流水线各阶段之间的依赖关系**和**计算资源的空闲等待**。以下是具体原因和机制分析：

---

### **1. 核心原因：流水线的启动与排空**
- **启动阶段（Filling the Pipeline）**  
  当第一个输入序列（或微批次）进入流水线时：
  - **Stage 1**（第一个设备）开始计算，而其他阶段处于空闲状态。
  - 直到第一个序列完成 **Stage 1** 的计算并传递给 **Stage 2** 后，**Stage 2** 才能开始工作，此时 **Stage 1** 可以处理第二个序列。
  - **前几个序列的处理过程中，后续阶段会因等待输入数据而空闲**，形成气泡（图中空白区域）。
  
- **排空阶段（Draining the Pipeline）**  
  当最后一个序列进入流水线后：
  - **Stage 1** 完成计算后可能提前空闲。
  - 最后一个序列需要依次流经所有阶段，**前面的阶段会提前结束工作，而后续阶段仍在忙碌**，再次产生气泡。

> ✅ **气泡本质：** 流水线中部分设备因等待数据输入/输出而处于空闲状态的时间段。

---

### **2. 图解气泡形成**
假设一个 **4阶段** 流水线处理 **6个序列**（微批次），时间线如下：
```
设备1: [A1][A2][A3][A4][A5][A6]       # 持续工作
设备2:  __ [A1][A2][A3][A4][A5][A6]   # 启动时等待A1（气泡）
设备3:  ______ [A1][A2][A3][A4][A5][A6] # 等待更久
设备4:  ________ [A1][A2][A3][A4][A5][A6] # 等待最久
```
- **`__`** 表示设备空闲（气泡），每个阶段启动时都有等待时间。
- 排空阶段同理（最后序列离开后，设备逐步空闲）。

---

### **3. LLM推理特有的加剧因素**
#### **(1) 自回归生成（Token-by-Token Generation）**
LLM推理是**逐Token生成**的循环过程：
- 每个Token的输出依赖前一个Token（`输出[t] = f(输入 + 输出[0..t-1])`）。
- **当前Token** 必须完成**整个流水线**的计算后，才能生成**下一个Token**。  
  **结果：**  
  - 每个Token的处理都需要等待流水线完整执行一次。
  - **气泡在每次Token生成循环中重复出现**，而非仅在启动/排空时发生。

#### **(2) 动态序列长度**
- 输入序列长度可能不同（如问答 vs 长文生成）。
- 输出长度未知（需生成至`<EOS>`标志）。
  **问题：**  
  流水线各阶段处理时间不均衡，**最慢的Token生成步骤**会阻塞后续所有阶段，放大气泡。

#### **(3) 微批次（Microbatch）调度限制**
- 训练中可用大量微批次“填满”流水线（减少气泡比例）。
- **推理场景**中：
  - 请求常以**单批次或小批量**到达（如API服务）。
  - 无法持续提供足够微批次，导致流水线频繁启停，**气泡占比显著增加**。

---

### **4. 气泡的影响**
- **资源浪费：** GPU等设备利用率下降（部分设备空闲）。
- **延迟增加：** 整体推理时间延长（等待时间计入总耗时）。
- **吞吐量下降：** 单位时间内处理的Token数量减少。

---

### **5. 优化气泡的方法**
#### (1) **连续批处理（Continuous Batching）**
- **动态合并多个请求**的Token生成过程。
- 当一个请求等待时，其他请求可占用空闲资源。
- **效果：** 大幅减少气泡，提升GPU利用率（如框架：vLLM, HuggingFace TGI）。

#### (2) **非阻塞流水线调度**
- 使用**异步通信**重叠计算与数据传输。
- 例如：当Stage 1计算时，提前将已完成的激活值发送给Stage 2。

#### (3) **自适应阶段划分**
- 将计算量大的层分配到更多设备（如Attention层拆分）。
- 平衡各阶段计算时间，减少阻塞等待。

#### (4) **推测解码（Speculative Decoding）**
- 用小模型预先生成多个Token，大模型并行验证。
- **突破自回归依赖**，减少流水线调用次数。

---

### **总结**
| **关键点**                  | **说明**                                                                 |
|---------------------------|-------------------------------------------------------------------------|
| **气泡成因**               | 流水线启动/排空 + 阶段间数据依赖导致设备空闲                                         |
| **LLM推理的特殊性**        | 自回归生成使气泡重复出现；动态序列长度加剧负载不均衡；小批量请求限制流水线填充                     |
| **核心矛盾**               | **计算依赖**（前序阶段完成才能开始）与**资源并行**的冲突                                    |
| **优化方向**               | 连续批处理 → 减少气泡<br>推测解码 → 打破自回归<br>负载均衡 → 降低阻塞等待                     |

> 💡 **核心结论：**  
> 流水线并行的气泡是分布式系统**资源同步代价**的体现。在LLM推理中，由于**强顺序依赖**和**动态负载**，气泡问题尤为显著。优化需结合**调度策略**（如连续批处理）和**算法改进**（如推测解码）协同解决。

---

### 🧑‍💻 User

对于这种情况，我们可以使用微批次（micro batch）来解决，它是如何减少气泡的，主流的推理框架（sglang、vllm等）具体是如何实现的？

---

### 🤖 Assistant

微批次（Micro-batching）是**流水线并行中减少气泡的核心技术**，尤其在LLM推理中结合**连续批处理（Continuous Batching）** 能显著提升效率。以下是其原理与主流框架的实现解析：

---

### **一、微批次如何减少气泡？**
#### **1. 核心机制：填充流水线**
- **问题根源**：流水线气泡源于设备等待数据（前向传播时等输入，反向传播时等梯度）。
- **解决方案**：将**一个批量（Batch）拆分为多个微批次（Micro-batch）**，让流水线持续处理数据。
- **效果**：
  - **启动阶段**：微批次快速填充流水线各阶段，减少初始等待时间。
  - **稳定阶段**：各设备持续工作（如设备1处理微批次N时，设备2处理微批次N-1）。
  - **排空阶段**：后续微批次继续进入，延迟排空空闲。

#### **2. 气泡减少的数学直观**
- **未使用微批次**（批大小=B）：
  - 气泡占比 ≈ \(\frac{P-1}{P}\)（P为流水线阶段数）
  - 设备利用率低（如4阶段流水线仅25%时间满负荷）。
- **使用微批次**（批大小=B，微批次数=M）：
  - 气泡占比 ≈ \(\frac{P-1}{M + P-1}\)
  - **M越大，气泡比例越低**（M=8时，4阶段气泡占比≈27%；M=32时仅≈11%）。

> ✅ **关键结论**：微批次通过增加并行任务量，让设备等待时间被有效计算覆盖。

---

### **二、LLM推理框架的实现：连续批处理（Continuous Batching）**
微批次在推理中的挑战：**请求动态到达、序列长度不一、输出长度未知**。  
主流框架通过**连续批处理**动态管理微批次，核心创新是**解耦计算与请求调度**。

#### **1. 核心实现原理**
| **技术**               | **作用**                                                                 | **代表框架**         |
|------------------------|-------------------------------------------------------------------------|--------------------|
| **动态请求调度**        | 实时接收新请求，加入待处理队列                                          | vLLM, SGLang       |
| **非连续序列批处理**    | 将不同长度、不同生成阶段的序列合并为微批次                                | vLLM（PagedAttention）|
| **细粒度内存管理**      | 高效复用显存（如KV Cache），支持动态序列                                 | vLLM, SGLang       |
| **中断与抢占**          | 长序列生成中插入高优先级请求                                            | SGLang（RadixAttention）|

---

### **三、框架实现细节**
#### **1. vLLM：基于PagedAttention的动态批处理**
- **微批次生成**：  
  - 调度器将**待处理请求**按策略（如FCFS、最短序列优先）组织为微批次。
  - **PagedAttention** 将KV Cache分割为固定大小“页”，类似虚拟内存：
    - 不同序列的KV Cache可非连续存储。
    - 微批次包含**不完整序列**（如序列A生成第5个token，序列B生成第10个token）。
- **流水线执行**：
  ```python
  # 伪代码：vLLM调度循环
  while has_pending_requests():
      micro_batch = scheduler.select_requests()  # 选择可执行的请求
      for pipe_stage in [stage1, stage2, ...]: 
          inputs = prepare_inputs(micro_batch)  # 按需拼接输入
          outputs = pipe_stage.execute(inputs)   # 并行计算
          update_kv_cache(outputs)               # 更新分页KV Cache
      micro_batch.complete_tokens += 1           # 推进生成进度
  ```
- **优势**：  
  - 显存利用率提升3-5倍，吞吐量达静态批处理的10倍以上。

#### **2. SGLang：RadixAttention控制流优化**
- **微批次调度**：
  - 利用**RadixAttention**缓存**公共前缀的KV Cache**（如系统提示词）。
  - 将包含相同前缀的请求动态合并为微批次，减少重复计算。
- **流水线优化**：
  - **嵌套并行**：对长序列拆分为多个子任务（如检索→生成），各子任务内部流水线并行。
  - **抢占式调度**：高优先级请求可中断低优先级序列的生成。
- **示例控制流**：
  ```python
  # SGLang 提示词模板（含检索）
  @sglang.function
  def rag_generate(query):
      retrieved = sglang.retrieve(query)          # 阶段1：检索（独立设备）
      answer = sglang.generate(f"Answer: {retrieved} {query}")  # 阶段2：生成
      return answer
  ```
  - 检索与生成可分配至不同流水线阶段，微批次跨阶段异步传递。

#### **3. DeepSpeed-Inference：多级流水线**
- **层级拆分**：将模型按层分组为**超级阶段（Super Stages）**，组内张量并行。
- **微批次通信**：
  - 使用**环形通信**在设备间传递微批次激活值，减少通信延迟。
  - 为每个微批次分配唯一ID，确保顺序一致性。

---

### **四、效果对比**
| **框架**   | **核心技术**               | **微批次调度特点**                     | **适用场景**               |
|------------|--------------------------|--------------------------------------|--------------------------|
| **vLLM**   | PagedAttention           | 动态合并任意生成进度序列                | 通用高吞吐推理             |
| **SGLang** | RadixAttention           | 基于前缀匹配合并请求，支持控制流中断     | 复杂提示词/代理场景        |
| **DeepSpeed** | 多级流水+张量并行         | 超级阶段内高带宽通信                   | 超大规模模型部署           |

---

### **五、关键优化总结**
1. **动态微批次填充**  
   - 框架持续监控请求队列，即时生成微批次（如vLLM每2ms调度一次）。
2. **内存与计算解耦**  
   - PagedAttention/RadixAttention使**显存分配与序列解耦**，支持碎片化序列合并。
3. **异步流水线推进**  
   - 微批次在各阶段间异步流动，设备无等待（如SGLang使用CUDA流重叠计算）。
4. **自适应批大小**  
   - 根据当前负载动态调整微批次大小（如负载高时增大微批次减少调度开销）。

> 💡 **最终效果**：在72B参数LLM推理中，连续批处理+微批次可将GPU利用率从<20%（静态批处理）提升至60%以上，吞吐量提升4-10倍，延迟降低50%。

通过微批次与连续批处理的深度结合，现代推理框架将流水线并行的“气泡”压缩到最低限度，使千亿模型的高效部署成为现实。

---

### 🧑‍💻 User

是否可以把micro batch可以理解为动态调整batch大小？

---

### 🤖 Assistant

可以部分这样理解，但**不能完全等同**。Micro batch（微批次）和动态调整batch大小（Dynamic Batching）在目标和技术实现上存在关键差异。以下是详细解析：

---

### **一、核心区别**
| **特性**          | **Micro Batch**                             | **动态调整Batch大小**                     |
|--------------------|---------------------------------------------|------------------------------------------|
| **本质目的**       | 解决**流水线气泡**问题                        | 解决**请求吞吐量**与**延迟**的平衡问题       |
| **操作对象**       | **单个流水线内部**的计算单元                   | **请求队列**的调度策略                     |
| **动态性**         | 微批次大小**通常固定**（预设）                 | Batch大小**实时变化**（根据负载动态调整）     |
| **技术层级**       | 模型并行策略的一部分                           | 推理调度器的核心功能                       |

---

### **二、Micro Batch 的本质**
1. **技术定位**  
   Micro Batch 是**流水线并行（Pipeline Parallelism）中的最小计算单元**：  
   - 将一个大Batch拆分成多个微批次（如Batch=32 → 8个Micro Batch×4）
   - 目标：**填充流水线各阶段**，使GPU持续工作（减少气泡）

2. **固定性特征**  
   - 在训练/推理启动时预设（如 `micro_batch_size=4`）
   - 同一流水线中所有Micro Batch大小**必须相同**（否则无法并行）

3. **与流水线的绑定关系**  
   ```mermaid
   graph LR
   A[Batch=32] --> B[拆分8个Micro Batch]
   B --> C[Micro Batch1-4进入流水线]
   C --> D[设备1计算MB1]
   D --> E[设备2计算MB1 同时 设备1计算MB2]
   ```

---

### **三、动态调整Batch大小（Dynamic Batching）**
1. **技术定位**  
   - **推理调度器**的核心策略（如vLLM, TensorRT-LLM）
   - 目标：**最大化GPU利用率**，平衡吞吐量（Throughput）和延迟（Latency）

2. **动态性实现**  
   | **策略**               | **原理**                                  | **示例**                     |
   |------------------------|------------------------------------------|-----------------------------|
   | **即时组批**            | 将**新到达请求**与**进行中请求**合并        | 新请求Q3加入正在生成Q1,Q2的Batch |
   | **自适应批大小**        | 根据GPU显存/计算力动态调整最大Batch大小      | 高负载时Batch=16，低负载时Batch=4 |
   | **优先级中断**          | 高优先级请求抢占资源（如SGLang）            | VIP用户请求中断普通请求         |

3. **与Micro Batch的关系**  
   - 动态Batch作为**输入层**，Micro Batch作为**执行层**：
     ```python
     # vLLM 调度流程示例
     dynamic_batch = scheduler.get_requests()  # 动态生成Batch（大小可变）
     for micro_batch in split(dynamic_batch, size=4):  # 拆分成固定大小Micro Batch
         run_pipeline(micro_batch)  # 送入流水线
     ```

---

### **四、主流框架的实现对比**
#### **1. vLLM：解耦动态Batch与Micro Batch**
- **动态Batch层**  
  使用 `PageAttention` 动态合并请求（不同长度/生成进度）
- **Micro Batch层**  
  固定大小的执行单元（如每Micro Batch处理4个序列）
  ```python
  # vLLM 核心逻辑
  while True:
      requests = scheduler.get_pending_requests()  # 动态获取请求
      batch = dynamic_batcher(requests)            # 动态组批（大小可变）
      for micro_batch in split_batch(batch, size=4):  # 拆分为Micro Batch
          execute_model(micro_batch)               # 固定大小执行
  ```

#### **2. SGLang：RadixAttention控制流**
- **动态Batch**  
  通过前缀匹配合并请求（如相同提示词的请求合并）
- **Micro Batch**  
  在流水线各阶段间传递固定大小的数据块
  ```python
  # SGLang 执行流程
  batch = match_radix_prefix(requests)  # 动态合并相同前缀请求
  for stage in [retrieve, generate, post_process]:
      micro_batches = split(batch, micro_size=4)
      for mb in micro_batches:
          stage.run(mb)  # 每个阶段用固定Micro Batch执行
  ```

#### **3. DeepSpeed-Inference：混合并行**
- **动态Batch**  
  由 `InferenceEngine` 动态调整输入Batch大小
- **Micro Batch**  
  在流水线超级阶段（Super Stage）内部固定拆分
  ```
  [动态Batch] → [按Micro Batch拆分] → [Super Stage1] → [Super Stage2] 
  ```

---

### **五、关键结论**
1. **Micro Batch ≠ 动态调整Batch**  
   - Micro Batch：**固定大小**的流水线执行单元（解决气泡）
   - 动态Batch：**可变大小**的请求调度策略（优化资源）

2. **协作关系**  
   ```mermaid
   graph LR
   A[动态请求队列] --> B[动态调整Batch大小]
   B --> C[拆分为固定Micro Batch]
   C --> D[流水线并行执行]
   D --> E[输出拼接]
   ```
   - 动态Batch为**输入调度层**，Micro Batch为**并行执行层**

3. **工程实践**  
   - 在LLM推理中：  
     **动态Batch（如vLLM调度器） + 固定Micro Batch（流水线并行）** 是最佳组合  
   - 示例配置：  
     ```yaml
     vLLM_params:
       max_batch_size: 16    # 动态Batch最大容量
       micro_batch_size: 4   # 流水线微批次大小
     ```

> 💡 **简单记忆**：  
> - **动态Batch是“装菜的篮子”**（根据市场供应调整篮子大小）  
> - **Micro Batch是“切菜的刀法”**（固定尺寸切分以适配炒锅流水线）  
> 两者协同实现高效烹饪（推理），但本质是不同的工具。