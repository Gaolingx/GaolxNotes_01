# LLMæ¨ç†é‡‡æ ·å®ç°ä¸ä»£ç è§£æ

### ğŸ§‘â€ğŸ’» User

llmæ¨ç†é‡‡æ ·é˜¶æ®µï¼Œç»“åˆpythonä»£ç è¯´è¯´ `top_k_top_p_sampling_from_logits` ä¸€èˆ¬æ˜¯å¦‚ä½•å®ç°çš„ï¼Ÿ

---

### ğŸ¤– Assistant

# LLM æ¨ç†é‡‡æ ·ï¼šTop-K + Top-P é‡‡æ ·å®ç°è¯¦è§£

## ğŸ“Š é‡‡æ ·æµç¨‹æ¦‚è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Logits (åŸå§‹è¾“å‡º)                         â”‚
â”‚                    [batch_size, vocab_size]                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Temperature Scaling  â”‚  â† æ§åˆ¶åˆ†å¸ƒå¹³æ»‘åº¦
                    â”‚   logits / temp       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     Top-K Filter      â”‚  â† ä¿ç•™æ¦‚ç‡æœ€é«˜çš„Kä¸ª
                    â”‚  è¿‡æ»¤æ‰ä½æ¦‚ç‡token      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     Top-P Filter      â”‚  â† ä¿ç•™ç´¯ç§¯æ¦‚ç‡è¾¾Pçš„
                    â”‚   (Nucleus Sampling)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Softmax + Sample    â”‚  â† å¤šé¡¹å¼é‡‡æ ·
                    â”‚    multinomial        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                        Token ID è¾“å‡º
```

## ğŸ”§ å®Œæ•´å®ç°ä»£ç 

```python
import torch
import torch.nn.functional as F
from typing import Optional, Tuple

def top_k_top_p_sampling_from_logits(
    logits: torch.Tensor,
    top_k: int = 50,
    top_p: float = 0.9,
    temperature: float = 1.0,
    min_tokens_to_keep: int = 1
) -> torch.Tensor:
    """
    ä»logitsä¸­è¿›è¡ŒTop-Kå’ŒTop-Pè”åˆé‡‡æ ·
    
    Args:
        logits: æ¨¡å‹è¾“å‡º [batch_size, vocab_size]
        top_k: ä¿ç•™æ¦‚ç‡æœ€é«˜çš„Kä¸ªtoken (0è¡¨ç¤ºä¸ä½¿ç”¨)
        top_p: ç´¯ç§¯æ¦‚ç‡é˜ˆå€¼ (1.0è¡¨ç¤ºä¸ä½¿ç”¨)
        temperature: æ¸©åº¦å‚æ•°ï¼Œè¶Šå¤§åˆ†å¸ƒè¶Šå¹³æ»‘
        min_tokens_to_keep: æœ€å°‘ä¿ç•™çš„tokenæ•°
    
    Returns:
        é‡‡æ ·çš„token ids [batch_size]
    """
    # ä¿å­˜åŸå§‹å½¢çŠ¶
    batch_size, vocab_size = logits.shape
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Step 1: Temperature Scaling (æ¸©åº¦ç¼©æ”¾)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # æ¸©åº¦è¶Šé«˜ â†’ logitså·®å¼‚è¶Šå° â†’ åˆ†å¸ƒè¶Šå¹³æ»‘ â†’ é‡‡æ ·è¶Šéšæœº
    # æ¸©åº¦è¶Šä½ â†’ logitså·®å¼‚è¶Šå¤§ â†’ åˆ†å¸ƒè¶Šå°–é” â†’ é‡‡æ ·è¶Šç¡®å®š
    if temperature != 1.0:
        logits = logits / temperature
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Step 2: Top-K Filtering (Top-Kè¿‡æ»¤)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if top_k > 0:
        # ç¡®ä¿top_kä¸è¶…è¿‡è¯è¡¨å¤§å°
        top_k = min(top_k, vocab_size)
        
        # æ–¹æ³•1: ä½¿ç”¨torch.topkæ‰¾é˜ˆå€¼
        # topkè¿”å› (values, indices), å–æœ€åä¸€ä¸ªå€¼ä½œä¸ºé˜ˆå€¼
        top_k_values, _ = torch.topk(logits, top_k, dim=-1)
        threshold = top_k_values[:, -1:]  # [batch, 1]
        
        # å°†ä½äºé˜ˆå€¼çš„ä½ç½®è®¾ä¸º-inf
        logits = torch.where(
            logits < threshold,
            torch.full_like(logits, float('-inf')),
            logits
        )
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Step 3: Top-P Filtering (Nucleus Sampling)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if top_p < 1.0:
        # 3.1 æŒ‰æ¦‚ç‡é™åºæ’åº
        sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)
        
        # 3.2 è®¡ç®—ç´¯ç§¯æ¦‚ç‡
        sorted_probs = F.softmax(sorted_logits, dim=-1)
        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
        
        # 3.3 æ‰¾åˆ°éœ€è¦ç§»é™¤çš„ä½ç½®ï¼ˆç´¯ç§¯æ¦‚ç‡è¶…è¿‡top_pçš„ï¼‰
        # æ³¨æ„ï¼šæˆ‘ä»¬è¦ä¿ç•™ç¬¬ä¸€ä¸ªä½¿ç´¯ç§¯æ¦‚ç‡è¶…è¿‡top_pçš„token
        sorted_indices_to_remove = cumulative_probs > top_p
        
        # å³ç§»ä¸€ä½ï¼šç¡®ä¿åˆšå¥½è¶…è¿‡top_pçš„é‚£ä¸ªtokenè¢«ä¿ç•™
        sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()
        sorted_indices_to_remove[:, 0] = False  # å§‹ç»ˆä¿ç•™æœ€é«˜æ¦‚ç‡çš„token
        
        # 3.4 å°†maskæ˜ å°„å›åŸå§‹é¡ºåº
        indices_to_remove = torch.zeros_like(logits, dtype=torch.bool)
        indices_to_remove.scatter_(
            dim=-1, 
            index=sorted_indices, 
            src=sorted_indices_to_remove
        )
        
        # 3.5 åº”ç”¨mask
        logits = logits.masked_fill(indices_to_remove, float('-inf'))
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Step 4: Sampling (é‡‡æ ·)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 4.1 è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
    probs = F.softmax(logits, dim=-1)
    
    # 4.2 å¤šé¡¹å¼é‡‡æ ·
    next_tokens = torch.multinomial(probs, num_samples=1)
    
    return next_tokens.squeeze(-1)  # [batch_size]
```

## ğŸ“ˆ å›¾è§£ Top-P è¿‡æ»¤è¿‡ç¨‹

```
åŸå§‹æ¦‚ç‡åˆ†å¸ƒ (å·²æ’åº):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Token:    A      B      C      D      E      F    ... â”‚
â”‚ Prob:   0.35   0.25   0.15   0.10   0.08   0.04   ... â”‚
â”‚ CumP:   0.35   0.60   0.75   0.85   0.93   0.97   ... â”‚
â”‚                                      â†‘                 â”‚
â”‚                              top_p=0.9 æˆªæ–­ç‚¹          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ä¿ç•™ token: A, B, C, D, E (ç´¯ç§¯æ¦‚ç‡ â‰¤ 0.9)
è¿‡æ»¤ token: F, ... (ç´¯ç§¯æ¦‚ç‡ > 0.9)
```

## ğŸš€ ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆæ”¯æŒæ‰¹å¤„ç† + æ€§èƒ½ä¼˜åŒ–ï¼‰

```python
def top_k_top_p_sampling_optimized(
    logits: torch.Tensor,
    top_k: int = 0,
    top_p: float = 1.0,
    temperature: float = 1.0,
    repetition_penalty: float = 1.0,
    generated_tokens: Optional[torch.Tensor] = None
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    ä¼˜åŒ–ç‰ˆé‡‡æ ·å‡½æ•°ï¼Œæ”¯æŒé‡å¤æƒ©ç½š
    
    Returns:
        (sampled_tokens, probs) - é‡‡æ ·çš„tokenå’Œå¯¹åº”æ¦‚ç‡
    """
    logits = logits.clone()
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # å¯é€‰: é‡å¤æƒ©ç½š (Repetition Penalty)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if repetition_penalty != 1.0 and generated_tokens is not None:
        for batch_idx in range(logits.size(0)):
            for token_id in set(generated_tokens[batch_idx].tolist()):
                if logits[batch_idx, token_id] > 0:
                    logits[batch_idx, token_id] /= repetition_penalty
                else:
                    logits[batch_idx, token_id] *= repetition_penalty
    
    # Temperature
    if temperature != 1.0:
        logits = logits / temperature
    
    # Top-K (å‘é‡åŒ–ä¼˜åŒ–)
    if top_k > 0:
        top_k = min(top_k, logits.size(-1))
        values, _ = logits.topk(top_k, dim=-1)
        min_values = values[:, -1].unsqueeze(-1)
        logits = torch.where(logits < min_values, float('-inf'), logits)
    
    # Top-P (å‘é‡åŒ–ä¼˜åŒ–)
    if top_p < 1.0:
        sorted_logits, sorted_indices = torch.sort(logits, descending=True)
        sorted_probs = F.softmax(sorted_logits, dim=-1)
        cumsum_probs = sorted_probs.cumsum(dim=-1)
        
        # åˆ›å»ºmask
        mask = cumsum_probs - sorted_probs > top_p
        sorted_logits[mask] = float('-inf')
        
        # æ¢å¤åŸå§‹é¡ºåº
        logits = sorted_logits.gather(-1, sorted_indices.argsort(-1))
    
    # é‡‡æ ·
    probs = F.softmax(logits, dim=-1)
    tokens = torch.multinomial(probs, num_samples=1).squeeze(-1)
    
    # è¿”å›tokenå’Œå¯¹åº”çš„æ¦‚ç‡
    token_probs = probs.gather(-1, tokens.unsqueeze(-1)).squeeze(-1)
    
    return tokens, token_probs
```

## ğŸ¯ ä½¿ç”¨ç¤ºä¾‹

```python
# æ¨¡æ‹Ÿåœºæ™¯
batch_size = 2
vocab_size = 50000

# æ¨¡æ‹Ÿæ¨¡å‹è¾“å‡ºçš„logits
logits = torch.randn(batch_size, vocab_size)

# é‡‡æ ·å‚æ•°
sampling_params = {
    'top_k': 50,           # åªè€ƒè™‘top 50ä¸ªtoken
    'top_p': 0.9,          # ç´¯ç§¯æ¦‚ç‡90%æˆªæ–­
    'temperature': 0.8,    # ç¨å¾®é™ä½éšæœºæ€§
}

# æ‰§è¡Œé‡‡æ ·
next_tokens = top_k_top_p_sampling_from_logits(logits, **sampling_params)
print(f"Sampled tokens: {next_tokens}")
# è¾“å‡º: Sampled tokens: tensor([12345, 6789])
```

## âš–ï¸ å‚æ•°æ•ˆæœå¯¹æ¯”

| å‚æ•° | ä½å€¼æ•ˆæœ | é«˜å€¼æ•ˆæœ |
|------|---------|---------|
| **temperature** | æ›´ç¡®å®šæ€§ã€æ›´ä¿å®ˆ | æ›´éšæœºã€æ›´å¤šæ · |
| **top_k** | å€™é€‰æ›´å°‘ã€æ›´èšç„¦ | å€™é€‰æ›´å¤šã€æ›´å¤šæ · |
| **top_p** | åªé€‰é«˜æ¦‚ç‡token | å…è®¸æ›´å¤šä½æ¦‚ç‡token |

```
temperature=0.1        temperature=1.0        temperature=2.0
    â–²                      â–²                      â–²
    â”‚â–ˆ                     â”‚â–“                     â”‚â–‘â–‘
    â”‚â–ˆ                     â”‚â–“â–“                    â”‚â–‘â–‘â–‘
    â”‚â–ˆ                     â”‚â–“â–“â–“                   â”‚â–‘â–‘â–‘â–‘â–‘
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    (å°–é”åˆ†å¸ƒ)              (æ­£å¸¸åˆ†å¸ƒ)              (å¹³æ»‘åˆ†å¸ƒ)
```

è¿™å°±æ˜¯ LLM æ¨ç†ä¸­ Top-K/Top-P é‡‡æ ·çš„æ ¸å¿ƒå®ç°ï¼