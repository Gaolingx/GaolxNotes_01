# LLMæ¨ç†é‡‡æ ·å®ç°ä¸ä»£ç è§£æ

### ğŸ§‘â€ğŸ’» User

llmæ¨ç†é‡‡æ ·é˜¶æ®µï¼Œç»“åˆpythonä»£ç è¯´è¯´ `top_k_top_p_sampling_from_logits` ä¸€èˆ¬æ˜¯å¦‚ä½•å®ç°çš„ï¼Ÿ

---

### ğŸ¤– Assistant

æˆ‘æ¥è¯¦ç»†è§£é‡Šä¸€ä¸‹LLMæ¨ç†ä¸­top-kå’Œtop-pé‡‡æ ·çš„å®ç°æ–¹æ³•ï¼Œå¹¶æä¾›å®Œæ•´çš„Pythonä»£ç ã€‚

## åŸºæœ¬æ¦‚å¿µ

- **top-ké‡‡æ ·**ï¼šåªä»æ¦‚ç‡æœ€é«˜çš„kä¸ªtokenä¸­é‡‡æ ·
- **top-pé‡‡æ ·ï¼ˆæ ¸é‡‡æ ·ï¼‰**ï¼šä»ç´¯ç§¯æ¦‚ç‡è¾¾åˆ°pçš„æœ€å°tokené›†åˆä¸­é‡‡æ ·

## å®Œæ•´å®ç°ä»£ç 

```python
import torch
import torch.nn.functional as F
from typing import Optional, Tuple
import numpy as np

def top_k_top_p_sampling_from_logits(
    logits: torch.Tensor,
    top_k: Optional[int] = None,
    top_p: Optional[float] = None,
    temperature: float = 1.0,
    return_probs: bool = False
) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
    """
    Top-kå’Œtop-pé‡‡æ ·å®ç°
    
    Args:
        logits: åŸå§‹logitså¼ é‡ [batch_size, vocab_size] æˆ– [vocab_size]
        top_k: ä¿ç•™çš„æœ€é«˜æ¦‚ç‡tokenæ•°é‡
        top_p: ç´¯ç§¯æ¦‚ç‡é˜ˆå€¼ (0-1)
        temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶éšæœºæ€§
        return_probs: æ˜¯å¦è¿”å›å¤„ç†åçš„æ¦‚ç‡åˆ†å¸ƒ
    
    Returns:
        sampled_tokens: é‡‡æ ·çš„token [batch_size] æˆ– scalar
        probs: å¤„ç†åçš„æ¦‚ç‡åˆ†å¸ƒ (å¯é€‰)
    """
    
    # ç¡®ä¿è¾“å…¥æ˜¯2Dçš„ [batch_size, vocab_size]
    if logits.dim() == 1:
        logits = logits.unsqueeze(0)
    
    # åº”ç”¨æ¸©åº¦å‚æ•°
    if temperature != 1.0:
        logits = logits / temperature
    
    # è½¬æ¢ä¸ºæ¦‚ç‡
    probs = F.softmax(logits, dim=-1)
    
    # åº”ç”¨top-kè¿‡æ»¤
    if top_k is not None and top_k > 0:
        # è·å–top-kçš„å€¼å’Œç´¢å¼•
        top_k_probs, top_k_indices = torch.topk(probs, top_k, dim=-1)
        
        # åˆ›å»ºæ–°çš„æ¦‚ç‡åˆ†å¸ƒï¼Œétop-kçš„ä½ç½®è®¾ä¸º0
        new_probs = torch.zeros_like(probs)
        new_probs.scatter_(-1, top_k_indices, top_k_probs)
        probs = new_probs
    
    # åº”ç”¨top-pè¿‡æ»¤
    if top_p is not None and top_p < 1.0:
        # å¯¹æ¦‚ç‡è¿›è¡Œæ’åº
        sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=-1)
        
        # è®¡ç®—ç´¯ç§¯æ¦‚ç‡
        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
        
        # æ‰¾åˆ°ç´¯ç§¯æ¦‚ç‡è¶…è¿‡top_pçš„ç¬¬ä¸€ä¸ªä½ç½®
        # æˆ‘ä»¬éœ€è¦ç§»é™¤ç´¯ç§¯æ¦‚ç‡è¶…è¿‡top_pçš„token
        sorted_indices_to_remove = cumulative_probs > top_p
        
        # ç¡®ä¿è‡³å°‘ä¿ç•™ä¸€ä¸ªtoken
        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
        sorted_indices_to_remove[..., 0] = 0
        
        # å°†éœ€è¦ç§»é™¤çš„ä½ç½®è®¾ä¸º0
        for i in range(probs.size(0)):
            indices_to_remove = sorted_indices[i][sorted_indices_to_remove[i]]
            probs[i].scatter_(-1, indices_to_remove, 0.0)
    
    # é‡æ–°å½’ä¸€åŒ–æ¦‚ç‡åˆ†å¸ƒ
    probs = probs / torch.sum(probs, dim=-1, keepdim=True)
    
    # ä»å¤„ç†åçš„åˆ†å¸ƒä¸­é‡‡æ ·
    if probs.size(0) == 1:
        # å•æ ·æœ¬æƒ…å†µ
        sampled_token = torch.multinomial(probs[0], num_samples=1).item()
        result = sampled_token
    else:
        # æ‰¹é‡é‡‡æ ·
        sampled_tokens = torch.multinomial(probs, num_samples=1).squeeze(-1)
        result = sampled_tokens
    
    if return_probs:
        return result, probs
    else:
        return result

# æ›´ç®€æ´çš„ç‰ˆæœ¬ï¼ˆå•æ ·æœ¬ï¼‰
def sample_from_logits_simple(
    logits: torch.Tensor,
    top_k: int = 50,
    top_p: float = 0.9,
    temperature: float = 1.0
) -> int:
    """ç®€åŒ–ç‰ˆçš„top-k top-pé‡‡æ ·"""
    
    # åº”ç”¨æ¸©åº¦
    logits = logits / temperature
    
    # top-kè¿‡æ»¤
    if top_k > 0:
        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]
        logits[indices_to_remove] = -float('Inf')
    
    # è½¬æ¢ä¸ºæ¦‚ç‡
    probs = F.softmax(logits, dim=-1)
    
    # top-pè¿‡æ»¤
    if top_p < 1.0:
        sorted_probs, sorted_indices = torch.sort(probs, descending=True)
        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
        
        # ç§»é™¤ç´¯ç§¯æ¦‚ç‡è¶…è¿‡pçš„token
        sorted_indices_to_remove = cumulative_probs > top_p
        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
        sorted_indices_to_remove[..., 0] = False
        
        indices_to_remove = sorted_indices[sorted_indices_to_remove]
        probs[indices_to_remove] = 0
        
        # é‡æ–°å½’ä¸€åŒ–
        if torch.sum(probs) > 0:
            probs = probs / torch.sum(probs)
    
    # é‡‡æ ·
    return torch.multinomial(probs, num_samples=1).item()
```

## ä½¿ç”¨ç¤ºä¾‹

```python
# ç¤ºä¾‹1ï¼šåŸºæœ¬ä½¿ç”¨
def demonstrate_sampling():
    # æ¨¡æ‹ŸLLMè¾“å‡ºçš„logits
    vocab_size = 10000
    logits = torch.randn(vocab_size)
    
    print("=== ä¸åŒé‡‡æ ·ç­–ç•¥å¯¹æ¯” ===")
    
    # 1. è´ªå©ªé‡‡æ ·ï¼ˆtemperature=0ï¼‰
    greedy_token = top_k_top_p_sampling_from_logits(
        logits, temperature=0.0
    )
    print(f"è´ªå©ªé‡‡æ ·: {greedy_token}")
    
    # 2. ä»…ä½¿ç”¨temperature
    temp_token = top_k_top_p_sampling_from_logits(
        logits, temperature=0.8
    )
    print(f"æ¸©åº¦é‡‡æ ·(0.8): {temp_token}")
    
    # 3. top-ké‡‡æ ·
    topk_token = top_k_top_p_sampling_from_logits(
        logits, top_k=50, temperature=1.0
    )
    print(f"Top-ké‡‡æ ·(k=50): {topk_token}")
    
    # 4. top-pé‡‡æ ·
    topp_token = top_k_top_p_sampling_from_logits(
        logits, top_p=0.9, temperature=1.0
    )
    print(f"Top-pé‡‡æ ·(p=0.9): {topp_token}")
    
    # 5. ç»“åˆä½¿ç”¨
    combined_token = top_k_top_p_sampling_from_logits(
        logits, top_k=50, top_p=0.9, temperature=0.8
    )
    print(f"ç»„åˆé‡‡æ ·: {combined_token}")

# ç¤ºä¾‹2ï¼šæ‰¹é‡é‡‡æ ·
def batch_sampling_example():
    batch_size = 3
    vocab_size = 1000
    batch_logits = torch.randn(batch_size, vocab_size)
    
    print("\n=== æ‰¹é‡é‡‡æ · ===")
    sampled_tokens = top_k_top_p_sampling_from_logits(
        batch_logits, top_k=40, top_p=0.95, temperature=0.7
    )
    print(f"æ‰¹é‡é‡‡æ ·ç»“æœ: {sampled_tokens}")

# ç¤ºä¾‹3ï¼šæŸ¥çœ‹æ¦‚ç‡åˆ†å¸ƒ
def probability_analysis():
    logits = torch.tensor([3.0, 1.0, 0.5, 0.1, -1.0, -2.0])
    
    print("\n=== æ¦‚ç‡åˆ†å¸ƒåˆ†æ ===")
    sampled_token, processed_probs = top_k_top_p_sampling_from_logits(
        logits, top_k=3, top_p=0.9, return_probs=True
    )
    
    original_probs = F.softmax(logits, dim=-1)
    
    print("åŸå§‹æ¦‚ç‡:", [f"{p:.3f}" for p in original_probs.tolist()])
    print("å¤„ç†åæ¦‚ç‡:", [f"{p:.3f}" for p in processed_probs[0].tolist()])
    print(f"é‡‡æ ·token: {sampled_token}")

if __name__ == "__main__":
    demonstrate_sampling()
    batch_sampling_example()
    probability_analysis()
```

## å®é™…åº”ç”¨ä¸­çš„æ³¨æ„äº‹é¡¹

```python
class TextGenerator:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
    
    def generate_text(
        self,
        prompt: str,
        max_length: int = 100,
        top_k: int = 50,
        top_p: float = 0.9,
        temperature: float = 1.0,
        repetition_penalty: float = 1.0
    ) -> str:
        """å®Œæ•´çš„æ–‡æœ¬ç”Ÿæˆå‡½æ•°"""
        
        input_ids = self.tokenizer.encode(prompt, return_tensors="pt")
        
        for _ in range(max_length):
            # è·å–æ¨¡å‹è¾“å‡º
            with torch.no_grad():
                outputs = self.model(input_ids)
                next_token_logits = outputs.logits[:, -1, :]
            
            # é‡å¤æƒ©ç½š
            if repetition_penalty != 1.0:
                self.apply_repetition_penalty(next_token_logits, input_ids, repetition_penalty)
            
            # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
            next_token = top_k_top_p_sampling_from_logits(
                next_token_logits[0],
                top_k=top_k,
                top_p=top_p,
                temperature=temperature
            )
            
            # æ·»åŠ åˆ°åºåˆ—ä¸­
            input_ids = torch.cat([input_ids, torch.tensor([[next_token]])], dim=-1)
            
            # æ£€æŸ¥ç»“æŸæ¡ä»¶
            if next_token == self.tokenizer.eos_token_id:
                break
        
        return self.tokenizer.decode(input_ids[0])
    
    def apply_repetition_penalty(self, logits, input_ids, penalty):
        """åº”ç”¨é‡å¤æƒ©ç½š"""
        for token_id in set(input_ids[0].tolist()):
            if logits[0, token_id] < 0:
                logits[0, token_id] *= penalty
            else:
                logits[0, token_id] /= penalty
```

## å…³é”®è¦ç‚¹

1. **æ‰§è¡Œé¡ºåº**ï¼šé€šå¸¸å…ˆåº”ç”¨temperatureï¼Œç„¶åtop-kï¼Œæœ€åtop-p
2. **æ•°å€¼ç¨³å®šæ€§**ï¼šæ³¨æ„å¤„ç†æ¦‚ç‡ä¸º0çš„æƒ…å†µ
3. **æ‰¹é‡å¤„ç†**ï¼šæ”¯æŒæ‰¹é‡é‡‡æ ·æé«˜æ•ˆç‡
4. **å‚æ•°è°ƒä¼˜**ï¼š
   - temperatureâ†‘ â†’ éšæœºæ€§â†‘
   - top-kâ†“ â†’ å¤šæ ·æ€§â†“
   - top-pâ†“ â†’ ç¡®å®šæ€§â†‘

è¿™ç§é‡‡æ ·ç­–ç•¥åœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼Œæä¾›äº†å¯¹è¾“å‡ºå¤šæ ·æ€§çš„ç²¾ç»†æ§åˆ¶ã€‚