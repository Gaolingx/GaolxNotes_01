# PyTorchå­¦ä¹ ç‡è°ƒåº¦å™¨åŠå¸¸è§æ–¹æ³•ä»‹ç»

### ğŸ§‘â€ğŸ’» User

ä»€ä¹ˆæ˜¯å­¦ä¹ ç‡è°ƒåº¦ï¼Œä»‹ç»ä¸‹pytorchä¸­å¸¸è§çš„å­¦ä¹ ç‡è°ƒåº¦å™¨ä»¥åŠç›¸å…³çš„functionã€‚

---

### ğŸ¤– Assistant

å½“ç„¶ï¼Œè¿™æ˜¯ä¸€ä¸ªå…³äºå­¦ä¹ ç‡è°ƒåº¦ä»¥åŠåœ¨PyTorchä¸­å¦‚ä½•å®ç°çš„è¯¦ç»†è§£é‡Šã€‚

### 1. ä»€ä¹ˆæ˜¯å­¦ä¹ ç‡è°ƒåº¦ï¼Ÿ

**å­¦ä¹ ç‡ï¼ˆLearning Rateï¼‰** æ˜¯ç¥ç»ç½‘ç»œè®­ç»ƒä¸­æœ€é‡è¦çš„è¶…å‚æ•°ä¹‹ä¸€ï¼Œå®ƒæ§åˆ¶ç€æ¨¡å‹æ ¹æ®æŸå¤±æ¢¯åº¦æ›´æ–°æƒé‡çš„æ­¥é•¿ã€‚

- **å­¦ä¹ ç‡è¿‡å¤§**ï¼šå¯èƒ½å¯¼è‡´æ¨¡å‹åœ¨æœ€ä¼˜å€¼é™„è¿‘éœ‡è¡ï¼Œæ— æ³•æ”¶æ•›ï¼Œç”šè‡³å‘æ•£ã€‚
- **å­¦ä¹ ç‡è¿‡å°**ï¼šå¯èƒ½å¯¼è‡´æ”¶æ•›é€Ÿåº¦ææ…¢ï¼Œå®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ç‚¹ã€‚

**å­¦ä¹ ç‡è°ƒåº¦ï¼ˆLearning Rate Schedulingï¼‰** æ˜¯ä¸€ç§åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­**åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡**çš„æŠ€æœ¯ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šå¹¶éåœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­éƒ½ä½¿ç”¨ä¸€ä¸ªå›ºå®šä¸å˜çš„å­¦ä¹ ç‡ï¼Œè€Œæ˜¯æ ¹æ®é¢„å®šä¹‰çš„è®¡åˆ’æˆ–è®­ç»ƒæ—¶çš„åé¦ˆï¼ˆå¦‚éªŒè¯é›†æŸå¤±ä¸å†ä¸‹é™ï¼‰æ¥è°ƒæ•´å­¦ä¹ ç‡ã€‚

**ä¸ºä»€ä¹ˆéœ€è¦å­¦ä¹ ç‡è°ƒåº¦ï¼Ÿ**
1.  **æ›´å¿«æ”¶æ•›**ï¼šåˆæœŸä½¿ç”¨è¾ƒå¤§çš„å­¦ä¹ ç‡å¯ä»¥å¿«é€Ÿä¸‹é™ï¼ŒåæœŸä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡å¯ä»¥ç²¾ç»†è°ƒæ•´ï¼Œé€¼è¿‘æœ€ä¼˜è§£ã€‚
2.  **é¿å…å±€éƒ¨æœ€ä¼˜**ï¼šæœ‰æ—¶é€‚å½“å¢å¤§å­¦ä¹ ç‡å¯ä»¥å¸®åŠ©æ¨¡å‹è·³å‡ºå°–é”çš„å±€éƒ¨æœ€ä¼˜ç‚¹ï¼Œæ‰¾åˆ°æ›´å¹³å¦ã€æ³›åŒ–èƒ½åŠ›æ›´å¼ºçš„åŒºåŸŸã€‚
3.  **æé«˜æ¨¡å‹æ€§èƒ½**ï¼šè¿™æ˜¯æœ€ç›´æ¥çš„ç›®æ ‡ï¼Œé€šè¿‡æ›´æœ‰æ•ˆçš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œæœ€ç»ˆè·å¾—åœ¨éªŒè¯é›†ä¸Šè¡¨ç°æ›´å¥½çš„æ¨¡å‹ã€‚

---

### 2. PyTorch ä¸­çš„å­¦ä¹ ç‡è°ƒåº¦å™¨ (`torch.optim.lr_scheduler`)

PyTorch åœ¨ `torch.optim.lr_scheduler` æ¨¡å—ä¸­æä¾›äº†å¤šç§å­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚è¿™äº›è°ƒåº¦å™¨é€šå¸¸æ¥æ”¶ä¸€ä¸ªä¼˜åŒ–å™¨ (`optimizer`) å®ä¾‹ä½œä¸ºå‚æ•°ï¼Œå¹¶é€šè¿‡å…¶ `step()` æ–¹æ³•åœ¨æ¯ä¸ª epoch æˆ–æ¯ä¸ª step åæ›´æ–°å­¦ä¹ ç‡ã€‚

#### å¸¸è§çš„å­¦ä¹ ç‡è°ƒåº¦å™¨åŠå…¶ç›¸å…³å‡½æ•°

ä»¥ä¸‹æ˜¯å‡ ä¸ªæœ€å¸¸ç”¨å’Œç»å…¸çš„å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼š

##### a. `StepLR`
- **å·¥ä½œåŸç†**ï¼šæ¯ç»è¿‡å›ºå®šçš„è®­ç»ƒè½®æ•°ï¼ˆepochï¼‰ï¼Œå°±å°†å­¦ä¹ ç‡ä¹˜ä»¥ä¸€ä¸ªå› å­ï¼ˆgammaï¼‰ã€‚
- **å…¬å¼**ï¼š`lr = initial_lr * gamma ^ (epoch // step_size)`
- **é€‚ç”¨åœºæ™¯**ï¼šè¿™æ˜¯æœ€åŸºç¡€çš„è°ƒåº¦ç­–ç•¥ï¼Œé€‚ç”¨äºç»éªŒæ€§è°ƒæ•´ã€‚
- **ä»£ç ç¤ºä¾‹**ï¼š
    ```python
    import torch.optim as optim
    import torch.optim.lr_scheduler as lr_scheduler

    optimizer = optim.SGD(model.parameters(), lr=0.1)
    # æ¯ç»è¿‡ 30 ä¸ª epochï¼Œå­¦ä¹ ç‡å˜ä¸ºåŸæ¥çš„ 0.1 å€
    scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)

    for epoch in range(100):
        train(...)
        validate(...)
        scheduler.step() # é€šå¸¸åœ¨æ¯ä¸ª epoch ç»“æŸåè°ƒç”¨
    ```

##### b. `MultiStepLR`
- **å·¥ä½œåŸç†**ï¼šä¸ `StepLR` ç±»ä¼¼ï¼Œä½†ä¸æ˜¯åœ¨å›ºå®šçš„é—´éš”æ­¥æ•°è¡°å‡ï¼Œè€Œæ˜¯åœ¨æŒ‡å®šçš„ epoch é‡Œç¨‹ç¢‘ï¼ˆmilestonesï¼‰å¤„è¡°å‡ã€‚
- **é€‚ç”¨åœºæ™¯**ï¼šå½“ä½ çŸ¥é“æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹çš„ç‰¹å®šé˜¶æ®µéœ€è¦è°ƒæ•´å­¦ä¹ ç‡æ—¶ï¼ˆä¾‹å¦‚ï¼Œåœ¨æ€» epoch æ•°çš„ 50% å’Œ 75% æ—¶ï¼‰ã€‚
- **ä»£ç ç¤ºä¾‹**ï¼š
    ```python
    # å½“è®­ç»ƒåˆ°ç¬¬ 50 å’Œç¬¬ 80 ä¸ª epoch æ—¶ï¼Œå„è¿›è¡Œä¸€æ¬¡è¡°å‡
    scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[50, 80], gamma=0.1)
    ```

##### c. `ExponentialLR`
- **å·¥ä½œåŸç†**ï¼šæ¯ä¸ª epoch éƒ½ä¼šå°†å­¦ä¹ ç‡ä¹˜ä»¥ä¸€ä¸ªè¡°å‡ç³»æ•° gammaã€‚
- **å…¬å¼**ï¼š`lr = initial_lr * gamma ^ epoch`
- **é€‚ç”¨åœºæ™¯**ï¼šéœ€è¦éå¸¸å¹³æ»‘ä¸”æŒç»­è¡°å‡çš„å­¦ä¹ ç‡è®¡åˆ’ã€‚
- **ä»£ç ç¤ºä¾‹**ï¼š
    ```python
    # æ¯ä¸ª epoch åï¼Œå­¦ä¹ ç‡éƒ½å˜ä¸ºåŸæ¥çš„ 0.95 å€
    scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)
    ```

##### d. `CosineAnnealingLR`
- **å·¥ä½œåŸç†**ï¼šä½¿ç”¨ä½™å¼¦å‡½æ•°æ¥è°ƒæ•´å­¦ä¹ ç‡ã€‚å­¦ä¹ ç‡ä»åˆå§‹å€¼å¼€å§‹ï¼Œå…ˆç¼“æ…¢ä¸‹é™ï¼Œç„¶ååœ¨ä¸­é—´é˜¶æ®µå¿«é€Ÿä¸‹é™ï¼Œæœ€åå†ç¼“æ…¢æ¥è¿‘æœ€å°å€¼ã€‚
- **å…¬å¼**ï¼š`Î·_t = Î·_min + 0.5 * (Î·_max - Î·_min) * (1 + cos(T_cur / T_max * Ï€))`
    - `Î·_max` æ˜¯åˆå§‹å­¦ä¹ ç‡ã€‚
    - `Î·_min` æ˜¯æœ€å°å­¦ä¹ ç‡ã€‚
    - `T_cur` æ˜¯å½“å‰ epoch æ•°ã€‚
    - `T_max` æ˜¯åŠä¸ªä½™å¼¦å‘¨æœŸçš„ epoch æ•°ï¼ˆé€šå¸¸è®¾ä¸ºæ€» epoch æ•°ï¼‰ã€‚
- **é€‚ç”¨åœºæ™¯**ï¼šåœ¨è®¸å¤šä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸã€‚å®ƒæä¾›äº†ä¸€ä¸ªéå¸¸å¹³æ»‘çš„é€€ç«è¿‡ç¨‹ã€‚
- **ä»£ç ç¤ºä¾‹**ï¼š
    ```python
    # å‡è®¾æ€»å…±è®­ç»ƒ 100 ä¸ª epochï¼Œå­¦ä¹ ç‡å°†ä» 0.1 æŒ‰ä½™å¼¦æ›²çº¿è¡°å‡åˆ° 0
    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=0)
    ```

##### e. `ReduceLROnPlateau`
- **å·¥ä½œåŸç†**ï¼š**åŸºäºéªŒè¯æŒ‡æ ‡ï¼ˆè€Œé epoch æ•°ï¼‰** çš„åŠ¨æ€è°ƒåº¦å™¨ã€‚å½“æŸä¸ªæŒ‡æ ‡ï¼ˆå¦‚éªŒè¯æŸå¤±ï¼‰åœæ­¢æ”¹å–„è¶…è¿‡ä¸€ä¸ªâ€œè€å¿ƒâ€ï¼ˆpatienceï¼‰å€¼æ—¶ï¼Œå°±ä¼šé™ä½å­¦ä¹ ç‡ã€‚
- **é€‚ç”¨åœºæ™¯**ï¼šéå¸¸å®ç”¨å’Œå¼ºå¤§ï¼Œå› ä¸ºä½ ä¸éœ€è¦é¢„å…ˆçŸ¥é“ä½•æ—¶éœ€è¦è¡°å‡ã€‚å®ƒè®©è®­ç»ƒè¿‡ç¨‹è‡ªåŠ¨å†³å®šä½•æ—¶è°ƒæ•´å­¦ä¹ ç‡ã€‚
- **é‡è¦å‚æ•°**ï¼š
    - `mode`ï¼š`min`ï¼ˆç›‘æ§æŒ‡æ ‡æ˜¯å¦ä¸å†ä¸‹é™ï¼‰æˆ– `max`ï¼ˆç›‘æ§æŒ‡æ ‡æ˜¯å¦ä¸å†ä¸Šå‡ï¼‰ã€‚
    - `factor`ï¼šè¡°å‡å› å­ã€‚
    - `patience`ï¼šèƒ½å®¹å¿æŒ‡æ ‡æ²¡æœ‰æ”¹å–„çš„ epoch æ•°ã€‚
    - `threshold`ï¼šè¡¡é‡æ–°çš„æœ€ä½³å€¼çš„é˜ˆå€¼ã€‚
- **ä»£ç ç¤ºä¾‹**ï¼š
    ```python
    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)

    for epoch in range(100):
        train(...)
        val_loss = validate(...)

        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å°†ç›‘æ§çš„æŒ‡æ ‡ä¼ å…¥ step() æ–¹æ³•
        scheduler.step(val_loss)
    ```

##### f. `OneCycleLR`
- **å·¥ä½œåŸç†**ï¼šä¸€ç§éå¸¸æµè¡Œçš„ç­–ç•¥ï¼Œå®ƒåœ¨ä¸€ä¸ªå‘¨æœŸå†…å…ˆè®©å­¦ä¹ ç‡ä»è¾ƒä½å€¼çº¿æ€§å¢åŠ åˆ°å¾ˆé«˜çš„å€¼ï¼Œç„¶åå†çº¿æ€§ä¸‹é™åˆ°ä¸€ä¸ªæ¯”åˆå§‹å€¼æ›´ä½çš„å€¼ã€‚å®ƒé€šå¸¸ä¸åŠ¨é‡ï¼ˆmomentumï¼‰çš„åå‘å‘¨æœŸç»“åˆä½¿ç”¨ã€‚
- **é€‚ç”¨åœºæ™¯**ï¼šè®­ç»ƒé€Ÿåº¦æå¿«ï¼Œå¹¶ä¸”é€šå¸¸èƒ½è¾¾åˆ°å¾ˆå¥½çš„æ€§èƒ½ã€‚è¢«ç§°ä¸ºâ€œè¶…çº§æ”¶æ•›â€ï¼ˆSuper-Convergenceï¼‰ç­–ç•¥ã€‚é€‚ç”¨äº epoch æ•°ç›¸å¯¹è¾ƒå°‘çš„è®­ç»ƒã€‚
- **ä»£ç ç¤ºä¾‹**ï¼š
    ```python
    scheduler = lr_scheduler.OneCycleLR(optimizer,
                                       max_lr=0.1,        # å³°å€¼å­¦ä¹ ç‡
                                       steps_per_epoch=len(train_loader),
                                       epochs=10)
    # æ³¨æ„ï¼šOneCycleLR é€šå¸¸éœ€è¦åœ¨æ¯ä¸ª BATCH åè°ƒç”¨ .step()
    for epoch in range(10):
        for batch in train_loader:
            train_batch(...)
            scheduler.step() # åœ¨æ¯ä¸ª batch åæ›´æ–°
    ```

---

### 3. æ ¸å¿ƒå‡½æ•°ä¸æ–¹æ³•

æ‰€æœ‰è°ƒåº¦å™¨éƒ½å…±äº«ä¸€äº›æ ¸å¿ƒæ–¹æ³•ï¼š

- `__init__(optimizer, ...)`ï¼šæ„é€ å‡½æ•°ï¼Œç¬¬ä¸€ä¸ªå‚æ•°å¿…é¡»æ˜¯ä¼˜åŒ–å™¨å®ä¾‹ï¼Œå…¶ä»–å‚æ•°å› è°ƒåº¦å™¨è€Œå¼‚ã€‚
- `step()`ï¼šæ›´æ–°å­¦ä¹ ç‡ã€‚å¯¹äºå¤§å¤šæ•°è°ƒåº¦å™¨ï¼Œè¿™æ˜¯åœ¨æ¯ä¸ª **epoch** åè°ƒç”¨ã€‚å¯¹äº `OneCycleLR` æˆ– `CyclicLR` ç­‰ï¼Œåˆ™éœ€è¦åœ¨æ¯ä¸ª **batch** åè°ƒç”¨ã€‚
    - **ç‰¹åˆ«æ³¨æ„**ï¼š`ReduceLROnPlateau.step()` éœ€è¦ä¸€ä¸ªæŒ‡æ ‡å€¼ä½œä¸ºå‚æ•°ã€‚
- `get_last_lr()`ï¼šè¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«æ‰€æœ‰å‚æ•°ç»„ï¼ˆparameter groupsï¼‰åœ¨ä¸Šä¸€æ¬¡ `step()` ä¹‹åä½¿ç”¨çš„å­¦ä¹ ç‡ã€‚
- `state_dict()` å’Œ `load_state_dict()`ï¼šç”¨äºä¿å­˜å’ŒåŠ è½½è°ƒåº¦å™¨çš„çŠ¶æ€ï¼Œè¿™å¯¹äºä¸­æ–­åç»§ç»­è®­ç»ƒè‡³å…³é‡è¦ã€‚

### æ€»ç»“ä¸é€‰æ‹©

| è°ƒåº¦å™¨ | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
| :--- | :--- | :--- | :--- |
| **StepLR** / **MultiStepLR** | ç®€å•ï¼Œç›´è§‚ | éœ€è¦é¢„å…ˆå®šä¹‰å¥½è¡°å‡æ—¶æœº | åŸºç¡€ä»»åŠ¡ï¼Œç»éªŒæ€§è°ƒæ•´ |
| **ExponentialLR** | è¡°å‡éå¸¸å¹³æ»‘ | å¯èƒ½è¡°å‡è¿‡å¿« | éœ€è¦æŒç»­å¹³æ»‘è¡°å‡çš„åœºæ™¯ |
| **CosineAnnealingLR** | ç†è®ºæ‰å®ï¼Œæ•ˆæœå¥½ï¼Œå¹³æ»‘ | å‘¨æœŸéœ€è¦é¢„å…ˆè®¾å®š | å›¾åƒåˆ†ç±»ç­‰å¸¸è§ä»»åŠ¡ï¼Œé¦–é€‰ä¹‹ä¸€ |
| **ReduceLROnPlateau** | è‡ªé€‚åº”ï¼Œæ— éœ€é¢„å…ˆè®¾å®š | éœ€è¦éªŒè¯é›†åé¦ˆ | å®ç”¨ä¸»ä¹‰ï¼Œä¸çŸ¥é“ä½•æ—¶è¡°å‡æ—¶å°±ç”¨å®ƒ |
| **OneCycleLR** | è®­ç»ƒå¿«ï¼Œæ€§èƒ½å¥½ | å‚æ•°è¾ƒå¤šï¼Œéœ€è°ƒæ•´ | è¿½æ±‚å¿«é€Ÿæ”¶æ•›å’Œæœ€ä½³æ€§èƒ½ |

**ä¸€èˆ¬å»ºè®®**ï¼š
- ä» `CosineAnnealingLR` æˆ– `ReduceLROnPlateau` å¼€å§‹å°è¯•ã€‚
- å¦‚æœè¿½æ±‚å¿«é€Ÿè®­ç»ƒå’Œéå¸¸å¥½çš„ç»“æœï¼Œå¯ä»¥ç ”ç©¶å¹¶è°ƒè¯• `OneCycleLR`ã€‚
- å§‹ç»ˆä½¿ç”¨ **TensorBoard** æˆ– **Weights & Biases** ç­‰å·¥å…·ç»˜åˆ¶å­¦ä¹ ç‡æ›²çº¿ï¼Œç›´è§‚åœ°äº†è§£è°ƒåº¦å™¨æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚

---

### ğŸ§‘â€ğŸ’» User

å‡å¦‚æˆ‘éœ€è¦è‡ªå·±è®¾è®¡ä¸€ä¸ªå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œè¦æ€ä¹ˆåšï¼Ÿ

---

### ğŸ¤– Assistant

å½“ç„¶ï¼åœ¨PyTorchä¸­è‡ªå·±è®¾è®¡ä¸€ä¸ªå­¦ä¹ ç‡è°ƒåº¦å™¨æ˜¯ä¸€ä¸ªéå¸¸å¸¸è§çš„éœ€æ±‚ï¼Œå› ä¸ºç ”ç©¶æˆ–é¡¹ç›®å¸¸å¸¸éœ€è¦ç‰¹å®šçš„å­¦ä¹ ç‡å˜åŒ–ç­–ç•¥ã€‚PyTorchè®©è¿™ä¸ªè¿‡ç¨‹å˜å¾—éå¸¸ç®€å•å’Œçµæ´»ã€‚

ä½ æœ‰ä¸¤ç§ä¸»è¦çš„æ–¹æ³•æ¥å®ç°è‡ªå®šä¹‰å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼š

1.  **LambdaLR**ï¼šä½¿ç”¨åŒ¿åå‡½æ•°å¿«é€Ÿå®ç°ç®€å•è§„åˆ™ã€‚
2.  **ç»§æ‰¿ `_LRScheduler` åŸºç±»**ï¼šå®ç°åŠŸèƒ½å®Œæ•´ã€å¤æ‚çš„è‡ªå®šä¹‰è°ƒåº¦å™¨ã€‚

---

### æ–¹æ³•ä¸€ï¼šä½¿ç”¨ `LambdaLR` ï¼ˆå¿«é€Ÿç®€å•ï¼‰

`LambdaLR` å…è®¸ä½ é€šè¿‡ä¸€ä¸ªç®€å•çš„Lambdaå‡½æ•°ï¼ˆæˆ–ä»»ä½•å¯è°ƒç”¨å¯¹è±¡ï¼‰æ¥å®šä¹‰å­¦ä¹ ç‡ç›¸å¯¹äºåˆå§‹å­¦ä¹ ç‡çš„å˜åŒ–å€æ•°ã€‚

**æ­¥éª¤ï¼š**
1.  å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè¯¥å‡½æ•°ä»¥å½“å‰ `epoch` ä¸ºè¾“å…¥ï¼Œè¿”å›ä¸€ä¸ª**ä¹˜æ•°å› å­ï¼ˆ multiplier ï¼‰**ã€‚
2.  å°†è¿™ä¸ªå‡½æ•°ä¼ é€’ç»™ `LambdaLR` è°ƒåº¦å™¨ã€‚

**ç¤ºä¾‹1ï¼šå®ç°ä¸€ä¸ªçº¿æ€§è¡°å‡è°ƒåº¦å™¨**

å‡è®¾ä½ æƒ³åœ¨æ¯ä¸ªepochåï¼Œå°†å­¦ä¹ ç‡çº¿æ€§è¡°å‡åˆ°ç¬¬100ä¸ªepochæ—¶ä¸º0ã€‚

```python
import torch.optim as optim
import torch.optim.lr_scheduler as lr_scheduler

optimizer = optim.SGD(model.parameters(), lr=0.1)

# 1. å®šä¹‰lambdaå‡½æ•°
# lr_lambda = lambda epoch: 1. - epoch / 100
def lr_lambda(epoch):
    return 1. - epoch / 100 # ä»1.0çº¿æ€§å‡å°‘åˆ°0.0

# 2. åˆ›å»ºè°ƒåº¦å™¨
scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)

for epoch in range(100):
    train(...)
    validate(...)
    scheduler.step() # æ›´æ–°å­¦ä¹ ç‡
    current_lr = scheduler.get_last_lr()[0]
    print(f'Epoch: {epoch}, Learning Rate: {current_lr}')
```

**ç¤ºä¾‹2ï¼šå®ç°ä¸€ä¸ªå¸¦çƒ­é‡å¯çš„ä½™å¼¦é€€ç«ï¼ˆSGDRçš„ç®€åŒ–ç‰ˆï¼‰**

```python
import math

def cosine_annealing_with_warm_restart(epoch, T_0=10, T_mult=2, eta_min=0):
    """
    T_0: ç¬¬ä¸€æ¬¡é‡å¯çš„å‘¨æœŸ
    T_mult: é‡å¯åå‘¨æœŸå˜å¤§çš„å€æ•°ï¼ˆé€šå¸¸>=1ï¼‰
    eta_min: æœ€å°å­¦ä¹ ç‡
    """
    # è®¡ç®—å½“å‰æ˜¯ç¬¬å‡ ä¸ªé‡å¯å‘¨æœŸ
    T_cur = epoch
    T_i = T_0
    while T_cur >= T_i:
        T_cur -= T_i
        T_i *= T_mult
    # ä½™å¼¦å…¬å¼ï¼Œè¿”å›å€¼æ˜¯ä¹˜æ•°å› å­
    return eta_min + 0.5 * (1 - eta_min) * (1 + math.cos(math.pi * T_cur / T_i))

optimizer = optim.SGD(model.parameters(), lr=0.1)
# ä½¿ç”¨lambdaåŒ…è£…ä¸€ä¸‹ï¼Œå› ä¸ºlr_lambdaå‡½æ•°åªæ¥æ”¶ä¸€ä¸ªå‚æ•°epoch
scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda e: cosine_annealing_with_warm_restart(e, T_0=10))
```

**ä¼˜ç‚¹**ï¼šå¿«é€Ÿã€ä»£ç ç®€æ´ã€‚
**ç¼ºç‚¹**ï¼šåŠŸèƒ½å—é™ï¼Œéš¾ä»¥å®ç°ä¾èµ–äºä¸Šä¸€æ­¥çŠ¶æ€ï¼ˆå¦‚ä¸Šä¸€æ­¥çš„å­¦ä¹ ç‡ï¼‰çš„å¤æ‚é€»è¾‘ã€‚

---

### æ–¹æ³•äºŒï¼šç»§æ‰¿ `_LRScheduler` åŸºç±»ï¼ˆåŠŸèƒ½å®Œæ•´ï¼‰

è¿™æ˜¯æœ€å¼ºå¤§å’Œæ¨èçš„æ–¹æ³•ã€‚ä½ éœ€è¦åˆ›å»ºä¸€ä¸ªç»§æ‰¿è‡ª `torch.optim.lr_scheduler._LRScheduler` çš„ç±»ã€‚

**æ ¸å¿ƒæ­¥éª¤ï¼š**
1.  **ç»§æ‰¿ `_LRScheduler`**ã€‚
2.  **å®ç° `__init__` æ–¹æ³•**ï¼šè°ƒç”¨çˆ¶ç±»çš„ `__init__` æ–¹æ³•ï¼Œå¹¶åˆå§‹åŒ–ä½ è‡ªå·±éœ€è¦çš„å‚æ•°ã€‚
3.  **é‡å†™ `get_lr` æ–¹æ³•**ï¼šè¿™æ˜¯æ ¸å¿ƒï¼è¯¥æ–¹æ³•è¿”å›ä¸€ä¸ª**åˆ—è¡¨**ï¼ŒåŒ…å«æ‰€æœ‰å‚æ•°ç»„ï¼ˆparameter groupsï¼‰**ä¸‹ä¸€æ­¥**åº”è¯¥ä½¿ç”¨çš„å­¦ä¹ ç‡ã€‚ä½ åœ¨è¿™é‡Œå®ç°ä½ çš„è®¡ç®—é€»è¾‘ã€‚**æ³¨æ„ï¼š** å®ƒè®¡ç®—çš„æ˜¯å­¦ä¹ ç‡å€¼ï¼Œè€Œä¸æ˜¯ä¹˜æ•°å› å­ã€‚

**ç¤ºä¾‹ï¼šå®ç°ä¸€ä¸ªæŒ‡æ•°è¡°å‡è°ƒåº¦å™¨ï¼ˆ`ExponentialLR` çš„æ‰‹åŠ¨å®ç°ï¼‰**

```python
from torch.optim.lr_scheduler import _LRScheduler

class MyExponentialLR(_LRScheduler):
    def __init__(self, optimizer, gamma, last_epoch=-1, verbose=False):
        # åˆå§‹åŒ–è‡ªå®šä¹‰å‚æ•°
        self.gamma = gamma
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–æ–¹æ³•ï¼Œå¿…é¡»åšï¼
        super().__init__(optimizer, last_epoch, verbose)

    def get_lr(self):
        # å¦‚æœ last_epoch = -1, è¯´æ˜æ˜¯åˆå§‹é˜¶æ®µï¼Œè¿”å›åˆå§‹å­¦ä¹ ç‡
        if self.last_epoch == -1:
            return [group['lr'] for group in self.optimizer.param_groups]
        
        # æ ¸å¿ƒè®¡ç®—ï¼šæ–°çš„å­¦ä¹ ç‡ = åˆå§‹å­¦ä¹ ç‡ * (gamma ^ last_epoch)
        # æ³¨æ„ï¼šself.base_lrs æ˜¯çˆ¶ç±»å±æ€§ï¼Œä¿å­˜äº†æ¯ä¸ªå‚æ•°ç»„çš„åˆå§‹å­¦ä¹ ç‡
        return [base_lr * (self.gamma ** self.last_epoch)
                for base_lr in self.base_lrs]

# ä½¿ç”¨è‡ªå®šä¹‰è°ƒåº¦å™¨
optimizer = optim.SGD(model.parameters(), lr=0.1)
scheduler = MyExponentialLR(optimizer, gamma=0.95)

for epoch in range(100):
    train(...)
    validate(...)
    scheduler.step()
```

**ç¤ºä¾‹ï¼šå®ç°ä¸€ä¸ªæ›´å¤æ‚çš„è‡ªå®šä¹‰è°ƒåº¦å™¨ï¼ˆä¾‹å¦‚ï¼Œå‰10ä¸ªepoché¢„çƒ­ï¼Œä¹‹åä½™å¼¦è¡°å‡ï¼‰**

```python
import math
from torch.optim.lr_scheduler import _LRScheduler

class WarmupCosineLR(_LRScheduler):
    def __init__(self, optimizer, warmup_epochs, max_epochs, eta_min=0, last_epoch=-1, verbose=False):
        self.warmup_epochs = warmup_epochs
        self.max_epochs = max_epochs
        self.eta_min = eta_min
        super().__init__(optimizer, last_epoch, verbose)

    def get_lr(self):
        if self.last_epoch < self.warmup_epochs:
            # é¢„çƒ­é˜¶æ®µï¼šçº¿æ€§å¢åŠ å­¦ä¹ ç‡
            return [base_lr * (self.last_epoch / self.warmup_epochs) for base_lr in self.base_lrs]
        else:
            # ä½™å¼¦è¡°å‡é˜¶æ®µ
            current_epoch = self.last_epoch - self.warmup_epochs
            total_cosine_epochs = self.max_epochs - self.warmup_epochs
            # ä½™å¼¦å…¬å¼
            return [self.eta_min + (base_lr - self.eta_min) * 
                    (1 + math.cos(math.pi * current_epoch / total_cosine_epochs)) / 2
                    for base_lr in self.base_lrs]

# ä½¿ç”¨
optimizer = optim.SGD(model.parameters(), lr=0.1)
scheduler = WarmupCosineLR(optimizer, warmup_epochs=5, max_epochs=100, eta_min=1e-5)
```

**ä¼˜ç‚¹**ï¼š
*   **åŠŸèƒ½å®Œæ•´**ï¼šå¯ä»¥å®ç°ä»»ä½•ä½ èƒ½æƒ³åˆ°çš„é€»è¾‘ã€‚
*   **çŠ¶æ€ç®¡ç†**ï¼šåŸºç±»å¸®ä½ å¤„ç†äº† `state_dict` å’Œ `load_state_dict`ï¼Œç¡®ä¿æ–­ç‚¹ç»­è®­æ­£å¸¸å·¥ä½œã€‚
*   **æ ‡å‡†æ¥å£**ï¼šå’Œä½¿ç”¨å†…ç½®è°ƒåº¦å™¨ä¸€æ¨¡ä¸€æ ·ï¼ˆ`.step()`, `.get_last_lr()`ï¼‰ã€‚

**å…³é”®ç‚¹**ï¼š
*   å§‹ç»ˆè®°å¾—åœ¨ `__init__` ä¸­è°ƒç”¨ `super().__init__(optimizer, last_epoch, verbose)`ã€‚
*   `self.base_lrs` æ˜¯åˆå§‹å­¦ä¹ ç‡åˆ—è¡¨ã€‚
*   `self.last_epoch` è®°å½•å·²ç»è¿›è¡Œäº†å¤šå°‘ä¸ª `step()`ï¼ˆé€šå¸¸æ˜¯epochæ•°ï¼‰ã€‚
*   `get_lr()` è¿”å›çš„æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œé•¿åº¦ç­‰äº `optimizer.param_groups` çš„æ•°é‡ã€‚

### æ€»ç»“ä¸å»ºè®®

| æ–¹æ³• | é€‚ç”¨åœºæ™¯ | éš¾åº¦ |
| :--- | :--- | :--- |
| **`LambdaLR`** | ç®€å•çš„ã€æ— çŠ¶æ€çš„ã€åŸºäºepochæ•°çš„è§„åˆ™ | ä½ |
| **ç»§æ‰¿ `_LRScheduler`** | å¤æ‚çš„ã€æœ‰çŠ¶æ€çš„ã€è‡ªé€‚åº”çš„è§„åˆ™ | ä¸­åˆ°é«˜ |

*   å¯¹äºç»å¤§å¤šæ•°éœ€æ±‚ï¼Œ**æ–¹æ³•äºŒï¼ˆç»§æ‰¿ `_LRScheduler`ï¼‰** æ˜¯æ›´ä¸“ä¸šå’Œå¯é çš„é€‰æ‹©ã€‚
*   åœ¨è®¾è®¡æ—¶ï¼Œ**åŠ¡å¿…ä½¿ç”¨ TensorBoard æˆ– matplotlib ç»˜åˆ¶å‡ºä½ çš„å­¦ä¹ ç‡å˜åŒ–æ›²çº¿**ï¼Œä»¥ç¡®ä¿å®ƒç¬¦åˆä½ çš„é¢„æœŸã€‚
*   å‚è€ƒPyTorchå®˜æ–¹æºç ä¸­å…¶ä»–è°ƒåº¦å™¨ï¼ˆå¦‚ `CosineAnnealingLR`ï¼‰çš„å®ç°ï¼Œè¿™æ˜¯æœ€å¥½çš„å­¦ä¹ ææ–™ã€‚