# LLM预训练数据集获取方法

### 🧑‍💻 User

llm预训练阶段的数据集如何获取？

---

### 🤖 Assistant

LLM（大型语言模型）预训练阶段的数据获取是构建模型最关键的基础步骤之一。数据的质量、多样性和规模直接决定了模型的最终能力。

获取预训练数据集通常有以下几种主要途径，涵盖了开源社区、公开网页抓取以及特定领域的收集：

### 1. 开源数据集平台 (最直接的方式)
对于大多数研究者和开发者来说，直接使用经过处理的开源数据集是最便捷的途径。这些数据集通常已经经过了清洗、去重和格式化。

*   **Hugging Face Datasets:**
    *   这是目前最大的NLP数据集中心。你可以在这里找到几乎所有主流的预训练数据集。
    *   **典型数据集:** `Common Crawl`, `C4` (Colossal Clean Crawled Corpus), `The Pile`, `Wikipedia` 等。
*   **EleutherAI (The Pile):**
    *   这是一个专门为训练大模型设计的开源数据集，包含了22个不同的子数据集，涵盖学术论文、代码、法律文件、网页等。
*   **RedPajama:**
    *   由Together AI发布，旨在复现LLaMA训练数据的开源版本，质量非常高。
*   **中文数据集:**
    *   **WuDaoCorpora (悟道):** 北京智源人工智能研究院发布的超大规模中文语料库。
    *   **CLUECorpus:** 来源于CLUE基准测评的中文语料库。
    *   **WanJuan (万卷):** 上海人工智能实验室发布的书生·万卷多模态语料库。

### 2. 通用网页抓取 (Web Crawling)
这是获取海量数据的主要来源，通常用于让模型学习通用的语言模式和世界知识。

*   **Common Crawl:**
    *   **描述:** 一个非营利组织，定期抓取整个互联网并免费提供PB级别的数据快照。
    *   **处理:** 原始的Common Crawl数据非常“脏”（包含广告、乱码、导航栏等），必须经过复杂的清洗管道（如CCNet、RefinedWeb流程）才能使用。
*   **Wudao / Baidu (针对中文):**
    *   对于中文模型，通常会爬取百度百科、知乎、微信公众号文章（搜狗入口）等公开内容。

### 3. 代码数据 (Code)
为了赋予模型逻辑推理能力和编程能力，代码数据必不可少。

*   **GitHub Public Dataset:**
    *   可以通过Google BigQuery访问GitHub的公共数据集。
*   **The Stack:**
    *   由BigCode项目发布，是一个大规模的、经过许可过滤的代码数据集，涵盖数百种编程语言。
*   **Stack Overflow:**
    *   利用其问答数据，模型可以学习代码的上下文解释和Bug修复。

### 4. 书籍与文献 (Books & Papers)
书籍数据通常具有长文本依赖和连贯的叙事逻辑，有助于提高模型的长上下文理解能力。

*   **Project Gutenberg:** 提供大量版权过期的经典文学作品。
*   **BookCorpus:** 包含了大量免费小说（常被用于BERT等早期模型）。
*   **ArXiv:** 包含物理、数学、计算机科学等领域的学术论文，通常包含大量的 $\LaTeX$ 公式，有助于提升科学推理能力。
*   **PubMed:** 生物医学领域的论文库。

### 5. 社交媒体与对话数据
用于让模型学习口语化表达、对话逻辑和即时信息。

*   **Reddit (OpenWebText):**
    *   通常选取Reddit上Upvote（点赞）数较高的帖子链接的内容，代表了相对高质量的讨论。
*   **Twitter / X (目前获取难度大):** 早期曾用于训练，但现在API限制较多。

### 数据获取后的处理流程 (至关重要)

仅仅“获取”数据是不够的，预训练数据的**清洗（Data Cleaning）**往往比获取更耗时：

1.  **质量过滤 (Quality Filtering):** 使用启发式规则（如文本长度、标点符号密度、特殊字符比例）剔除低质量文本。
2.  **去重 (Deduplication):**
    *   **精确去重:** 删除完全一样的字符串。
    *   **模糊去重 (Fuzzy Deduplication):** 使用 MinHash + LSH (Locality Sensitive Hashing) 技术去除高度相似的文档，防止模型死记硬背。
3.  **隐私消除 (PII Removal):** 移除所有的个人身份信息（邮箱、电话、IP地址等）。
4.  **去毒 (De-toxification):** 过滤掉仇恨言论、色情、暴力等有害内容。

### 法律与合规性提示

在自行收集数据时，必须注意以下几点：
*   **robots.txt:** 遵守网站的爬虫协议。
*   **版权 (Copyright):** 如纽约时报起诉OpenAI案例所示，使用受版权保护的新闻、书籍进行训练存在法律灰色地带。目前趋势是尽可能使用Public Domain或授权数据。
*   **GDPR:** 涉及欧盟用户数据时需极其谨慎。

### 总结建议

如果你是**个人开发者或小型团队**，建议直接使用 Hugging Face 上托管的成熟数据集（如 **RefinedWeb**, **RedPajama**, **SlimPajama**）。自行爬取和清洗Common Crawl级别的互联网数据需要巨大的工程资源和算力支持。