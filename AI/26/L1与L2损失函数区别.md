# L1ä¸ŽL2æŸå¤±å‡½æ•°åŒºåˆ«

### ðŸ§‘â€ðŸ’» User

æœºå™¨å­¦ä¹ ä¸­çš„l1æŸå¤±å‡½æ•°å’Œl2æŸå¤±å‡½æ•°æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿå¦‚ä½•æ ¹æ®ä»»åŠ¡ç±»åž‹é€‰æ‹©åˆé€‚çš„loss funcï¼Ÿ

---

### ðŸ¤– Assistant

åœ¨æœºå™¨å­¦ä¹ ï¼ˆä¸»è¦æ˜¯å›žå½’ä»»åŠ¡ï¼‰ä¸­ï¼Œ**L1 æŸå¤±å‡½æ•°**ï¼ˆé€šå¸¸å¯¹åº”å¹³å‡ç»å¯¹è¯¯å·®ï¼ŒMAEï¼‰å’Œ **L2 æŸå¤±å‡½æ•°**ï¼ˆé€šå¸¸å¯¹åº”å‡æ–¹è¯¯å·®ï¼ŒMSEï¼‰æ˜¯ä¸¤ä¸ªæœ€åŸºç¡€ä¸”æœ€é‡è¦çš„æ¦‚å¿µã€‚

ä»¥ä¸‹æ˜¯å®ƒä»¬çš„è¯¦ç»†å¯¹æ¯”ã€æ•°å­¦åŽŸç†ä»¥åŠå¦‚ä½•æ ¹æ®ä»»åŠ¡è¿›è¡Œé€‰æ‹©çš„æŒ‡å—ã€‚

---

### 1. å®šä¹‰ä¸Žå…¬å¼

å‡è®¾ $y$ æ˜¯çœŸå®žå€¼ï¼Œ$f(x)$ æ˜¯é¢„æµ‹å€¼ï¼Œé‚£ä¹ˆï¼š

*   **L1 Loss (Mean Absolute Error - MAE)**
    è®¡ç®—é¢„æµ‹å€¼ä¸ŽçœŸå®žå€¼ä¹‹é—´**å·®å€¼çš„ç»å¯¹å€¼**ä¹‹å’Œã€‚
    $$ L_1 = \sum_{i=1}^{n} |y_i - f(x_i)| $$

*   **L2 Loss (Mean Squared Error - MSE)**
    è®¡ç®—é¢„æµ‹å€¼ä¸ŽçœŸå®žå€¼ä¹‹é—´**å·®å€¼çš„å¹³æ–¹**ä¹‹å’Œã€‚
    $$ L_2 = \sum_{i=1}^{n} (y_i - f(x_i))^2 $$

---

### 2. æ ¸å¿ƒåŒºåˆ« (è¯¦ç»†å¯¹æ¯”)

| ç‰¹æ€§ | L1 Loss (MAE) | L2 Loss (MSE) |
| :--- | :--- | :--- |
| **å¯¹å¼‚å¸¸å€¼çš„æ•æ„Ÿåº¦** | **é²æ£’æ€§å¼º (Robust)**ã€‚å¼‚å¸¸å€¼å¸¦æ¥çš„æ¢¯åº¦æ˜¯æ’å®šçš„ï¼Œä¸ä¼šä¸»å¯¼æ•´ä¸ªæŸå¤±ã€‚ | **éžå¸¸æ•æ„Ÿ**ã€‚è¯¯å·®è¢«å¹³æ–¹æ”¾å¤§ï¼Œæ¨¡åž‹ä¼šä¸ºäº†â€œè¿å°±â€ä¸€ä¸ªå·¨å¤§çš„å¼‚å¸¸å€¼è€Œç‰ºç‰²æ•´ä½“æ€§èƒ½ã€‚ |
| **æ¢¯åº¦çš„ç¨³å®šæ€§** | **æ¢¯åº¦æ’å®š**ã€‚æ— è®ºè¯¯å·®å¤šå¤§ï¼Œæ¢¯åº¦è¦ä¹ˆæ˜¯ 1 è¦ä¹ˆæ˜¯ -1ï¼ˆé™¤äº† 0 ç‚¹ï¼‰ã€‚ | **æ¢¯åº¦éšè¯¯å·®å˜åŒ–**ã€‚è¯¯å·®è¶Šå¤§æ¢¯åº¦è¶Šå¤§ï¼›è¯¯å·®è¶Šå°æ¢¯åº¦è¶Šå°ã€‚ |
| **é›¶ç‚¹å¤„çš„å¯¼æ•°** | **ä¸å¯å¯¼** (åœ¨ $x=0$ å¤„)ã€‚éœ€è¦ä½¿ç”¨æ¬¡æ¢¯åº¦æˆ–å…¶ä»–å¹³æ»‘æ‰‹æ®µå¤„ç†ã€‚ | **å¤„å¤„å¯å¯¼**ã€‚ä¸”åœ¨æŽ¥è¿‘ 0 æ—¶æ¢¯åº¦é€æ¸å‡å°ï¼Œåˆ©äºŽæ”¶æ•›ã€‚ |
| **è§£çš„ç¨€ç–æ€§** | å€¾å‘äºŽäº§ç”Ÿ**ç¨€ç–è§£** (Sparse Solution)ï¼Œå³è®¸å¤šå‚æ•°å˜ä¸º 0ï¼ˆå¸¸ç”¨äºŽç‰¹å¾é€‰æ‹©ï¼‰ã€‚ | å€¾å‘äºŽäº§ç”Ÿ**å¹³æ»‘è§£**ï¼Œå‚æ•°é€šå¸¸æŽ¥è¿‘ 0 ä½†ä¸ç­‰äºŽ 0ã€‚ |
| **è§£çš„æ€§è´¨** | å¯¹åº”äºŽ**ä¸­ä½æ•°** (Median) ä¼°è®¡ã€‚ | å¯¹åº”äºŽ**å‡å€¼** (Mean) ä¼°è®¡ã€‚ |

#### æ·±å…¥ç†è§£ï¼šä¸ºä»€ä¹ˆ L2 å¯¹å¼‚å¸¸å€¼æ•æ„Ÿï¼Ÿ
å‡è®¾å¤§éƒ¨åˆ†æ•°æ®çš„è¯¯å·®æ˜¯ $1$ï¼Œæœ‰ä¸€ä¸ªå¼‚å¸¸å€¼çš„è¯¯å·®æ˜¯ $10$ã€‚
*   **L1:**  $Loss \propto 10$ã€‚æ¨¡åž‹è§‰å¾—è¿™ä¸ªé”™è¯¯æ˜¯æ™®é€šé”™è¯¯çš„ 10 å€ã€‚
*   **L2:** $Loss \propto 10^2 = 100$ã€‚æ¨¡åž‹è§‰å¾—è¿™ä¸ªé”™è¯¯æ˜¯æ™®é€šé”™è¯¯çš„ 100 å€ï¼
**ç»“æžœï¼š** ä½¿ç”¨ L2 Loss æ—¶ï¼Œæ¨¡åž‹ä¼šæžåŠ›åŽ»ä¿®æ­£è¿™ä¸ªå¼‚å¸¸å€¼ï¼Œå¯¼è‡´æ­£å¸¸æ•°æ®çš„é¢„æµ‹è¢«â€œæ‹‰åâ€ã€‚

#### æ·±å…¥ç†è§£ï¼šæ”¶æ•›é—®é¢˜
*   **L2:** å½“æ¨¡åž‹æŽ¥è¿‘æ”¶æ•›ï¼ˆè¯¯å·®æŽ¥è¿‘ 0ï¼‰æ—¶ï¼Œæ¢¯åº¦ä¹Ÿä¼šå˜å°ï¼Œè¿™ä½¿å¾—æ¨¡åž‹èƒ½æ…¢æ…¢åœ°ã€ç²¾ç¡®åœ°æ”¶æ•›åˆ°æœ€å°å€¼ã€‚
*   **L1:** å³ä½¿è¯¯å·®å¾ˆå°ï¼Œæ¢¯åº¦ä¾ç„¶ä¿æŒä¸º 1 æˆ– -1ã€‚å¦‚æžœä¸éšç€è®­ç»ƒé™ä½Žå­¦ä¹ çŽ‡ï¼ˆLearning Rate Decayï¼‰ï¼Œæ¨¡åž‹å¯èƒ½ä¼šåœ¨æœ€å°å€¼é™„è¿‘éœ‡è¡ï¼Œéš¾ä»¥ç²¾ç¡®æ”¶æ•›ã€‚

---

### 3. å¦‚ä½•æ ¹æ®ä»»åŠ¡ç±»åž‹é€‰æ‹© Loss Functionï¼Ÿ

é€‰æ‹©çš„æ ¸å¿ƒåœ¨äºŽä½ **å¦‚ä½•çœ‹å¾…æ•°æ®ä¸­çš„å¼‚å¸¸å€¼ (Outliers)**ã€‚

#### åœºæ™¯ Aï¼šå¼‚å¸¸å€¼æ˜¯â€œå™ªéŸ³â€æˆ–â€œé”™è¯¯æ•°æ®â€ (é€‰æ‹© L1)
å¦‚æžœä½ çš„æ•°æ®ä¸­åŒ…å«ç”±äºŽä¼ æ„Ÿå™¨æ•…éšœã€å½•å…¥é”™è¯¯ç­‰äº§ç”Ÿçš„ç¦»ç¾¤ç‚¹ï¼Œä¸”ä½ ä¸å¸Œæœ›æ¨¡åž‹è¢«è¿™äº›é”™è¯¯æ•°æ®å¸¦åã€‚
*   **é€‰æ‹©ï¼š** **L1 Loss (MAE)**
*   **ç†ç”±ï¼š** L1 å¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿï¼Œæ¨¡åž‹ä¸»è¦æ‹Ÿåˆå¤§éƒ¨åˆ†æ­£å¸¸æ•°æ®ï¼ˆä¸­ä½æ•°å›žå½’ï¼‰ã€‚

#### åœºæ™¯ Bï¼šå¼‚å¸¸å€¼æ˜¯â€œå¿…é¡»å…³æ³¨çš„ä¸¥é‡é”™è¯¯â€ (é€‰æ‹© L2)
å¦‚æžœä½ ä¸èƒ½å®¹å¿å¤§çš„è¯¯å·®ï¼Œæˆ–è€…æ•°æ®éžå¸¸å¹²å‡€ï¼ˆæ²¡æœ‰å™ªéŸ³ï¼‰ã€‚ä¾‹å¦‚åœ¨è‡ªåŠ¨é©¾é©¶é¢„æµ‹åˆ¹è½¦è·ç¦»æ—¶ï¼Œé¢„æµ‹åå·® 1 ç±³å’Œåå·® 10 ç±³çš„åŽæžœå®Œå…¨ä¸åŒï¼Œåå·® 10 ç±³æ˜¯ä¸å¯æŽ¥å—çš„ã€‚
*   **é€‰æ‹©ï¼š** **L2 Loss (MSE)**
*   **ç†ç”±ï¼š** L2 ä¼šå¯¹å¤§è¯¯å·®è¿›è¡Œå¹³æ–¹æƒ©ç½šï¼Œè¿«ä½¿æ¨¡åž‹å°½å…¨åŠ›æ¶ˆé™¤å¤§çš„åå·®ã€‚æ­¤å¤–ï¼ŒL2 å¤„å¤„å¯å¯¼ï¼Œè®­ç»ƒé€šå¸¸æ›´ç¨³å®šã€æ”¶æ•›æ›´å¿«ã€‚

#### åœºæ™¯ Cï¼šæŠ˜ä¸­æ–¹æ¡ˆ (Smooth L1 Loss æˆ– Huber Loss)
å¦‚æžœä½ æ—¢æƒ³è¦ L1 å¯¹å¼‚å¸¸å€¼çš„é²æ£’æ€§ï¼Œåˆæƒ³è¦ L2 åœ¨ 0 ç‚¹é™„è¿‘çš„å¹³æ»‘æ”¶æ•›æ€§ï¼ˆä»¥åŠå¯å¯¼æ€§ï¼‰ã€‚
*   **é€‰æ‹©ï¼š** **Huber Loss** æˆ– **Smooth L1 Loss**
*   **å®šä¹‰ï¼š**
    $$
    L_{\delta}(y, f(x)) =
    \begin{cases}
    \frac{1}{2}(y - f(x))^2 & \text{for } |y - f(x)| \le \delta \\
    \delta |y - f(x)| - \frac{1}{2}\delta^2 & \text{otherwise}
    \end{cases}
    $$
*   **è§£é‡Šï¼š** å½“è¯¯å·®å¾ˆå°ï¼ˆå°äºŽ $\delta$ï¼‰æ—¶ï¼Œå®ƒæ˜¯ L2ï¼ˆå¹³æ–¹ï¼Œæ˜“æ”¶æ•›ï¼‰ï¼›å½“è¯¯å·®å¾ˆå¤§æ—¶ï¼Œå®ƒæ˜¯ L1ï¼ˆçº¿æ€§ï¼ŒæŠ—å¹²æ‰°ï¼‰ã€‚ç›®å‰åœ¨ç›®æ ‡æ£€æµ‹ï¼ˆå¦‚ Faster R-CNN, YOLOï¼‰å›žå½’æ¡†é¢„æµ‹ä¸­éžå¸¸å¸¸ç”¨ã€‚

### 4. æ€»ç»“ä¸Žé€ŸæŸ¥è¡¨

1.  **å¤§éƒ¨åˆ†å›žå½’ä»»åŠ¡çš„é»˜è®¤é€‰æ‹©ï¼š** å…ˆè¯• **L2 (MSE)**ã€‚å› ä¸ºå®ƒä¼˜åŒ–èµ·æ¥æœ€æ–¹ä¾¿ï¼Œæ”¶æ•›æœ€å¿«ã€‚
2.  **æ•°æ®å¦‚æžœä¸å¹²å‡€ (Dirty Data)ï¼š** å°è¯• **L1 (MAE)**ã€‚
3.  **ç›®æ ‡æ£€æµ‹/é«˜ç²¾åº¦å›žå½’ï¼š** ä½¿ç”¨ **Smooth L1** æˆ– **Huber Loss**ã€‚
4.  **ä¸šåŠ¡éœ€æ±‚ï¼š** å¦‚æžœä¸šåŠ¡KPIæ˜¯â€œå¹³å‡è¯¯å·®â€ï¼Œé€‰ L1ï¼›å¦‚æžœæ˜¯â€œé¿å…å¤§äº‹æ•…â€ï¼Œé€‰ L2ã€‚

---

### ðŸ§‘â€ðŸ’» User

ä¸¾ä¾‹è¯´è¯´ï¼Œä¼˜åŒ–å™¨å’Œè¿™äº›æŸå¤±å‡½æ•°ä¹‹é—´æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ

---

### ðŸ¤– Assistant

è¿™æ˜¯ä¸€ä¸ªéžå¸¸æ·±åˆ»çš„é—®é¢˜ã€‚ç®€å•æ¥è¯´ï¼Œ**æŸå¤±å‡½æ•°å†³å®šäº†â€œåœ°å½¢çš„å½¢çŠ¶â€ï¼ˆå±±è°·åœ¨å“ªé‡Œï¼‰ï¼Œè€Œä¼˜åŒ–å™¨å†³å®šäº†â€œä¸‹å±±çš„æ­¥ä¼å’Œç­–ç•¥â€ï¼ˆæ€Žä¹ˆèµ°æ‰å¿«ä¸”ç¨³ï¼‰ã€‚**

å®ƒä»¬ä¹‹é—´çš„æ ¸å¿ƒè”ç³»åœ¨äºŽ **æ¢¯åº¦ï¼ˆGradientï¼‰**ã€‚

*   **æŸå¤±å‡½æ•° ($L$)** è´Ÿè´£è®¡ç®—æ¢¯åº¦ï¼š$\nabla L$ã€‚å®ƒå‘Šè¯‰æˆ‘ä»¬åœ¨å½“å‰ä½ç½®ï¼Œå“ªä¸ªæ–¹å‘æ˜¯â€œä¸‹å¡â€çš„æ–¹å‘ï¼Œä»¥åŠå¡åº¦æœ‰å¤šé™¡ã€‚
*   **ä¼˜åŒ–å™¨ (Optimizer)** è´Ÿè´£åˆ©ç”¨æ¢¯åº¦æ›´æ–°å‚æ•°ï¼š$\theta_{new} = \theta_{old} - \text{Optimizer}(\nabla L, \dots)$ã€‚å®ƒå†³å®šäº†å¾€é‚£ä¸ªæ–¹å‘èµ°å¤šè¿œï¼ˆå­¦ä¹ çŽ‡ï¼‰ï¼Œæ˜¯å¦è¦åŠ é€Ÿï¼ˆåŠ¨é‡ï¼‰ï¼Œæˆ–è€…æ˜¯å¦è¦æ ¹æ®åœ°å½¢è°ƒæ•´æ­¥ä¼ï¼ˆè‡ªé€‚åº”ï¼‰ã€‚

ä¸‹é¢æˆ‘é€šè¿‡å…·ä½“çš„ä¾‹å­æ¥åˆ†æž L1 å’Œ L2 æŸå¤±å‡½æ•°ä¸Žä¼˜åŒ–å™¨ï¼ˆå¦‚ SGDã€Adamï¼‰åœ¨â€œé…åˆâ€æ—¶ä¼šå‡ºçŽ°çš„çŽ°è±¡ï¼š

---

### ä¾‹å­ 1ï¼šL1 Loss (MAE) + æœ´ç´  SGD çš„â€œéœ‡è¡ä¹‹èˆžâ€

å‡è®¾æˆ‘ä»¬ä½¿ç”¨ **L1 Loss**ï¼Œä¼˜åŒ–å™¨ä½¿ç”¨æœ€ç®€å•çš„ **SGD**ï¼ˆéšæœºæ¢¯åº¦ä¸‹é™ï¼Œä¸”å­¦ä¹ çŽ‡ $\eta$ å›ºå®šï¼‰ã€‚

#### çŽ°è±¡æè¿°
å½“æ¨¡åž‹è®­ç»ƒæŽ¥è¿‘ç»ˆç‚¹ï¼ˆæœ€ä¼˜è§£ï¼Œå³è¯¯å·®ä¸º 0ï¼‰æ—¶ï¼Œä½ å¯èƒ½ä¼šå‘çŽ° loss **æ— æ³•ç¨³å®šä¸‹é™ï¼Œè€Œæ˜¯åœ¨æœ€å°å€¼é™„è¿‘ç–¯ç‹‚éœ‡è¡**ã€‚

#### åŽŸç†è§£æž
*   **L1 çš„æ¢¯åº¦ç‰¹æ€§ï¼š** L1 Loss çš„å¯¼æ•°ï¼ˆæ¢¯åº¦ï¼‰æ˜¯å¸¸æ•°ã€‚
    $$ \frac{\partial |x|}{\partial x} = \begin{cases} 1 & x > 0 \\ -1 & x < 0 \end{cases} $$
    ä¸ç®¡ä½ çš„è¯¯å·®æ˜¯ 10000 è¿˜æ˜¯ 0.0001ï¼ŒL1 ç»™å‡ºçš„æ¢¯åº¦å¤§å°æ°¸è¿œæ˜¯ $1$ï¼ˆæˆ–è€…æ˜¯ $-1$ï¼‰ã€‚
*   **ä¼˜åŒ–å™¨çš„å›°å¢ƒï¼š** å¦‚æžœä½¿ç”¨å›ºå®šå­¦ä¹ çŽ‡çš„ SGDï¼Œæ›´æ–°å…¬å¼ä¸º $\theta = \theta - \eta \cdot (\pm 1)$ã€‚
    å½“æ¨¡åž‹ç¦»ç»ˆç‚¹å¾ˆè¿‘ï¼ˆæ¯”å¦‚è·ç¦»åªæœ‰ 0.001ï¼‰ï¼Œä½†ä½ çš„å­¦ä¹ çŽ‡ $\eta$ æ˜¯ 0.01ã€‚
    1.  ä½ å‘å·¦èµ° 0.01ï¼ˆè·¨è¿‡äº†ç»ˆç‚¹ï¼Œè¯¯å·®å˜æˆ -0.009ï¼‰ã€‚
    2.  ä¸‹ä¸€æ­¥æ¢¯åº¦åå‘ï¼Œä½ å‘å³èµ° 0.01ï¼ˆåˆè·¨è¿‡äº†ï¼Œè¯¯å·®å˜æˆ 0.001ï¼‰ã€‚
    3.  **ç»“æžœï¼š** æ¨¡åž‹æ°¸è¿œåœ¨ç»ˆç‚¹å·¦å³æ¨ªè·³ï¼Œæ— æ³•æ”¶æ•›ã€‚

#### è§£å†³æ–¹æ¡ˆï¼ˆä¼˜åŒ–å™¨çš„ä½œç”¨ï¼‰
å¦‚æžœä½ å¿…é¡»ç”¨ L1 Lossï¼Œä½ éœ€è¦æ›´èªæ˜Žçš„ä¼˜åŒ–å™¨ç­–ç•¥ï¼š
1.  **Learning Rate Decay (å­¦ä¹ çŽ‡è¡°å‡)ï¼š** éšç€è®­ç»ƒè¿›è¡Œï¼Œå¼ºåˆ¶å‡å° SGD çš„å­¦ä¹ çŽ‡ï¼Œè®©æ­¥ä¼å˜å°ï¼Œæ‰èƒ½èµ°è¿› L1 é‚£ä¸ªç‹­çª„çš„è°·åº•ã€‚
2.  **ä½¿ç”¨ Adam ç­‰è‡ªé€‚åº”ä¼˜åŒ–å™¨ï¼š** Adam ä¼šæ ¹æ®æ¢¯åº¦çš„åŽ†å²å‡å€¼æ¥è°ƒæ•´æ­¥é•¿ï¼Œè™½ç„¶ L1 æ¢¯åº¦æ’å®šï¼Œä½† Adam çš„äºŒé˜¶çŸ©ä¼°è®¡èƒ½å¸®åŠ©ç¨³å®šæ›´æ–°å¹…åº¦ã€‚

---

### ä¾‹å­ 2ï¼šL2 Loss (MSE) + SGD çš„â€œç”±å¿«åˆ°æ…¢â€

å‡è®¾æˆ‘ä»¬ä½¿ç”¨ **L2 Loss**ï¼Œé…åˆ **SGD**ã€‚

#### çŽ°è±¡æè¿°
è®­ç»ƒåˆšå¼€å§‹æ—¶é€Ÿåº¦æžå¿«ï¼ŒLoss è¿…é€Ÿä¸‹é™ï¼›ä½†éšç€è®­ç»ƒæ·±å…¥ï¼Œæ”¶æ•›é€Ÿåº¦ä¼šå˜å¾—éžå¸¸æ…¢ï¼Œåƒèœ—ç‰›ä¸€æ ·æŒªåŠ¨ã€‚

#### åŽŸç†è§£æž
*   **L2 çš„æ¢¯åº¦ç‰¹æ€§ï¼š** L2 Loss çš„å¯¼æ•°ä¸Žè¯¯å·®æˆæ­£æ¯”ã€‚
    $$ \frac{\partial x^2}{\partial x} = 2x $$
*   **èµ·æ­¥é˜¶æ®µï¼š** è¯¯å·®å¾ˆå¤§ï¼ˆä¾‹å¦‚ $x=100$ï¼‰ï¼Œæ¢¯åº¦é«˜è¾¾ $200$ã€‚ä¼˜åŒ–å™¨è¿ˆå‡ºçš„æ­¥å­å·¨å¤§ï¼Œæ¨¡åž‹è¿…é€ŸæŽ¥è¿‘æžå€¼ç‚¹ã€‚ï¼ˆ*æ³¨ï¼šå¦‚æžœå­¦ä¹ çŽ‡è®¾å¤ªå¤§ï¼Œè¿™é‡Œå®¹æ˜“å¯¼è‡´æ¢¯åº¦çˆ†ç‚¸*ï¼‰ã€‚
*   **æ”¶æ•›é˜¶æ®µï¼š** è¯¯å·®å¾ˆå°ï¼ˆä¾‹å¦‚ $x=0.01$ï¼‰ï¼Œæ¢¯åº¦åªæœ‰ $0.02$ã€‚
*   **ä¼˜åŒ–å™¨çš„å›°å¢ƒï¼š** æ­¤æ—¶ $\theta = \theta - \eta \cdot (0.02)$ã€‚ç”±äºŽæ¢¯åº¦å¤ªå°ï¼Œæ›´æ–°çš„æ­¥ä¼å‡ ä¹Žè‚‰çœ¼ä¸å¯è§ï¼Œè®­ç»ƒæ•ˆçŽ‡æžä½Žã€‚è¿™ä¹Ÿå®¹æ˜“é™·å…¥â€œå±€éƒ¨æœ€å°å€¼â€æˆ–â€œéžç‚¹â€ï¼Œå› ä¸ºé‚£é‡Œçš„æ¢¯åº¦æŽ¥è¿‘ 0ï¼ŒSGD å°±æŽ¨ä¸åŠ¨äº†ã€‚

#### è§£å†³æ–¹æ¡ˆï¼ˆä¼˜åŒ–å™¨çš„ä½œç”¨ï¼‰
1.  **Momentum (åŠ¨é‡)ï¼š** ç»™ SGD åŠ ä¸ŠåŠ¨é‡ï¼ˆSGD + Momentumï¼‰ã€‚å°±åƒä¸‹å¡çš„çƒæœ‰æƒ¯æ€§ä¸€æ ·ï¼Œå³ä½¿åœ¨å¡åº¦ï¼ˆæ¢¯åº¦ï¼‰å¾ˆå¹³ç¼“çš„åœ°æ–¹ï¼Œé ç€ä¹‹å‰çš„æƒ¯æ€§ï¼Œä¼˜åŒ–å™¨ä¾ç„¶èƒ½å†²è¿‡åŽ»ï¼ŒåŠ å¿«æ”¶æ•›é€Ÿåº¦ã€‚
2.  **Adamï¼š** Adam ä¼šè®¡ç®—æ¢¯åº¦çš„äºŒé˜¶çŸ©ï¼ˆå¹³æ–¹çš„ç§»åŠ¨å¹³å‡ï¼‰ï¼Œå®žé™…ä¸Šæ˜¯å¯¹æ¢¯åº¦åšäº†ä¸€ä¸ªå½’ä¸€åŒ–ã€‚å³ä½¿ L2 åœ¨è¿™ä¸€ç‚¹çš„æ¢¯åº¦å¾ˆå°ï¼ŒAdam å¯èƒ½ä¼šæ ¹æ®åŽ†å²ä¿¡æ¯æ”¾å¤§æ›´æ–°å¹…åº¦ï¼Œé˜²æ­¢è®­ç»ƒåœæ»žã€‚

---

### 3. å¤æ‚åœ°å½¢ä¸‹çš„é…åˆï¼šéžç‚¹ (Saddle Point)

åœ¨æ·±åº¦å­¦ä¹ çš„é«˜ç»´ç©ºé—´ä¸­ï¼ŒæŸå¤±å‡½æ•°çš„å½¢çŠ¶å¾€å¾€å¾ˆå¤æ‚ï¼Œå……æ»¡éžç‚¹ï¼ˆä¸€ä¸ªæ–¹å‘æ˜¯å¡åº•ï¼Œå¦ä¸€ä¸ªæ–¹å‘æ˜¯å¡é¡¶ï¼Œå½¢çŠ¶åƒé©¬éžï¼‰ã€‚

*   **L2 Loss + SGDï¼š** åœ¨éžç‚¹å¤„ï¼Œæ¢¯åº¦æŽ¥è¿‘ 0ã€‚L2 çš„ç‰¹æ€§ä½¿å¾—åœ¨å¹³å¦åŒºåŸŸæ¢¯åº¦æ›´å°ã€‚æ™®é€šçš„ SGD æžå®¹æ˜“åœ¨éžç‚¹â€œå¡æ­»â€ï¼Œå› ä¸ºå®ƒå®Œå…¨ä¾èµ–å½“å‰æ¢¯åº¦ã€‚
*   **L1 Loss + SGDï¼š** L1 åœ¨é›¶ç‚¹ä¸å¯å¯¼ï¼Œä½†åœ¨éžé›¶ç‚¹æ¢¯åº¦æ’å®šã€‚è¿™åœ¨æŸäº›ç¨€ç–ç‰¹å¾åœºæ™¯ä¸‹åè€Œæœ‰åŠ©äºŽâ€œé€ƒç¦»â€æŸäº›å¹³å¦åŒºåŸŸï¼Œå› ä¸ºåªè¦æœ‰è¯¯å·®ï¼Œå®ƒå°±æä¾›æ’å®šçš„æŽ¨åŠ›ã€‚
*   **Adam / RMSpropï¼š** è¿™äº›ä¼˜åŒ–å™¨è¢«è®¾è®¡ç”¨æ¥å¤„ç†è¿™ç§å¤æ‚å…³ç³»ã€‚å®ƒä»¬é€šè¿‡è§‚å¯Ÿè¿‡åŽ»ä¸€æ®µæ—¶é—´æ¢¯åº¦çš„å˜åŒ–çŽ‡ï¼ˆæ–¹å·®ï¼‰ï¼Œæ¥åˆ¤æ–­ï¼šâ€œå˜¿ï¼Œè¿™é‡Œè™½ç„¶æ¢¯åº¦å¾ˆå°ï¼ˆL2 é€ æˆæˆ–è€…æ˜¯åœ°å½¢å¹³å¦ï¼‰ï¼Œä½†æˆ‘ä»¬ä¸€ç›´æ²¡æ€Žä¹ˆç§»åŠ¨ï¼Œæ‰€ä»¥æˆ‘å¾—æŠŠå­¦ä¹ çŽ‡ä¸´æ—¶è°ƒå¤§ä¸€ç‚¹ï¼ŒæŠŠæ¨¡åž‹è¸¢å‡ºè¿™ä¸ªå‘ã€‚â€

### æ€»ç»“ï¼šå¦‚ä½•æ­é…ï¼Ÿ

| Loss Function | æ½œåœ¨é—®é¢˜ | ä¼˜åŒ–å™¨çš„é…åˆç­–ç•¥ |
| :--- | :--- | :--- |
| **L1 Loss** | æŽ¥è¿‘ç»ˆç‚¹æ—¶æ¢¯åº¦ä¸å‡å°ï¼Œå¯¼è‡´éœ‡è¡ã€‚ | **å¿…é¡»**é…åˆ**å­¦ä¹ çŽ‡è¡°å‡ (Learning Rate Decay)**ï¼Œæˆ–è€…ç›´æŽ¥ä½¿ç”¨ **Adam** (è‡ªå¸¦ä¸€å®šçš„è‡ªé€‚åº”èƒ½åŠ›)ã€‚ |
| **L2 Loss** | ç¦»ç¾¤ç‚¹å¯¼è‡´æ¢¯åº¦çˆ†ç‚¸ï¼›æŽ¥è¿‘ç»ˆç‚¹æ—¶æ¢¯åº¦è¿‡å°å¯¼è‡´æ”¶æ•›æ…¢ã€‚ | é…åˆ **Gradient Clipping (æ¢¯åº¦è£å‰ª)** é˜²æ­¢çˆ†ç‚¸ï¼›é…åˆ **Momentum** æˆ– **Adam** åŠ å¿«åŽæœŸçš„æ”¶æ•›é€Ÿåº¦ã€‚ |

**ä¸€å¥è¯æ€»ç»“ï¼š**
L1 å’Œ L2 æä¾›äº†**åŽŸå§‹çš„åŠ¨åŠ›ï¼ˆæ¢¯åº¦ï¼‰**ï¼Œè€Œä¼˜åŒ–å™¨æ˜¯**å˜é€Ÿç®±å’Œæ–¹å‘ç›˜**ã€‚L1 åƒä¸€ä¸ªåªä¼šæ²¹é—¨è¸©åˆ°åº•çš„å¸æœºï¼ˆæ’å®šæ¢¯åº¦ï¼‰ï¼Œéœ€è¦ä¼˜åŒ–å™¨é€šè¿‡åˆ¹è½¦ï¼ˆé™ä½Žå­¦ä¹ çŽ‡ï¼‰æ¥åœè½¦ï¼›L2 åƒä¸€ä¸ªè¶Šé è¿‘ç»ˆç‚¹è¶Šä¸æ•¢è¸©æ²¹é—¨çš„å¸æœºï¼ˆæ¢¯åº¦è¡°å‡ï¼‰ï¼Œéœ€è¦ä¼˜åŒ–å™¨æä¾›æƒ¯æ€§ï¼ˆåŠ¨é‡ï¼‰æ»‘è¡Œåˆ°ç»ˆç‚¹ã€‚