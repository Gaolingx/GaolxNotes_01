# 拒绝采样应用总结

### 🧑‍💻 User

拒绝采样在llm后训练阶段有哪些应用？

---

### 🤖 Assistant

拒绝采样（Rejection Sampling, RS）在 LLM **后训练（post-training）阶段** 的应用越来越广泛，特别是在对齐（alignment）、SFT 数据清洗、RLHF、偏好优化、推断优化等方面。下面我给你一个系统性的总结，并补充一些前沿论文中的使用方式。

---

# ✅ **拒绝采样在 LLM 后训练阶段的主要应用**

## **1. 用于构造高质量 SFT 样本（Self-Improving SFT / “Best-of-N SFT”）**

思路：

1. 给教师模型（Teacher）或学生模型（Student）生成 **N 个候选回答**
2. 用 reward model 或规则评估它们
3. **只挑选最好的一个** 作为训练数据

这是一种典型的拒绝采样策略：
→ 生成大池子 → 拒绝低质量 → 只保留高质量样本。

### 📝 应用场景：

* Qwen 使用 `/think`+`/no_think` 混合输出做 **高质量蒸馏数据构建**
* Llama 3.1、OpenAI SFT 数据构建
* RLAIF（用规则或 GPT-4 自动打分）构造 SFT 数据
* 多步思考（CoT）数据过滤：

  * 只保留「思路清晰 + 最终答案正确」的样本

### 📈 效果

* 小模型质量大幅提升（尤其数学与推理）
* 明显减少模型产生幻觉
* 数据质量可以媲美人工标注（甚至更好）

---

# **2. 用于生成 Preference 数据（Best-of-N Ranking）**

替代 RLHF（pairwise preference learning）里的人工偏好标注。

流程：

1. 对 prompt 采样 N 个回答
2. 使用 RM 或 GPT-4 / 规则系统对它们排序
3. 取出：

   * winner（最好）
   * loser（最差）

用于训练：

* DPO / IPO
* SimPO
* ORPO
* KTO
* 等各种偏好优化方法

### 为什么拒绝采样有效？

因为 LLM 本身采样的多样性提供了多级质量差异 → 更容易学到偏好梯度。

---

# **3. 用于 “Best-of-N Decoding” 再蒸馏（Boosting Small Models）**

例如：

* **Self-Play / Self-Distillation**
  Student 采样多次输出 → 选最优 → 训练 Student
* **Step-by-step reasoning**
  对每一步生成多个想法（samples）
  → 拒绝不一致或错误步骤
  → 保留一步最优解
  → 从而构成高质量链式思考数据（filtered chain-of-thought）

在 **推理到知识蒸馏（Reasoning-to-Direct Distillation）** 中尤其常见。

案例：

* DeepMind's Sparrow
* Google PaLM 的 "Verification Guided Generation"
* OpenAI Q* 系列的多重采样+验证策略（公开资料推测）

---

# **4. 用于 RLHF 训练中的 Off-policy 过滤**

在 RLHF 中，模型生成大量 trajectories，但：

* 不安全（toxic）
* 不符合规范
* 不符合 reward model 偏好

必须通过拒绝采样过滤掉。

特别在 PPO-based RLHF 中，拒绝采样用于：

* **拒绝 reward 太低的样本（避免模型崩掉）**
* **拒绝 KL divergence 过大的样本（保持稳定）**
* **拒绝有安全风险的样本**

有些策略叫：

* KL-penalized rejection
* reward-shaping filtering
* RM-verified sampling

---

# **5. 用于安全性对齐（Safety Alignment）**

拒绝采样从模型产生的多个回答里：

* 过滤掉不安全
* 过滤掉违反政策
* 过滤掉含有偏见/攻击性的回答

用于构造：

* 安全 SFT 数据
* Safety RM 数据
* RLHF 训练数据

例如：

* Anthropic constitutional AI = “reject unacceptable, keep acceptable”
* OpenAI OAI Safety Spec 也是类似流程

这是拒绝采样最“对齐工程化”的应用。

---

# **6. 用于推理阶段的再排序（Decoding Time Rejection Sampling）**

在 inference 中也有应用，但属于“后训练之外的部署优化”：

* Best-of-N 推理（从 N 个采样选最好的）
* VQ-VAE / MoE gating 校验
* Verified Sampling: 数学题验证步骤（如果不正确→reject→重新采样）

例如：

* DeepMind AlphaCode：采样 100-1000 个程序 → 测试用例验证 → 拒绝错误的 → 得到正确解

---

# **7. 用于蒸馏多思考模式（Think / No-Think 模式）**

你提到的 Qwen3 分析完全正确：

Teacher 使用：

* `/think` 输出
* `/no_think` 输出

Student 学到：

* 如何在 **不同思维模式** 之间自主切换
  这也是通过拒绝采样过滤掉 teacher 的垃圾输出。

---

# 📌 **总结：拒绝采样在 LLM 后训练阶段的 7 类用途**

| 应用               | 描述                            |
| ---------------- | ----------------------------- |
| **SFT 数据构造**     | Best-of-N → 高质量 SFT           |
| **偏好数据构造**       | winner/loser 训练 DPO/IPO       |
| **self-distill** | Student 多次 sampling → 选 best  |
| **RLHF 训练过滤**    | 过滤 toxic 或 low reward         |
| **安全对齐**         | 拒绝不安全输出构造安全数据                 |
| **推理质量提升**       | Verified sampling / Best-of-N |
| **多模式蒸馏**        | think/no-think 的数据过滤          |

---

# 如果你想，我可以继续深入：

### ➤ 给你写一套 **“拒绝采样增强的蒸馏 pipeline”**

### ➤ 对比 **拒绝采样 vs 专家混合（MoE gating）** 下的差异

### ➤ 总结各大公司的具体做法（Qwen / OAI / Meta / Anthropic）

你想继续哪个方向？
