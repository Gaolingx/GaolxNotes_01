# 拒绝采样应用总结

### 🧑‍💻 User

拒绝采样在llm后训练阶段有哪些应用？

---

### 🤖 Assistant

拒绝采样（Rejection Sampling），在大型语言模型（LLM）的后训练（Post-training）阶段，通常指的是一种**基于“生成-评估-选择”的策略**。它主要用于从模型自身的输出中筛选高质量样本，以进一步优化模型。

在LLM语境下，它常被称为 **Best-of-N** 采样或 **Rejection Sampling Fine-Tuning (RSFT)**。

以下是拒绝采样在LLM后训练阶段的几个核心应用场景：

### 1. 强化学习对齐（RLHF）的简化与替代
在传统的 RLHF 流程（如 PPO 算法）中，训练过程往往不稳定且对超参数敏感。拒绝采样提供了一种更简单、更稳定的替代方案。

*   **工作流程**：
    1.  **生成**：对于给定的提示词（prompt）$x$，让当前的策略模型 $\pi_\theta$ 生成 $N$ 个不同的回复 $\{y_1, y_2, ..., y_N\}$。
    2.  **评分**：使用训练好的奖励模型（Reward Model, $R$）对这 $N$ 个回复进行打分。
    3.  **拒绝/选择**：根据分数设定阈值，或者直接选择得分最高的一个或前 $k$ 个回复（即 Best-of-N）。
    4.  **微调**：将选出的高质量 $(x, y_{best})$ 对作为新的监督微调（SFT）数据，重新训练模型。

*   **应用实例**：**LLaMA 2** 的论文中明确提到，他们在 RLHF 阶段大量使用了拒绝采样。他们发现，通过这种方式迭代训练（即用上一轮模型生成的最佳样本训练下一轮模型），可以显著提升模型性能，有时效果甚至优于 PPO。

### 2. 构建偏好数据对（为 DPO/IPO 准备数据）
直接偏好优化（DPO）等算法需要成对的偏好数据 $(x, y_w, y_l)$，其中 $y_w$ 优于 $y_l$。拒绝采样是构建这类数据的高效手段。

*   **应用逻辑**：
    *   模型生成多个回复。
    *   利用奖励模型打分。
    *   选择得分最高的作为 $y_w$（赢家），得分较低的作为 $y_l$（输家）。
    *   这种方法构造的数据通常具有**同分布（On-policy）**特性，即数据来源于模型当前的分布，这对 DPO 的训练稳定性非常有帮助。

### 3. 数学推理与代码生成的自我提升 (Self-Improvement)
在数学和代码领域，答案的正确性通常是可以验证的（通过执行器或求解器），这使得拒绝采样特别有效。

*   **STaR (Self-Taught Reasoner) 方法**：
    *   模型生成推理过程（Chain-of-Thought, CoT）和最终答案。
    *   **过滤标准**：如果最终答案与标准答案（Ground Truth）一致，或者生成的代码通过了单元测试，则保留该推理路径。
    *   **训练**：将这些“生成的正确推理路径”作为新的训练数据。
    *   **意义**：这解决了“只有最终答案，没有推理过程”的数据匮乏问题。通过拒绝采样，模型学会了如何通过正确的 CoT 到达正确答案。

### 4. 蒸馏与合成数据清洗 (Knowledge Distillation)
当使用更强的模型（如 GPT-4）来教导较弱的模型（如 7B 参数的小模型）时，拒绝采样用于清洗合成数据。

*   **应用逻辑**：
    *   强模型（Teacher）生成多个复杂的指令或回复。
    *   弱模型（Student）或者一个独立的验证器（Verifier）对这些样本进行评估（例如评估指令遵循的难度或回复的逻辑性）。
    *   通过拒绝采样过滤掉低质量、幻觉严重或重复的合成数据，只保留高质量的蒸馏数据用于训练 Student 模型。

### 5. 推理阶段的计算换质量 (Inference-time Compute)
虽然这严格来说属于“推理”阶段，但它与后训练紧密相关，因为许多后训练策略旨在训练一个能够辅助拒绝采样的验证器（Verifier）。

*   **ORM (Outcome Reward Model) / PRM (Process Reward Model)**：
    *   在解决复杂问题时，不直接输出第一个结果，而是采样 $N$ 条路径。
    *   利用在后训练阶段训练好的 **Reward Model** 或 **Verifier** 选出最好的答案输出。
    *   OpenAI 的 o1 系列模型背后的逻辑之一，就是通过大量的搜索和采样来提升推理能力。

### 拒绝采样的数学直观

假设我们希望模型逼近一个理想分布 $p^*(y|x) \propto \exp(R(x, y) / \beta) \cdot \pi_{ref}(y|x)$。

拒绝采样本质上是通过在原有策略 $\pi_\theta$ 上进行过采样，将分布向高奖励区域“推”。当 $N \to \infty$ 时，Best-of-N 策略的期望奖励会随着 $N$ 的增加而对数增长。

### 总结：优缺点对比

| 维度 | 优势 | 劣势 |
| :--- | :--- | :--- |
| **稳定性** | 相比 PPO，不涉及复杂的价值函数更新和梯度策略，训练非常稳定（本质是 SFT）。 | 上限可能不如 PPO，因为它只能从模型已有的能力中筛选，难以探索出全新的策略。 |
| **实现难度** | 低。只需要推理和 SFT 流程。 | 推理成本高。需要生成大量的样本（$N$ 越大效果越好，但计算开销越大）。 |
| **数据质量** | 能有效去除模型自身的幻觉和错误逻辑。 | 极度依赖 Reward Model 或 Verifier 的准确性。如果 RM 被 hack（奖励劫持），模型会学习到错误的模式。 |

总而言之，拒绝采样已成为现代 LLM 后训练管线（Pipeline）中不可或缺的一环，特别是作为连接 SFT 和 RLHF 的桥梁，以及在推理类任务中提升数据质量的关键技术。