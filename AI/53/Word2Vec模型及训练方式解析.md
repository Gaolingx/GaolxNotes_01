# Word2Vec模型及训练方式解析

### 🧑‍💻 User

说说word2vec的CBOW与SkipGram模型及两种训练方式(负采样\层级softmax)， 两种训练方式的区别和应用场景。

---

### 🤖 Assistant

### 一、Word2Vec的CBOW与SkipGram模型

#### 1. **CBOW（Continuous Bag-of-Words）**
- **原理**：通过上下文的词向量（Context）预测中心词（Target Word）。
- **特点**：
  - 结构简单：输入层是上下文词的向量拼接或平均，输出层通过Softmax预测目标词。
  - 适合大规模语料：训练速度快，对高频词的泛化能力强。
  - 对低频词效果一般（因为需要看到足够多的上下文样例）。

#### 2. **SkipGram（连续词袋模型的逆过程）**
- **原理**：通过中心词（Target Word）预测上下文（Context）。
- **特点**：
  - 结构：输入层是中心词的向量，输出层通过Softmax预测多个上下文词。
  - 适合小数据或低频词：能更好捕捉稀疏词之间的共现关系。
  - 训练速度慢于CBOW，但对词义的表达更优。

---

### 二、两种训练方式

#### 1. **负采样（Negative Sampling）**
- **思想**：从语料库中随机采样一部分**负样本**（噪声词）参与训练，而非计算整个词汇表的概率分布。
- **实现**：
  - 目标：最大化正样本（真实共现词）的似然，同时最小化负样本的似然。
  - 常用策略：根据词频的3/4次方调整采样分布（高频词更可能被选为负样本）。
- **优点**：
  - 计算量大幅降低（仅需更新小批量样本）。
  - 适合小规模数据或低资源场景。
- **缺点**：
  - 需要手动设计负样本数量（通常5~20个）。

#### 2. **层级Softmax（Hierarchical Softmax）**
- **思想**：将词表构造成**二叉霍夫曼树**（Huffman Tree），通过路径预测代替传统Softmax。
- **实现**：
  - 每个叶子节点对应一个词，路径上的每个非叶节点使用逻辑回归（Sigmoid）判断左右分支。
  - 计算复杂度从`O(V)`（词汇表大小）降低到`O(log V)`。
- **优点**：
  - 无需显式定义负样本，适合大规模词表（如百万级词汇）。
  - 对低频词的向量更新更充分（高频词路径更短，更新更频繁）。
- **缺点**：
  - 实现复杂，需预先构建Huffman树。

---

### 三、两种训练方式的区别与应用场景

| **维度**               | **负采样（Negative Sampling）**                     | **层级Softmax（Hierarchical Softmax）**       |
|------------------------|---------------------------------------------------|---------------------------------------------|
| **计算复杂度**         | 低（仅更新正样本和少量负样本）                    | 中等（更新路径上的非叶节点）                |
| **适用场景**           | 小数据、低资源设备（如移动端）                    | 大规模语料（如百万级词汇）                  |
| **训练效率**           | 更快（尤其在小数据上）                            | 稍慢但稳定性更高（适合分布式训练）          |
| **词频敏感度**         | 高频词占优（负采样更关注高频词）                  | 对低频词更友好（Huffman树中低频词路径更长）  |
| **实现复杂度**         | 简单（无需特殊数据结构）                          | 复杂（需构建Huffman树）                     |
| **典型应用**           | 社交媒体文本、小规模领域语料                      | 大规模通用语料（如维基百科）                |

---

### 四、总结

1. **CBOW vs. SkipGram**：
   - CBOW训练更快，适合语料充足、对低频词敏感度要求低的场景。
   - SkipGram更擅长捕捉低频词和复杂语义，但需要更多训练数据。

2. **负采样 vs. 层级Softmax**：
   - 负采样：轻量高效，适合资源有限的场景，但需权衡负样本数量与效果。
   - 层级Softmax：更适合大规模语料，能避免显式负采样，但需处理Huffman编码的实现复杂度。

3. **实践建议**：
   - 小数据或资源有限：优先使用**负采样+SkipGram**（如Gensim默认参数）。
   - 大规模语料或百万级词表：使用**层级Softmax+SkipGram**（如Word2Vec原论文设定）。
   - 对低频词敏感的任务（如同义词识别）：优先选择**SkipGram+负采样**。

通过灵活选择模型与训练方式，Word2Vec可在不同场景下达到效率与效果的平衡。