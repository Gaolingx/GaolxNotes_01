# PyTorchå®ç°DDIM ODE

### ğŸ§‘â€ğŸ’» User

åœ¨pytorchä¸­ï¼Œå¦‚ä½•ç”¨x0-pred odeå®ç°ddimï¼Ÿ

---

### ğŸ¤– Assistant

åœ¨PyTorchä¸­ä½¿ç”¨DDIM(Denoising Diffusion Implicit Models)æ—¶ï¼Œx0-prediction(é¢„æµ‹åŸå§‹æ•°æ®)æ˜¯å®ç°çš„ä¸€ç§æ–¹å¼ã€‚æˆ‘ä¼šä¸ºä½ è§£é‡ŠDDIMçš„åŸç†ï¼Œå¹¶æä¾›å®Œæ•´çš„å®ç°ä»£ç ã€‚

## DDIMåŸºæœ¬åŸç†

DDIMæ˜¯æ‰©æ•£æ¨¡å‹çš„ä¸€ç§å˜ä½“ï¼Œå®ƒå…è®¸æˆ‘ä»¬è¿›è¡Œæ›´å°‘æ­¥éª¤çš„é‡‡æ ·ã€‚ä¸æ ‡å‡†DDPMä¸åŒï¼ŒDDIMé€šè¿‡ä¸€ä¸ªéé©¬å°”å¯å¤«è¿‡ç¨‹æ¥ç”Ÿæˆæ•°æ®ï¼Œä½¿å¾—æˆ‘ä»¬å¯ä»¥ç”¨æ›´å°‘çš„æ­¥æ•°è¾¾åˆ°ç›¸ä¼¼çš„ç”Ÿæˆæ•ˆæœã€‚

åœ¨DDIMä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©é¢„æµ‹å™ªå£°(Îµ-prediction)æˆ–é¢„æµ‹åŸå§‹æ•°æ®(x0-prediction)ã€‚è¿™é‡Œæˆ‘ä»¬ä¸»è¦å…³æ³¨x0-predictionã€‚

## DDIM x0-prediction ODEå®ç°

ä»¥ä¸‹æ˜¯DDIM x0-predictionåœ¨PyTorchä¸­çš„å®Œæ•´å®ç°ï¼š

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional

class GaussianDiffusion(nn.Module):
    def __init__(
        self,
        model,
        betas,
        loss_type="l2",
        rescale_timesteps=False,
    ):
        super().__init__()
        self.model = model
        self.loss_type = loss_type
        self.rescale_timesteps = rescale_timesteps

        # æ³¨å†ŒbetasåŠå…¶ç›¸å…³å˜é‡
        if isinstance(betas, str):
            betas = self.get_betas(betas)
      
        assert len(betas.shape) == 1, "betas must be 1-D"
        assert (betas > 0).all() and (betas <= 1).all(), "betas must be in (0, 1]"
      
        self.register_buffer("betas", betas)
        self.register_buffer("alphas", 1.0 - betas)
        self.register_buffer("alphas_cumprod", torch.cumprod(self.alphas, axis=0))
        self.register_buffer("sqrt_alphas_cumprod", torch.sqrt(self.alphas_cumprod))
        self.register_buffer("sqrt_one_minus_alphas_cumprod", torch.sqrt(1.0 - self.alphas_cumprod))
        self.register_buffer("log_one_minus_alphas_cumprod", torch.log(1.0 - self.alphas_cumprod))
        self.register_buffer("sqrt_recip_alphas_cumprod", torch.sqrt(1.0 / self.alphas_cumprod))
      
        # è®¡ç®—åéªŒæ–¹å·®
        self.register_buffer("posterior_variance", betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod))
      
        # è®¡ç®—å¯¹æ•°æ–¹å·®
        self.register_buffer("posterior_log_variance_clipped", torch.log(self.posterior_variance.clamp(min=1e-20)))
        self.register_buffer("posterior_mean_coef1", betas * torch.sqrt(self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod))
        self.register_buffer("posterior_mean_coef2", (1.0 - self.alphas_cumprod_prev) * torch.sqrt(self.alphas) / (1.0 - self.alphas_cumprod))
      
        # DDIMå‚æ•°
        self.register_buffer("ddim_sqrt_alphas_cumprod", self.sqrt_alphas_cumprod)
        self.register_buffer("ddim_sqrt_one_minus_alphas_cumprod", self.sqrt_one_minus_alphas_cumprod)
        self.register_buffer("ddim_sigmas", torch.sqrt(betas))
      
    def get_betas(self, beta_schedule):
        if beta_schedule == "linear":
            scale = 1000 / self.num_timesteps
            beta_start = scale * 0.0001
            beta_end = scale * 0.02
            return torch.linspace(beta_start, beta_end, self.num_timesteps, dtype=torch.float64)
        elif beta_schedule == "cosine":
            return self.cosine_beta_schedule(self.num_timesteps)
        else:
            raise NotImplementedError(f"unknown beta schedule: {beta_schedule}")
  
    def cosine_beta_schedule(self, timesteps, s=0.008):
        """
        cosine schedule as proposed in https://arxiv.org/abs/2102.09672
        """
        steps = timesteps + 1
        x = torch.linspace(0, timesteps, steps, dtype=torch.float64)
        alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2
        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]
        betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
        return torch.clip(betas, 0, 0.999)
  
    def q_mean_variance(self, x_start, t):
        """
        è®¡ç®—q(x_t | x_0)çš„å‡å€¼å’Œæ–¹å·®
        """
        mean = self._extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start
        variance = self._extract(1.0 - self.alphas_cumprod, t, x_start.shape)
        log_variance = self._extract(self.log_one_minus_alphas_cumprod, t, x_start.shape)
        return mean, variance, log_variance
  
    def q_sample(self, x_start, t, noise=None):
        """
        æ‰©æ•£è¿‡ç¨‹ï¼šq(x_t | x_0)çš„è®¡ç®—
        """
        if noise is None:
            noise = torch.randn_like(x_start)
      
        sqrt_alphas_cumprod_t = self._extract(self.sqrt_alphas_cumprod, t, x_start.shape)
        sqrt_one_minus_alphas_cumprod_t = self._extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape)
      
        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise
  
    def p_mean_variance(self, model, x, t, clip_denoised=True, denoised_fn=None):
        """
        è®¡ç®—p(x_{t-1} | x_t)çš„å‡å€¼å’Œæ–¹å·®ï¼ˆä½¿ç”¨x0-predictionï¼‰
        """
        # é¢„æµ‹x_0
        pred_x0 = model(x, t)
      
        if clip_denoised:
            pred_x0 = pred_x0.clamp(-1, 1)
      
        if denoised_fn is not None:
            pred_x0 = denoised_fn(pred_x0)
      
        # è®¡ç®—åéªŒå‡å€¼å’Œæ–¹å·®
        model_mean, _, _ = self.q_posterior_mean_variance(x_start=pred_x0, x_t=x, t=t)
      
        return model_mean, self.posterior_variance
  
    def q_posterior_mean_variance(self, x_start, x_t, t):
        """
        è®¡ç®—åéªŒq(x_{t-1} | x_t, x_0)çš„å‡å€¼å’Œæ–¹å·®
        """
        at = self._extract(self.alphas_cumprod, t, x_t.shape)
        at_prev = self._extract(self.alphas_cumprod_prev, t, x_t.shape)
        sqrt1 = self._extract(self.sqrt_one_minus_alphas_cumprod, t, x_t.shape)
      
        # è®¡ç®—åéªŒå‡å€¼
        posterior_mean = (
            self._extract(self.posterior_mean_coef1, t, x_t.shape) * x_start
            + self._extract(self.posterior_mean_coef2, t, x_t.shape) * x_t
        )
      
        # è®¡ç®—åéªŒæ–¹å·®
        posterior_variance = self._extract(self.posterior_variance, t, x_t.shape)
      
        return posterior_mean, posterior_variance, sqrt1
  
    def p_sample(self, model, x, t, clip_denoised=True, denoised_fn=None):
        """
        ä»p(x_{t-1} | x_t)é‡‡æ ·
        """
        mean, variance = self.p_mean_variance(model, x, t, clip_denoised, denoised_fn)
        noise = torch.randn_like(x)
      
        # ä¸æ·»åŠ å™ªå£°ï¼ˆt=0æ—¶ï¼‰
        nonzero_mask = (
            (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))
        )
      
        return mean + nonzero_mask * torch.sqrt(variance) * noise
  
    def ddim_step(self, model, x, t, timestep_prev=None, eta=0.0):
        """
        DDIMé‡‡æ ·æ­¥éª¤ï¼ˆä½¿ç”¨x0-predictionï¼‰:
        x_{t-1} = sqrt(alpha_{t-1}) * pred_x0 + sqrt(1-alpha_{t-1}-sigma_t^2) * direction + sigma_t * noise
      
        å…¶ä¸­etaæ§åˆ¶éšæœºæ€§ï¼Œå½“eta=0æ—¶ç›¸å½“äºODEï¼Œå³ç¡®å®šæ€§è¿‡ç¨‹
        """
        # é¢„æµ‹x_0
        pred_x0 = model(x, t)
      
        # è·å–alphaå€¼
        alpha_cumprod_t = self._extract(self.alphas_cumprod, t, x.shape)
        sqrt_alpha_cumprod_t = self._extract(self.sqrt_alphas_cumprod, t, x.shape)
      
        if timestep_prev is None:
            timestep_prev = t - 1
      
        alpha_cumprod_t_prev = self._extract(self.alphas_cumprod_prev, t, x.shape)
        sqrt_alpha_cumprod_t_prev = self._extract(self.sqrt_alphas_cumprod_prev, x.shape)
      
        # è®¡ç®—æ–¹å‘æŒ‡å‘x_t
        direction = torch.sqrt(1 - alpha_cumprod_t_prev) * pred_x0
      
        # è®¡ç®—å™ªå£°é¡¹
        noise = torch.randn_like(x)
        sigma_t = eta * torch.sqrt((1 - alpha_cumprod_t_prev) / (1 - alpha_cumprod_t)) * torch.sqrt(1 - alpha_cumprod_t / alpha_cumprod_t_prev)
      
        # è®¡ç®—DDIMé‡‡æ ·ç»“æœ
        x_prev = sqrt_alpha_cumprod_t_prev * pred_x0 + direction + sigma_t * noise
      
        return x_prev
  
    def ddim_sample(self, model, shape, num_steps=50, eta=0.0, clip_denoised=True):
        """
        DDIMé‡‡æ ·è¿‡ç¨‹ï¼ˆODEç‰ˆæœ¬ï¼Œeta=0ï¼‰
      
        å‚æ•°:
        - model: è®­ç»ƒå¥½çš„æ‰©æ•£æ¨¡å‹
        - shape: ç”Ÿæˆæ ·æœ¬çš„å½¢çŠ¶
        - num_steps: DDIMé‡‡æ ·æ­¥æ•°ï¼ˆå°‘äºåŸå§‹æ‰©æ•£æ­¥æ•°ï¼‰
        - eta: éšæœºæ€§å‚æ•°ï¼Œ0è¡¨ç¤ºç¡®å®šæ€§è¿‡ç¨‹ï¼ˆODEï¼‰
        - clip_denoised: æ˜¯å¦å¯¹é¢„æµ‹çš„x0è¿›è¡Œè£å‰ª
      
        è¿”å›:
        - ç”Ÿæˆçš„æ ·æœ¬
        """
        # è®¡ç®—é‡‡æ ·çš„æ—¶é—´æ­¥
        time_steps = self.get_ddim_timesteps(num_steps)
      
        # ä»çº¯å™ªå£°å¼€å§‹
        img = torch.randn(shape)
      
        # éå†æ¯ä¸ªæ—¶é—´æ­¥
        for i, (t, timestep_prev) in enumerate(time_steps):
            t_tensor = torch.full((shape[0],), t, device=img.device, dtype=torch.long)
          
            # ä½¿ç”¨DDIMæ­¥éª¤æ›´æ–°å›¾åƒ
            img = self.ddim_step(model, img, t_tensor, timestep_prev, eta)
          
            # å¦‚æœéœ€è¦ï¼Œå¯¹é¢„æµ‹çš„x0è¿›è¡Œè£å‰ª
            if clip_denoised:
                img = img.clamp(-1, 1)
      
        return img
  
    def get_ddim_timesteps(self, num_steps):
        """
        è·å–DDIMé‡‡æ ·çš„æ—¶é—´æ­¥åºåˆ—
      
        å‚æ•°:
        - num_steps: é‡‡æ ·æ­¥æ•°
      
        è¿”å›:
        - æ—¶é—´æ­¥åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º(t, timestep_prev)
        """
        # è®¡ç®—åŸå§‹æ‰©æ•£è¿‡ç¨‹çš„æ—¶é—´æ­¥é—´éš”
        step_ratio = self.num_timesteps // num_steps
      
        # åˆ›å»ºç­‰é—´éš”çš„æ—¶é—´æ­¥
        timesteps = (torch.arange(num_steps, 0, -1) * step_ratio).long()
      
        # åˆ›å»ºæ—¶é—´æ­¥å¯¹
        timesteps_with_prev = []
        for i in range(len(timesteps)):
            t = timesteps[i]
            if i < len(timesteps) - 1:
                timestep_prev = timesteps[i+1]
            else:
                timestep_prev = None
            timesteps_with_prev.append((t, timestep_prev))
      
        return timesteps_with_prev
  
    def _extract(self, a, t, x_shape):
        """
        ä»aä¸­æå–ç‰¹å®šæ—¶é—´æ­¥tçš„å€¼ï¼Œå¹¶è°ƒæ•´å½¢çŠ¶ä»¥åŒ¹é…x_shape
        """
        batch_size = t.shape[0]
        out = a.to(t.device).gather(0, t)
        return out.reshape(batch_size, *((1,) * (len(x_shape) - 1)))
  
    def loss(self, x_start, t, noise=None):
        """
        è®¡ç®—æŸå¤±å‡½æ•°
        """
        if noise is None:
            noise = torch.randn_like(x_start)
      
        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)
      
        # ä½¿ç”¨æ¨¡å‹é¢„æµ‹x_0
        pred_x0 = self.model(x_noisy, t)
      
        if self.loss_type == 'l1':
            loss = F.l1_loss(pred_x0, x_start)
        elif self.loss_type == 'l2':
            loss = F.mse_loss(pred_x0, x_start)
        elif self.loss_type == 'huber':
            loss = F.smooth_l1_loss(pred_x0, x_start)
        else:
            raise NotImplementedError()
      
        return loss
```

## å¦‚ä½•ä½¿ç”¨ä¸Šè¿°ä»£ç 

ä»¥ä¸‹æ˜¯ä½¿ç”¨ä¸Šè¿°DDIMå®ç°çš„ç®€å•ç¤ºä¾‹ï¼š

```python
import torch
import torch.nn as nn
import torchvision
from torchvision import transforms
from torch.utils.data import DataLoader
from torch.optim import Adam
import matplotlib.pyplot as plt

# å®šä¹‰ä¸€ä¸ªç®€å•çš„UNetæ¨¡å‹
class SimpleUNet(nn.Module):
    def __init__(self, in_channels=1, out_channels=1):
        super().__init__()
        # è¿™é‡Œç®€åŒ–äº†UNetç»“æ„ï¼Œå®é™…åº”ç”¨ä¸­åº”è¯¥ä½¿ç”¨æ›´å®Œæ•´çš„æ¶æ„
        self.conv1 = nn.Conv2d(in_channels + 1, 32, kernel_size=3, padding=1)  # +1 for time embedding
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
      
        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)
        self.upconv2 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)
        self.conv4 = nn.Conv2d(32, out_channels, kernel_size=3, padding=1)
      
        self.time_embedding = nn.Linear(1, 1)
      
    def forward(self, x, t):
        # æ—¶é—´åµŒå…¥
        t = t.float()
        t = t.unsqueeze(1).unsqueeze(2).unsqueeze(3)
        t = self.time_embedding(t) / 1000.0
      
        # æ‰©å±•æ—¶é—´åµŒå…¥åˆ°ä¸xç›¸åŒçš„å°ºå¯¸
        t = t.expand(-1, -1, x.shape[2], x.shape[3])
      
        # è¿æ¥è¾“å…¥å’Œæ—¶é—´åµŒå…¥
        x = torch.cat([x, t], dim=1)
      
        # ä¸‹é‡‡æ ·
        x = F.relu(self.conv1(x))
        x = F.max_pool2d(x, kernel_size=2)
        x = F.relu(self.conv2(x))
        x = F.max_pool2d(x, kernel_size=2)
        x = F.relu(self.conv3(x))
      
        # ä¸Šé‡‡æ ·
        x = F.relu(self.upconv1(x))
        x = F.relu(self.upconv2(x))
      
        # è¾“å‡º
        x = torch.sigmoid(self.conv4(x))
      
        return x

# è®¾ç½®å‚æ•°
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
batch_size = 128
epochs = 10
timesteps = 1000  # æ‰©æ•£è¿‡ç¨‹çš„æ—¶é—´æ­¥æ•°
ddim_steps = 20   # DDIMé‡‡æ ·çš„æ—¶é—´æ­¥æ•°

# åŠ è½½MNISTæ•°æ®é›†
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# åˆ›å»ºæ¨¡å‹å’Œæ‰©æ•£è¿‡ç¨‹
model = SimpleUNet().to(device)
betas = torch.linspace(0.0001, 0.02, timesteps)
diffusion = GaussianDiffusion(model, betas).to(device)

# ä¼˜åŒ–å™¨
optimizer = Adam(model.parameters(), lr=1e-4)

# è®­ç»ƒæ¨¡å‹
for epoch in range(epochs):
    for i, (images, _) in enumerate(train_loader):
        images = images.to(device)
      
        # éšæœºæ—¶é—´æ­¥
        t = torch.randint(0, timesteps, (images.shape[0],), device=device)
      
        # è®¡ç®—æŸå¤±
        loss = diffusion.loss(images, t)
      
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
      
        if i % 100 == 0:
            print(f"Epoch {epoch}, Batch {i}, Loss {loss.item():.4f}")

# ä½¿ç”¨DDIMç”Ÿæˆæ ·æœ¬
print("Generating samples using DDIM...")
with torch.no_grad():
    shape = (16, 1, 28, 28)  # 16ä¸ª28x28çš„MNISTæ ·æœ¬
    samples = diffusion.ddim_sample(model, shape, num_steps=ddim_steps, eta=0.0)
  
    # å¯è§†åŒ–ç”Ÿæˆçš„æ ·æœ¬
    fig, axes = plt.subplots(4, 4, figsize=(10, 10))
    for i, ax in enumerate(axes.flat):
        ax.imshow(samples[i, 0].cpu().numpy(), cmap='gray')
        ax.axis('off')
    plt.tight_layout()
    plt.savefig('ddim_samples.png')
    plt.show()
```

## DDIM x0-predictionä¸Îµ-predictionçš„åŒºåˆ«

DDIMä¸»è¦æœ‰ä¸¤ç§é¢„æµ‹æ–¹å¼ï¼š

1. **x0-prediction**ï¼šæ¨¡å‹ç›´æ¥é¢„æµ‹åŸå§‹æ•°æ®x0ã€‚è¿™ä¹Ÿæ˜¯æˆ‘ä»¬åœ¨ä»£ç ä¸­å®ç°çš„æ–¹å¼ã€‚

2. **Îµ-prediction**ï¼šæ¨¡å‹é¢„æµ‹æ·»åŠ çš„å™ªå£°Îµã€‚è¿™æ˜¯åŸå§‹DDPMè®ºæ–‡ä¸­ä½¿ç”¨çš„æ–¹å¼ã€‚

è¿™ä¸¤ç§æ–¹å¼å¯ä»¥é€šè¿‡ç®€å•çš„æ•°å­¦å…³ç³»ç›¸äº’è½¬æ¢ï¼š
- å¦‚æœæ¨¡å‹é¢„æµ‹x0ï¼Œé‚£ä¹ˆé¢„æµ‹çš„å™ªå£°ä¸ºï¼š\(\hat{\epsilon} = \frac{x_t - \sqrt{\bar{\alpha}_t} \hat{x}_0}{\sqrt{1 - \bar{\alpha}_t}}\)
- å¦‚æœæ¨¡å‹é¢„æµ‹Îµï¼Œé‚£ä¹ˆé¢„æµ‹çš„åŸå§‹æ•°æ®ä¸ºï¼š\(\hat{x}_0 = \frac{x_t - \sqrt{1 - \bar{\alpha}_t} \hat{\epsilon}}{\sqrt{\bar{\alpha}_t}}\)

## DDIMçš„ODEè§£é‡Š

å½“eta=0æ—¶ï¼ŒDDIMé‡‡æ ·è¿‡ç¨‹å˜æˆäº†ä¸€ä¸ªç¡®å®šæ€§çš„ODEï¼ˆå¸¸å¾®åˆ†æ–¹ç¨‹ï¼‰è¿‡ç¨‹ã€‚è¿™ä½¿å¾—æˆ‘ä»¬å¯ä»¥ç”¨æ›´å°‘çš„æ­¥æ•°ä»å™ªå£°ç”Ÿæˆæ•°æ®ï¼ŒåŒæ—¶ä¿æŒä¸åŸå§‹DDPMç›¸ä¼¼çš„åˆ†å¸ƒã€‚

æ•°å­¦ä¸Šï¼ŒDDIMçš„ODEå½¢å¼å¯ä»¥è¡¨ç¤ºä¸ºï¼š
\[dx_t = f(x_t, t) dt\]

å…¶ä¸­ï¼š
- \(x_t\) æ˜¯æ—¶é—´æ­¥tçš„æ•°æ®
- \(f(x_t, t)\) æ˜¯ç”±ç½‘ç»œå†³å®šçš„å‘é‡åœº

è¿™ç§ç¡®å®šæ€§é‡‡æ ·è¿‡ç¨‹ä¸éšæœºé‡‡æ ·ï¼ˆå¯¹åº”äºSDEï¼Œéšæœºå¾®åˆ†æ–¹ç¨‹ï¼‰ç›¸æ¯”ï¼Œæœ‰ä»¥ä¸‹ä¼˜ç‚¹ï¼š
1. æ›´å¿«çš„é‡‡æ ·é€Ÿåº¦ï¼ˆå¯ä»¥ç”¨æ›´å°‘çš„æ­¥éª¤ï¼‰
2. æ›´ä¸€è‡´å’Œå¯é‡å¤çš„é‡‡æ ·ç»“æœ
3. æ›´å®¹æ˜“è¿›è¡Œæ’å€¼å’Œæ“çºµæ½œåœ¨ç©ºé—´

## æ€»ç»“

ä»¥ä¸Šä»£ç å±•ç¤ºäº†å¦‚ä½•åœ¨PyTorchä¸­å®ç°DDIMçš„x0-prediction ODEç‰ˆæœ¬ã€‚å…³é”®ç‚¹åŒ…æ‹¬ï¼š

1. å®ç°äº†æ‰©æ•£è¿‡ç¨‹çš„å‰å‘å’ŒåéªŒåˆ†å¸ƒ
2. å®ç°äº†DDIMé‡‡æ ·æ­¥éª¤ï¼Œç‰¹åˆ«æ˜¯x0-predictionç‰ˆæœ¬
3. é€šè¿‡å°†etaå‚æ•°è®¾ä¸º0æ¥å®ç°ç¡®å®šæ€§ODEé‡‡æ ·
4. æä¾›äº†å®Œæ•´çš„è®­ç»ƒå’Œé‡‡æ ·æµç¨‹

è¿™ç§æ–¹æ³•åœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼Œå¤§å¤§å‡å°‘äº†é‡‡æ ·æ‰€éœ€çš„æ­¥éª¤æ•°ï¼Œä½¿å¾—æ‰©æ•£æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­æ›´åŠ é«˜æ•ˆã€‚