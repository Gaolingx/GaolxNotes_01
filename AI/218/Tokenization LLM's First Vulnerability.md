# Tokenization LLM's First Vulnerability

### ğŸ§‘â€ğŸ’» User

â€œ9.11å’Œ9.8å“ªä¸ªæ›´å¤§ã€strawberryä¸­æœ‰å‡ ä¸ªrâ€ï¼Œä¸ºä»€ä¹ˆè¿™äº›é—®é¢˜æœ€æ—©å°±æ˜¯tokenizeræ”»å‡»é—®é¢˜ï¼Ÿ

---

### ğŸ¤– Assistant

## The "Strawberry" and "9.11 vs. 9.8" Conundrums: How Seemingly Simple Questions Exposed Critical Flaws in AI

The seemingly trivial questions, "Which is bigger, 9.11 or 9.8?" and "How many 'r's are in 'strawberry'?", have become canonical examples in the world of artificial intelligence, not for their simplicity, but for their profound ability to expose a fundamental vulnerability in large language models (LLMs): the tokenizer. While not "attacks" in the traditional, malicious sense, these questions were among the earliest and most prominent public demonstrations of how the inner workings of a tokenizer could lead to illogical and incorrect outputs, paving the way for the later development of sophisticated tokenizer-based attacks.

### The Tokenizer's Double-Edged Sword

At its core, a tokenizer is a crucial component of an LLM that breaks down vast streams of text into smaller, manageable units called "tokens." These tokens can be words, parts of words (subwords), or even individual characters. This process is essential for the model to process and understand human language. However, the way a tokenizer deconstructs text can sometimes be counterintuitive to human logic.

This is precisely where the "strawberry" and "9.11 vs. 9.8" problems originate. They highlight a critical disconnect between how humans perceive information and how a machine, through the lens of its tokenizer, interprets the same data.

### Deconstructing the "Strawberry" Riddle

For a human, counting the letter 'r' in "strawberry" is a straightforward task of character-by-character scanning. An LLM, however, might not "see" the word in the same way. A common tokenization of "strawberry" is to split it into `["straw", "berry"]`.

Once tokenized in this manner, the model processes the semantic meaning of these two distinct tokens. The concept of the individual letters within these tokens can be lost or obscured. The model might know a lot about "straw" and "berry," but it struggles to perform a simple character count on the original, untokenized word. This tokenization-induced "blindness" to the word's constituent letters is why early LLMs would frequently and confidently answer that there are two 'r's in "strawberry."

### The Ambiguity of "9.11 vs. 9.8"

The question of whether 9.11 is greater than 9.8 should be a simple mathematical comparison. However, for an LLM, the context is not always purely numerical. The tokenization of these numbers can lead to multiple interpretations, often influenced by the vast dataset the model was trained on.

For example, "9.11" is a powerful and frequently occurring token associated with the September 11th attacks, a date. Similarly, "9.8" might be interpreted as a software version number or a section in a document. When the model encounters "9.11" and "9.8," it may not default to a numerical comparison. Instead, it might draw upon the more prevalent contextual meanings in its training data. In the context of version numbers, 9.11 is indeed a later and therefore "greater" version than 9.8. This ambiguity, introduced at the tokenization stage, leads the model to an answer that is logical within a non-mathematical framework but incorrect in the context the user intended.

### From Revealing Flaws to Inspiring Attacks

These early and widely publicized failures were not, in themselves, malicious attacks. There was no ill-intent in asking the questions. However, they were the "canary in the coal mine," revealing a fundamental vulnerability. They demonstrated that the output of a powerful LLM could be manipulated or distorted based on how the input was tokenized.

This understanding laid the groundwork for what are now formally recognized as **tokenizer attacks**. Security researchers and malicious actors realized that by carefully crafting inputs, they could exploit the tokenization process to:

* **Bypass safety filters:** By splitting malicious words or phrases into innocuous-looking tokens, attackers can sometimes sneak harmful prompts past the model's safety mechanisms.
* **Induce specific, unintended behaviors:** Similar to how "9.11" can trigger a non-mathematical comparison, carefully chosen inputs can manipulate the model into performing actions it was not designed to do.
* **Create "jailbreaks":** These are techniques that exploit system vulnerabilities, including those in the tokenizer, to circumvent the safety and ethical guidelines programmed into the LLM.

In essence, the "strawberry" and "9.11 vs. 9.8" problems were the initial, innocent probes that exposed a crack in the foundation of LLMs. They highlighted that the translation from human language to machine-readable tokens was not a flawless process and that this very process could be a vector for unexpected and potentially harmful behavior. They were the first whispers of a vulnerability that has since become a significant area of research in AI safety and security.