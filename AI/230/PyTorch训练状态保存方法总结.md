# PyTorchè®­ç»ƒçŠ¶æ€ä¿å­˜æ–¹æ³•æ€»ç»“

### ğŸ§‘â€ğŸ’» User

pytorchä¸­ï¼Œå¦‚ä½•dumpæ¯ä¸€æ­¥çš„param gradient optim stateï¼Ÿ

---

### ğŸ¤– Assistant

# PyTorch ä¸­ Dump å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€çš„æ–¹æ³•

åœ¨ PyTorch ä¸­ï¼Œæ‚¨å¯ä»¥é€šè¿‡å¤šç§æ–¹å¼åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ dump æ¯ä¸€æ­¥çš„å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€ã€‚ä»¥ä¸‹æ˜¯å‡ ç§å¸¸ç”¨æ–¹æ³•ï¼š

## æ–¹æ³•ä¸€ï¼šæ‰‹åŠ¨è®°å½•æ¯ä¸ªè®­ç»ƒæ­¥éª¤

```python
import torch
import torch.nn as nn
import torch.optim as optim
import pickle
import os

# åˆ›å»ºä¿å­˜ç›®å½•
os.makedirs('./training_dumps', exist_ok=True)

def dump_training_state(step, model, optimizer, loss):
    """ä¿å­˜è®­ç»ƒçŠ¶æ€"""
    state_dict = {
        'step': step,
        'loss': loss.item(),
        'model_params': {name: param.detach().clone() for name, param in model.named_parameters()},
        'model_grads': {name: param.grad.detach().clone() if param.grad is not None else None 
                       for name, param in model.named_parameters()},
        'optimizer_state': optimizer.state_dict()
    }
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    with open(f'./training_dumps/step_{step}.pkl', 'wb') as f:
        pickle.dump(state_dict, f)
    
    return state_dict

# ç¤ºä¾‹è®­ç»ƒå¾ªç¯
model = nn.Linear(10, 1)
optimizer = optim.Adam(model.parameters(), lr=0.01)
criterion = nn.MSELoss()

for step in range(100):
    # æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
    inputs = torch.randn(32, 10)
    targets = torch.randn(32, 1)
    
    # å‰å‘ä¼ æ’­
    outputs = model(inputs)
    loss = criterion(outputs, targets)
    
    # åå‘ä¼ æ’­
    optimizer.zero_grad()
    loss.backward()
    
    # åœ¨ä¼˜åŒ–å™¨æ­¥éª¤ä¹‹å‰ä¿å­˜çŠ¶æ€
    if step % 10 == 0:  # æ¯10æ­¥ä¿å­˜ä¸€æ¬¡
        dump_training_state(step, model, optimizer, loss)
    
    # æ›´æ–°å‚æ•°
    optimizer.step()
```

## æ–¹æ³•äºŒï¼šä½¿ç”¨é’©å­ï¼ˆHooksï¼‰è‡ªåŠ¨è®°å½•æ¢¯åº¦

```python
import torch
import torch.nn as nn

# å­˜å‚¨æ¢¯åº¦
gradients = {}

def save_grad(name):
    """ä¿å­˜æ¢¯åº¦çš„é’©å­å‡½æ•°"""
    def hook(grad):
        gradients[name] = grad.detach().clone()
    return hook

# æ³¨å†Œæ¢¯åº¦é’©å­
model = nn.Linear(10, 1)
for name, param in model.named_parameters():
    param.register_hook(save_grad(name))

# è®­ç»ƒå¾ªç¯ä¸­
for step in range(100):
    # è®­ç»ƒæ­¥éª¤...
    inputs = torch.randn(32, 10)
    targets = torch.randn(32, 1)
    
    outputs = model(inputs)
    loss = nn.MSELoss()(outputs, targets)
    
    optimizer.zero_grad()
    loss.backward()
    
    # æ­¤æ—¶gradientså­—å…¸ä¸­å·²ä¿å­˜æ‰€æœ‰æ¢¯åº¦
    print(f"Step {step} gradients:", gradients)
    
    optimizer.step()
```

## æ–¹æ³•ä¸‰ï¼šä½¿ç”¨å›è°ƒå‡½æ•°çš„é«˜çº§å®ç°

```python
import torch
import torch.nn as nn
import json
import numpy as np

class TrainingDumper:
    def __init__(self, model, optimizer, save_dir='./training_dumps'):
        self.model = model
        self.optimizer = optimizer
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)
        
        # å­˜å‚¨å†å²è®°å½•
        self.history = []
    
    def dump_step(self, step, loss):
        """ä¿å­˜å½“å‰æ­¥éª¤çš„çŠ¶æ€"""
        # è·å–å‚æ•°å’Œæ¢¯åº¦
        params = {name: param.detach().cpu().numpy() 
                 for name, param in self.model.named_parameters()}
        
        grads = {name: param.grad.detach().cpu().numpy() if param.grad is not None else None
                for name, param in self.model.named_parameters()}
        
        # è·å–ä¼˜åŒ–å™¨çŠ¶æ€
        optim_state = self.optimizer.state_dict()
        
        # è½¬æ¢ä¸ºå¯JSONåºåˆ—åŒ–çš„æ ¼å¼
        state = {
            'step': step,
            'loss': float(loss),
            'params': {k: v.tolist() for k, v in params.items()},
            'grads': {k: v.tolist() if v is not None else None for k, v in grads.items()},
            'optimizer': {
                'state': {k: {k2: v2.tolist() if isinstance(v2, torch.Tensor) else v2 
                             for k2, v2 in v.items()} 
                         for k, v in optim_state['state'].items()},
                'param_groups': optim_state['param_groups']
            }
        }
        
        # ä¿å­˜åˆ°å†å²è®°å½•
        self.history.append(state)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(f'{self.save_dir}/step_{step}.json', 'w') as f:
            json.dump(state, f, indent=2)
            
        return state

# ä½¿ç”¨ç¤ºä¾‹
model = nn.Linear(10, 1)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
dumper = TrainingDumper(model, optimizer)

for step in range(100):
    # è®­ç»ƒæ­¥éª¤...
    inputs = torch.randn(32, 10)
    targets = torch.randn(32, 1)
    
    outputs = model(inputs)
    loss = nn.MSELoss()(outputs, targets)
    
    optimizer.zero_grad()
    loss.backward()
    
    # ä¿å­˜çŠ¶æ€
    if step % 10 == 0:
        dumper.dump_step(step, loss)
    
    optimizer.step()
```

## æ–¹æ³•å››ï¼šä½¿ç”¨PyTorch Lightningçš„å›è°ƒ

å¦‚æœæ‚¨ä½¿ç”¨PyTorch Lightningï¼Œå¯ä»¥åˆ©ç”¨å…¶å›è°ƒç³»ç»Ÿï¼š

```python
import pytorch_lightning as pl
import torch

class DumpCallback(pl.Callback):
    def __init__(self, save_dir='./lightning_dumps'):
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)
    
    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):
        """åœ¨æ¯ä¸ªè®­ç»ƒæ‰¹æ¬¡ç»“æŸæ—¶è°ƒç”¨"""
        if batch_idx % 10 == 0:  # æ¯10ä¸ªæ‰¹æ¬¡ä¿å­˜ä¸€æ¬¡
            # ä¿å­˜å‚æ•°å’Œæ¢¯åº¦
            state = {
                'step': trainer.global_step,
                'params': {name: param.detach().cpu().numpy() 
                          for name, param in pl_module.named_parameters()},
                'grads': {name: param.grad.detach().cpu().numpy() if param.grad is not None else None
                         for name, param in pl_module.named_parameters()}
            }
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            torch.save(state, f'{self.save_dir}/step_{trainer.global_step}.pt')

# åœ¨Lightningè®­ç»ƒä¸­ä½¿ç”¨
trainer = pl.Trainer(callbacks=[DumpCallback()])
trainer.fit(model, train_loader)
```

## æ³¨æ„äº‹é¡¹

1. **æ€§èƒ½è€ƒè™‘**ï¼šé¢‘ç¹ä¿å­˜çŠ¶æ€ä¼šæ˜¾è‘—å‡æ…¢è®­ç»ƒé€Ÿåº¦å¹¶å ç”¨å¤§é‡å­˜å‚¨ç©ºé—´ï¼Œè¯·æ ¹æ®éœ€è¦è°ƒæ•´ä¿å­˜é¢‘ç‡ã€‚
2. **å†…å­˜ç®¡ç†**ï¼šä½¿ç”¨`.detach().cpu().numpy()`å°†å¼ é‡ç§»è‡³CPUå¹¶è½¬æ¢ä¸ºNumPyæ•°ç»„å¯ä»¥å‡å°‘GPUå†…å­˜ä½¿ç”¨ã€‚
3. **é€‰æ‹©æ€§ä¿å­˜**ï¼šè€ƒè™‘åªä¿å­˜æ‚¨å…³å¿ƒçš„ç‰¹å®šå‚æ•°ï¼Œè€Œä¸æ˜¯æ‰€æœ‰å‚æ•°ã€‚
4. **åºåˆ—åŒ–æ ¼å¼**ï¼šJSONé€‚åˆå°æ•°æ®ä¸”éœ€è¦å¯è¯»æ€§çš„æƒ…å†µï¼ŒPickleæˆ–PyTorchçš„ä¿å­˜æ ¼å¼é€‚åˆå¤§å‹å¼ é‡ã€‚

é€‰æ‹©å“ªç§æ–¹æ³•å–å†³äºæ‚¨çš„å…·ä½“éœ€æ±‚ã€æ¨¡å‹å¤æ‚åº¦å’Œå­˜å‚¨çº¦æŸã€‚