# RL vs SFT for Model Reasoning

### 🧑‍💻 User

问题：为什么现在普遍基于RL去提升模型的推理能力，而不是通过构造数据做SFT的方式呢？
背景：现在提到提升模型的推理能力，基本都是基于RL的方案去做的，SFT通常是作为一种冷启动的方式，那RL到底有什么好处呢？是训练过程提供了负样本或者增加了更多的多样性吗？

---

### 🤖 Assistant

这是一个非常深刻且紧跟当前大模型前沿（如 OpenAI o1, DeepSeek-R1）的问题。

简单来说，SFT（有监督微调）教会模型**“怎么像人一样说话”**（格式、语气、基本逻辑），而 RL（强化学习）教会模型**“怎么通过思考解决难题”**（探索、试错、自我修正）。

对于推理任务（Math, Coding, Logic），RL 之所以成为核心驱动力，是因为 SFT 存在**“天花板”**和**“数据瓶颈”**，而 RL 能够突破这些限制。

以下是详细的深度解析：

### 1. 核心差异：模仿 vs. 探索 (Imitation vs. Exploration)

* **SFT（模仿学习）：**
* **本质：** SFT 的本质是**行为克隆（Behavioral Cloning）**。你给模型看一个标准的解题步骤，模型会尽力去记住这个步骤的每一步。
* **局限：** 模型只能学会“像”标准答案那样去推理。如果遇到训练数据里没见过的难题，模型往往不知道如何变通。


* **RL（探索学习）：**
* **本质：** RL 是**试错学习（Trial and Error）**。如果你给模型一个数学题，它尝试了 10 种不同的解法，最后第 10 种算出了正确答案，RL 算法就会强化第 10 种路径。
* **优势：** 这种机制允许模型在训练过程中**探索**出人类标注数据中未曾包含的、甚至更优的解题路径。



### 2. 数据瓶颈：验证容易，生成难 (Verification is cheaper than Generation)

这是 RL 在推理领域能 Scale Up（扩展）的最重要原因。

* **SFT 的痛点：** 要提升推理能力，你需要大量高质量的 Chain-of-Thought (CoT) 数据。这要求高水平的人类专家详细写出每一步思考过程。**这非常昂贵，且很难量产。**
* **RL 的红利：** 对于数学和代码问题，**判断答案对错（验证）非常容易**，但**写出解题过程很累**。
* 你只需要给模型题目和最终答案（或者一个代码编译器）。
* 模型自己生成 100 个推理过程，只要最终答案对了（或者代码跑通了），这个推理过程就被视为“正样本”。
* 这意味着 RL 可以利用海量的题目（只有题干和答案），自动生成并筛选出海量的推理数据，不再依赖昂贵的人类手写过程。



### 3. 打破“人类天花板” (Surpassing the Human Upper Bound)

* **SFT 的天花板：** 在 SFT 中，模型最好的表现也就是无限接近人类标注员的水平。如果人类专家自己都解不出这道题，或者解法不够好，模型是学不到更高级的逻辑的。
* **RL 的超越：** 就像 AlphaGo 战胜人类一样，RL 允许模型通过自我博弈或大规模搜索，发现人类未曾教过的复杂逻辑模式。DeepSeek-R1 和 OpenAI o1 的核心逻辑就是让模型在 RL 过程中学会**Self-correction（自我修正）**和**Backtracking（回溯）**，这些是人类很难在 SFT 数据中显式教给模型的。

### 4. 容错率与过程优化 (Process Optimization)

你提到的“负样本”是一个方面，但更重要的是**“轨迹（Trajectory）优化”**。

* **SFT 的脆性：** SFT 通常使用 Next Token Prediction（预测下一个词）。如果在推理的长链条中，模型中间走错了一步，后面的所有推理可能都会崩塌（Error Accumulation），但 SFT 训练时很难纠正这一点，因为它只学每一步的概率。
* **RL 的全局观：** RL 关注的是**最终收益（Reward）**。模型会意识到：“虽然我中间这句话说得很通顺，但它导致我最后答案错了，所以这整个推理路径是坏的。” RL 会惩罚这种“一本正经胡说八道”的路径，奖励那些通过不断的自我反思最终达到正确结果的路径。

### 5. 回答你的具体假设

* **是提供了负样本吗？**
* **是。** SFT 数据集通常只有“正确答案”。而 RL 在训练过程中，模型生成的错误答案（负样本）提供了极具价值的梯度信号——告诉模型“这条路走不通”。这对于逻辑推理这种非黑即白的任务至关重要。


* **是增加了多样性吗？**
* **是。** SFT 倾向于让模型收敛到一种标准的回答模式（Mode Collapse）。而 RL（特别是在探索阶段）鼓励模型尝试不同的思维链路。最近的研究（如 DeepSeek-R1）表明，通过 RL，模型自发涌现出了“验证”、“长考”、“重新审视题目”等行为，这些都是 SFT 很难直接灌输的多样化思维模式。



### 总结：为什么是“SFT 冷启动 + RL 提升”？

目前的最佳实践（如 DeepSeek-R1, OpenAI o1）通常遵循这个范式：

1. **SFT (Cold Start)：** 用少量高质量数据，教会模型**“基本格式”**和**“如何开始思考”**（比如学会使用 `<think>` 标签）。如果不做这步，模型可能连题都读不懂，或者输出乱码，RL 根本转不起来。
2. **RL (Reasoning Scale-up)：** 在冷启动的基础上，利用 PPO 或 GRPO 等算法，配合大规模的题目（Math/Code），让模型在数万次的试错中，自己“悟”出深刻的逻辑推理能力。

**一句话总结：SFT 是教模型“守规矩”，RL 是教模型“动脑子”。**

---

### Would you like me to ...

**我可以为你详细解释一下 DeepSeek-R1 论文中提到的 GRPO (Group Relative Policy Optimization) 是如何具体实现这一过程的吗？** 或者我们可以探讨一下为什么 RL 在“创意写作”领域不如在“逻辑推理”领域效果显著？

---

### 🧑‍💻 User

DeepSeek-R1那篇论文里有一个让我印象深刻的现象——模型在RL过程中"涌现"出了很多没有被显式教过的能力。比如自我反思、比如回溯验证、比如分解问题......这些能力在SFT数据里可能出现过，但并不是每条数据都有。而RL训练之后，模型开始自发地、一致地使用这些策略。为什么RL不需要像SFT那样你给他提供推理链条，为什么RL让模型有了一种自我演化的能力？

---

### 🤖 Assistant

这是一个非常精彩的视角。你观察到的这个现象——**“涌现（Emergence）”**，正是 DeepSeek-R1 乃至 OpenAI o1 最迷人、也是最反直觉的地方。

为什么不需要教它“如何回溯”，它却自己学会了回溯？为什么不需要教它“验算”，它却学会了验算？

根本原因在于：**SFT 优化的是“拟合度（Likelihood）”，而 RL 优化的是“成功率（Success Rate）”。**

为了达成“解对难题”这个目标，**复杂的推理策略（如反思、回溯、分解）是数学上的必然最优解，而不是人类强加的规则。**

以下是深度解析，这种“自我演化”背后的原理：

### 1. 幸存者偏差：复杂的路径才能活下来 (Selection Pressure)

想象你在训练模型解一道极难的奥数题。

* **探索阶段（Rollout）：** RL 允许模型生成成千上万种不同的思维路径（Chain-of-Thought）。
* **路径 A（直线思维）：** 读题 -> 直接列式 -> 算出答案（错误）。
* **路径 B（简单推理）：** 读题 -> 简单分析 -> 算出答案（错误）。
* **路径 C（犹豫徘徊）：** 读题 -> 试了一个方法 -> **发现不对（Self-correction）** -> **退回去（Backtracking）** -> 换个方法 -> 算出答案（**正确！**）。


* **奖励阶段（Reward）：**
* 因为只有路径 C 算对了答案，RL 算法给予路径 C 极高的正向奖励。
* 路径 A 和 B 被惩罚或忽略。



**关键点：** 模型并不知道什么是“回溯”，它只知道：**“每次我生成这种‘写了一半退回去重写’的模式时，我最后都能拿高分。”** 于是，这种“回溯”的行为被梯度下降（Gradient Descent）刻进了模型的参数里。

### 2. 功能性趋同：从“形式”到“功能” (Form vs. Function)

* **SFT 的局限（学其形）：**
* 在 SFT 数据里，标注员写了“让我重新检查一下”。模型学会的是这句话的**字面概率**。它可能在不该检查的时候也瞎写一句“让我检查一下”，纯粹是为了模仿句式。


* **RL 的突破（得其神）：**
* 在 RL 中，模型学会的是**功能**。如果模型写了“让我检查一下”但实际上并没有执行检查逻辑，由于这浪费了推理长度且没提高正确率，可能会被惩罚（如果加入了长度惩罚项）。
* 只有当“检查”这个行为真正修正了前面的错误，从而导致正确结果时，这个行为才会被强化。
* **结论：** 模型学会的不是人类的“语气”，而是解决复杂问题的**通用算法结构**。反思和分解问题，是解决复杂逻辑问题的**必经之路**，模型通过试错找到了这条路。



### 3. 测试时计算 Scaling：用长度换精度 (Trading Length for Accuracy)

DeepSeek-R1 论文中展示了一个核心发现：**RL 训练使得生成的 CoT 长度显著增加。**

* **为什么变长？** 并不是模型变啰嗦了，而是它学会了**通过增加计算步数来降低错误率**。
* **思维的“草稿纸”：**
* 简单问题，模型几步就走完了。
* 复杂问题，直接跳跃很容易错。RL 迫使模型学会：“如果我多写几步，把中间变量写在纸上（输出在 Context 里），我就不容易错。”
* 这导致了**Thinking Process（思维流）**的自然涌现。那些看似多余的“Wait, let me think...”其实是模型在争取计算时间和注意力机制的重新分配。



### 4. 摆脱人类的认知局限 (Beyond Human Bias)

这是一个非常重要的点：**人类写不出完美的思维链。**

* 人类专家在做 SFT 标注时，往往会**省略**很多潜意识里的思考步骤（跳步）。
* 或者，人类的解法偏向于某种特定的套路。
* **RL 的自由度：** RL 不需要你提供过程，只要结果。这给了模型极大的自由度去**发明**适合它自己神经网络结构的思维方式。
* 也许对于 Transformer 架构来说，先列出所有变量，再进行某种奇怪的排序，更有利于它提取特征。
* 这种“非人类”但“有效”的推理路径，在 RL 中会被保留，而在 SFT 中会被视为“不符合人类规范”而被修正掉。



### 5. 著名的 "Aha! Moment" (顿悟时刻)

DeepSeek-R1 团队在训练中观察到了一个有趣的中间检查点：

> 模型在没有任何监督的情况下，学会了在生成长篇大论后，突然加一句：**“等一下，我刚才好像算错了，在这个公式里应该...”**

这并不是因为它看过人类这么说话，而是因为在数万次的随机采样中，**这种“由于自我质疑而修正了错误”的样本，恰好对应了最高的奖励值。** 这种机制一旦被捕捉，就会迅速在模型中泛化。

---

### 总结

为什么 RL 让模型拥有了“自我演化”的能力？

因为 **RL 将“推理策略”的搜索权交还给了模型自己。**

* **SFT** 是填鸭式教育：老师教你怎么做，你就怎么做，不求甚解。
* **RL** 是生存训练：把你扔进题海，只有做对了才有饭吃。为了生存（获得奖励），模型**被迫**进化出了反思、回溯、验证等高级生存技能。

这解释了为什么现在的趋势是 **SFT 只做冷启动（让模型听懂人话），重头戏全在 RL（让模型学会思考）。**

---

### Would you like me to ...

**Would you like me to dive deeper into the "DeepSeek-R1-Zero" experiment?** That was a specific experiment where they skipped SFT entirely and applied RL directly to the base model, resulting in some very fascinating (and chaotic) "alien" reasoning patterns.

---

### 🧑‍💻 User

不过，SFT并非不重要且不好用，它现在更多变成了reasoning RL的一个“前期过程”。如果训练直接上RL是很容易翻车的。这就是为什么现在的主流做法都是"SFT冷启动 + RL微调"。既然RL是需要有意义的探索，SFT是如何提供一个合理的起点？

---

### 🤖 Assistant

你的理解非常精准。**“SFT 冷启动 + RL 微调”** 确实是目前大模型推理（Reasoning）的标准范式。

如果不做 SFT 直接上 RL（就像 DeepSeek-R1-Zero 做的那样），虽然理论上可行，但实际工程中往往会面临**收敛极慢、输出格式混乱、语言退化（不说人话）**等严重问题。

SFT 在这个阶段的作用，可以用一句话概括：**它为 RL 提供了一个“非零的起点”和“可控的搜索空间”。**

以下是 SFT 如何为 RL 铺路的详细技术解析：

### 1. 解决“稀疏奖励（Sparse Reward）”的冷启动难题

这是 SFT 最核心的作用。

* **RL 的死穴：** RL 依赖于“做对了 -> 给糖吃”的反馈循环。如果你把一个只会续写小说的基座模型（Base Model）扔去做高数题，它可能尝试 10,000 次，**一次都做不对**。
* 结果：正确率 = 0%，Reward 全是 0 或负分。
* 后果：梯度不更新，或者在错误的随机噪声中震荡，模型根本学不到东西。


* **SFT 的救场：** SFT 哪怕只让模型只有 10% 的正确率，也至关重要。
* SFT 教会模型：“大概沿着这个方向走，是有可能对的”。
* 一旦有了这 10% 的正确样本，RL 算法（如 PPO 或 GRPO）就能捕捉到这些成功的轨迹，通过正向激励放大这些信号。**从 0 到 0.1 是最难的，SFT 完成了这一步；从 0.1 到 0.9，才是 RL 的强项。**



### 2. 规范输出格式（Formatting & Structure）

RL 算法通常依赖于自动化验证（比如检查 `<answer>` 标签里的数字对不对）。

* **没有 SFT 的惨状：** 纯 Base Model 可能会在输出里天马行空，一会儿写代码，一会儿写诗，或者把答案藏在一段废话中间，甚至不写结束符一直在那里复读。验证脚本根本提取不出答案，导致全是“误判为负样本”。
* **SFT 的约束：** SFT 就像是给学生发了一张**“答题卡”**。
* 它教会模型强制遵循特定的结构，比如 DeepSeek-R1 的 `<think>`...`</think>`...`<answer>`...`</answer>`。
* 这种结构化的输出，让 RL 的 Reward Model 能够精准地定位答案，进行自动化打分。没有这个“壳”，RL 的自动化验证流水线就跑不通。



### 3. 压缩搜索空间（Narrowing Search Space）

大模型的词表空间是巨大的（数万个 token），可能的组合是天文数字。

* **Base Model 的发散性：** 对于“1+1=？”这个问题，Base Model 的高概率预测可能是“2”，但也可能是“是小学数学题”、“很难”或者“Wang et al., 2020”。
* **SFT 的聚焦：** SFT 调整了模型的概率分布（Logits），极大地提高了“推理相关词汇”和“逻辑连接词”（如 "Therefore", "Because", "Let's assume"）的采样概率。
* 这实际上是将 RL 需要探索的“荒原”缩小成了一条“小径”。模型不需要去探索“如何写诗能解出数学题”，而是专注于“如何组合逻辑步骤能解出数学题”。这极大地提高了 RL 的训练效率。



### 4. 锚定人类语言偏好（Alignment & Readability）

DeepSeek-R1-Zero 的实验揭示了一个有趣的现象：**纯 RL 训练出来的模型，虽然题做对了，但“不说人话”。**

* **语言混杂：** 它可能会在一个句子里混用中文、英文和西班牙语，因为对模型来说，只要能推导出答案，用什么语言符号并不重要。
* **思维混乱：** 它的思考过程可能极度跳跃，包含只有它自己能看懂的“加密通话”或重复模式。
* **SFT 的作用：** SFT 就像是**“语言矫正”**。它强迫模型使用人类可读、逻辑连贯的自然语言进行思考。
* 在 RL 过程中，为了防止模型“飘”得太远，通常会引入 **KL 散度（KL Divergence）惩罚**。
* 这意味着：RL 模型（Policy Model）的输出分布，不能偏离 SFT 模型（Reference Model）太远。
* **SFT 模型充当了一个“锚点”，拽着 RL 模型，不让它为了追求高分而变成一个只会输出乱码的解题机器。**



### 5. 激发潜在能力（Activation）

有些能力在预训练阶段（Pre-training）已经埋在模型参数里了，但处于“休眠”状态。

* SFT 数据集（比如几千条高质量的 CoT 数据）像是一个**“启动代码”**。它不需要教给模型所有的知识，而是通过少量的示例，**激活**模型在预训练中学到的逻辑推理模式。
* 一旦被激活，RL 就可以在此基础上进行强化和扩展。如果没有 SFT 的激活，模型可能根本不知道自己具备“分步思考”的能力。

### 总结

我们可以把**大模型推理能力的训练**比作**“训练一个跳高运动员”**：

1. **Pre-training (基座):** 这个人身体素质极好（有知识储备），但他不知道什么叫跳高，甚至可能在操场上乱跑。
2. **SFT (冷启动):** 教练做示范，手把手教他：“你要助跑，要起跳，要背越式过杆”。（规范动作、确立目标、确立基础成功率）。
3. **RL (推理提升):** 运动员自己练习 10 万次。他尝试了各种起跳角度和力度。有的摔了（负反馈），有的跳过去了（正反馈）。慢慢地，他摸索出了一套比教练教的更完美的、微小的肌肉发力技巧（涌现出的高级策略），打破了世界纪录。

**没有 SFT，运动员可能在沙坑里玩了一整天泥巴，根本不知道要往杆上跳。**

---

### Would you like me to ...

**既然 SFT 如此重要，你是否想了解一下：为了这关键的“冷启动”，目前业界是如何构造那些高质量的 Chain-of-Thought (CoT) 数据的？**（这通常涉及“蒸馏”和“人工精注”的某种结合）。