# DeepSpeed ZeRO优化器三个阶段解析

### 🧑‍💻 User

DeepSpeed的ZeRO，零冗余优化器的三个阶段分别是什么？

---

### 🤖 Assistant

DeepSpeed 的 **ZeRO（Zero Redundancy Optimizer）** 通过分阶段消除冗余数据（如优化器状态、梯度、模型参数的冗余存储）来优化大规模模型训练的显存效率。其核心思想是将数据并行训练中的冗余部分逐步消除，并分为三个递进的阶段。以下是每个阶段的详细说明：

---

### **1. ZeRO-1（优化器状态分片，Optimizer State Partitioning）**
- **目标**：减少每个设备上存储的优化器状态（如 Adam 优化器中的动量、二阶矩等）的冗余。
- **实现方式**：
  - 将优化器状态（如动量、方差等）按数据并行的设备数量进行分片（例如，16 个设备则每个设备存储 1/16 的优化器状态）。
  - 每个设备仅保存与自身计算梯度对应的优化器状态部分。
- **效果**：显存占用降低至原来的 $1/N$（N 为设备数量），但梯度和模型参数的冗余仍然存在。

---

### **2. ZeRO-2（梯度分片，Gradient Partitioning）**
- **目标**：在 ZeRO-1 的基础上，进一步消除梯度存储的冗余。
- **实现方式**：
  - 将梯度（梯度张量）按设备数量进行分片，每个设备仅存储与自己负责的梯度部分相关的梯度。
  - 在参数更新时，通过通信操作（如 AllReduce）聚合梯度，确保模型参数更新的准确性。
- **效果**：进一步将显存占用降低至 $1/N$（N 为设备数量），但需要增加设备间通信的开销。

---

### **3. ZeRO-3（参数分片，Parameter Partitioning）**
- **目标**：消除模型参数的冗余存储，进一步降低单卡的内存需求。
- **实现方式**：
  - 将模型参数按设备数量进行分片，每个设备仅存储和计算自己负责的参数部分。
  - 在前向和反向传播时，通过通信操作将参数动态同步到所需设备（例如，需要时将其他卡的参数拉取到本地进行计算）。
- **效果**：显存占用降至 $1/N$（N 为设备数量），但需要更复杂的通信和同步逻辑。

---

### **阶段递进关系**
ZeRO 的三个阶段是 **递进式** 的：
1. **ZeRO-1** 解决了优化器状态的冗余（基础阶段）。
2. **ZeRO-2** 在 ZeRO-1 的基础上进一步优化了梯度存储。
3. **ZeRO-3** 在 ZeRO-2 的基础上进一步优化了模型参数的存储。

---

### **实际应用中的权衡**
- **ZeRO-1**：通信量与数据并行相同（需在更新前同步优化器状态），但内存节省有限。
- **ZeRO-2**：需要更复杂的梯度同步，但能进一步降低内存占用。
- **ZeRO-3**：内存节省最多，但需要动态参数同步，可能增加通信量。

---

### **总结**
| **阶段**       | 优化目标         | 冗余消除对象       | 显存复杂度         | 通信需求           |
|----------------|------------------|--------------------|--------------------|--------------------|
| **ZeRO-1**     | 优化器状态分片   | 优化器状态         | $O(1/N)$         | 中等（优化器状态） |
| **ZeRO-2**     | 梯度分片         | 优化器状态 + 梯度  | $O(1/N^2)$       | 高（梯度同步）     |
| **ZeRO-3**     | 参数分片         | 优化器状态 + 梯度 + 参数 | $O(1/N^3)$     | 极高（参数同步）   |

通过这三个阶段的递进优化，DeepSpeed 的 ZeRO 技术能够显著降低大规模模型训练的显存需求，从而支持更大模型的训练或更高的训练吞吐量。