# SGD 范数最小化解

### 🧑‍💻 User

sgd为什么已经被证明能够得到范数最小化的解决方案？

---

### 🤖 Assistant

这是一个非常深刻且处于深度学习理论前沿的问题。

简单来说，SGD（随机梯度下降）之所以被证明能够收敛到范数最小（Minimum Norm）的解决方案，主要归功于**隐式正则化（Implicit Regularization）**或**隐式偏差（Implicit Bias）**。

在过参数化（Over-parameterized）的场景下（即参数数量远多于训练数据量），存在无穷多个解可以使训练误差为零。然而，即使没有任何显式的正则化项（如 $L_1$ 或 $L_2$ 正则化），SGD 依然倾向于收敛到其中具有特定简单性（通常是最小范数）的解。

这一现象主要在以下两种经典情况中得到了严格的数学证明：

### 1. 线性回归（平方损失）：收敛到最小 $L_2$ 范数解

这是最直观且最容易证明的情况。

**设定：**
考虑一个过参数化的线性回归问题，目标是最小化 $L(w) = \frac{1}{2}\|Xw - y\|^2$。由于参数多于数据，方程 $Xw=y$ 有无穷多个解。

**原因分析：**
SGD（或 GD）的更新公式为：
$$w_{t+1} = w_t - \eta \nabla L(w_t)$$
对于平方损失，梯度 $\nabla L(w)$ 是数据点 $x_i$ 的线性组合：
$$\nabla L(w) = X^T(Xw - y) = \sum_{i=1}^{n} (x_i^T w - y_i) x_i$$

这意味着，梯度的方向总是位于数据矩阵 $X$ 的**行空间（Row Space）**内（即由数据样本 $x_i$ 张成的空间）。

**结论：**
*   如果在初始化时 $w_0 = 0$（或者 $w_0$ 在行空间内），那么所有的更新量 $-\eta \nabla L$ 也都在行空间内。
*   因此，最终收敛的解 $w^*$ 也必须位于行空间内。
*   线性代数告诉我们，在满足 $Xw=y$ 的所有解中，**唯一**位于 $X$ 行空间内的解，就是**欧几里得范数（$L_2$ Norm）最小的解**。

**数学表达：**
$$w^* = \arg\min_w \|w\|_2 \quad \text{s.t.} \quad Xw = y$$

### 2. 线性分类（指数族损失）：收敛到最大间隔解

对于逻辑回归（Logistic Regression）或使用交叉熵损失的线性分类问题，情况略有不同，但本质相似。Soudry 等人（2018）证明了这一重要结论。

**设定：**
针对线性可分数据，使用梯度下降优化指数损失（Exponential Loss）或逻辑损失。由于数据可分，为了使损失函数趋近于 0，权重向量 $w$ 的范数 $\|w\|$ 会趋向于无穷大。

**原因分析：**
虽然 $\|w\|$ 会爆炸，但在无限时间极限下，我们可以考察归一化的方向向量 $\frac{w(t)}{\|w(t)\|}$。
研究证明，梯度下降会沿着特定的路径使得损失下降最快。在分类问题中，这等价于寻找能将两类数据分得“最开”的边界。

**结论：**
SGD 收敛的方向与**硬间隔支持向量机（Hard Margin SVM）**的解一致。
SVM 的对偶问题本质上就是在满足分类正确的前提下，最小化 $\|w\|_2$。
因此，SGD 在分类任务中收敛到了**最大间隔（Max-Margin）**解，这对应于在满足间隔约束下的**最小 $L_2$ 范数**解。

### 3. 为什么是“隐式”的？

之所以称为“隐式”，是因为我们并没有在损失函数中添加 $\lambda \|w\|^2$ 这样的项。这种正则化效果完全来自于**算法本身（SGD/GD）**与**初始化策略**的相互作用。

*   **初始化的重要性：** 如果初始化 $w_0$ 非常大且随机，SGD 可能会收敛到离 $w_0$ 最近的某个解，而不一定是全局范数最小的解。通常要求初始化接近于 0。
*   **几何直觉：** 在参数空间中，SGD 走的是“最速下降”路径。如果在原点附近开始，并且没有“力”（梯度）推动它去往数据张成空间之外的方向（零空间），它就不会去那里。因此，它只在必要的维度上通过最短路径到达零损失流形。

### 4. 深度神经网络中的推广（矩阵分解与卷积）

对于非线性的深度网络，情况变得更加复杂，但核心直觉依然存在：

*   **矩阵分解（Matrix Factorization）：** Gunasekar 等人证明，在矩阵分解任务中，使用梯度下降求解 $U V^T = Y$，倾向于得到**核范数（Nuclear Norm）**最小的解，而不是 Frobenius 范数最小的解。这解释了为什么深度学习能学到低秩表示。
*   **卷积神经网络：** 类似于线性模型中的 $L_2$ 范数，对于具有特定架构的网络，SGD 偏向于在某些特定的函数空间范数下的最小解。

### 总结

SGD 被证明能得到范数最小化解决方案，核心原因在于**优化路径的几何特性**：

1.  **对于回归（$L_2$ 损失）：** 梯度总是位于数据的线性跨度（Span）中。从零出发，权重永远不会包含与数据正交的“无用”分量（即零空间分量），从而保证了 $L_2$ 范数最小。
2.  **对于分类（Logistic 损失）：** 梯度下降的动力学驱使决策边界逐渐对齐到最大间隔方向（SVM 解），这等价于最小化权重的 $L_2$ 范数。

这就是为什么深度学习模型虽然参数巨大，却不容易过拟合，并且具有良好泛化能力的理论基石之一。

---

### 🧑‍💻 User

为什么在合适 (线性 / 过参数且可插值) 的条件下，SGD 的解 ≡ 最小-范数 (minimum-norm) 插值解。

---

### 🤖 Assistant

这个问题可以从**线性代数（正交分解）**和**动力学（梯度方向）**两个角度来严格且直观地解释。

核心结论是：**在初始化为零（$w_0=0$）的情况下，SGD 的每一次迭代更新都限制在数据矩阵的“行空间”内，永远不会触及与数据无关的“零空间”。**

以下是分步推导：

### 1. 问题的几何设定

假设我们要解决一个过参数化的线性回归问题（$d > n$，即特征数 > 样本数）：
*   数据集：$X \in \mathbb{R}^{n \times d}$，标签 $y \in \mathbb{R}^n$。
*   模型：$f(x) = w^T x$。
*   损失函数（平方损失）：$L(w) = \frac{1}{2} \|Xw - y\|^2_2 = \frac{1}{2} \sum_{i=1}^n (w^T x_i - y_i)^2$。
*   **插值条件**：由于是过参数化系统，方程 $Xw=y$ 有无穷多个解。所有的解构成了一个仿射子空间（Solution Manifold）。

### 2. 空间的正交分解（关键点）

根据线性代数基本定理，参数空间 $\mathbb{R}^d$ 可以分解为两个正交子空间：

1.  **行空间 (Row Space, $\mathcal{R}(X^T)$)**：由数据样本 $\{x_1, ..., x_n\}$ 张成的空间。
    $$ \text{Span}(x_1, \dots, x_n) $$
2.  **零空间 (Null Space, $\mathcal{N}(X)$)**：所有与数据正交的方向。如果 $v$ 在零空间中，那么 $Xv = 0$。

任意一个权重向量 $w$ 都可以唯一分解为：
$$ w = w_{\text{row}} + w_{\text{null}} $$
其中 $w_{\text{row}} \in \mathcal{R}(X^T)$， $w_{\text{null}} \in \mathcal{N}(X)$。

**为什么范数最小解必须在行空间里？**
观察 $Xw = y$：
$$ X(w_{\text{row}} + w_{\text{null}}) = y \implies X w_{\text{row}} + 0 = y $$
这说明，$w_{\text{null}}$ 对预测结果没有任何贡献，它不影响损失函数的值。但是，它会增加范数：
$$ \|w\|^2 = \|w_{\text{row}}\|^2 + \|w_{\text{null}}\|^2 $$
显然，为了最小化范数 $\|w\|^2$，我们必须让 **$w_{\text{null}} = 0$**。

**结论：** 最小范数解 $w^*$ 是那个**完全位于行空间内**，且满足 $Xw=y$ 的唯一解。

---

### 3. SGD 的动力学证明

现在我们要证明：**SGD 算法是如何保证 $w_{\text{null}}$ 始终为 0 的。**

#### 步骤 A: 梯度的形式
SGD 在第 $t$ 步随机选取一个样本 $(x_{i_t}, y_{i_t})$，损失为 $l_t(w) = \frac{1}{2}(w^T x_{i_t} - y_{i_t})^2$。
其梯度为：
$$ \nabla l_t(w) = (w^T x_{i_t} - y_{i_t}) \cdot x_{i_t} $$

**关键观察：** 梯度向量 $\nabla l_t(w)$ 是样本向量 $x_{i_t}$ 的标量倍。这意味着**梯度永远位于行空间内**。

#### 步骤 B: 迭代更新
SGD 的更新规则是：
$$ w_{t+1} = w_t - \eta \nabla l_t(w_t) $$
$$ w_{t+1} = w_t - \eta \cdot (\text{scalar} \cdot x_{i_t}) $$

#### 步骤 C: 数学归纳法
1.  **初始化**：假设我们从原点出发，即 $w_0 = 0$。显然 $w_0$ 在行空间内（它是零向量）。
2.  **递推**：假设 $w_t$ 在行空间内。
    由于更新量 $-\eta \nabla l_t(w_t)$ 是 $x_{i_t}$ 的倍数，它也在行空间内。
    根据向量空间的封闭性，行空间向量 + 行空间向量 = 行空间向量。
    因此，$w_{t+1}$ 也在行空间内。
3.  **结论**：对任意 $t$，只要 $w_0=0$，则 $w_t$ 始终正交于零空间。

### 4. 收敛时的状态

当 SGD 收敛时（假设步长合适，能够收敛到零训练误差）：
1.  **满足插值条件**：$X w_{\infty} = y$。
2.  **几何约束**：根据上述推导，$w_{\infty}$ 完全位于行空间内，即 $w_{\infty} = w_{\text{row}}$，且 $w_{\text{null}} = 0$。

结合这两点，$w_{\infty}$ 就是那个**行空间内的唯一解**。

根据第2节的推导，这正是**最小范数解 (Minimum Norm Solution)**。

数学形式上，这个解就是伪逆解：
$$ w_{\text{SGD}} = w^* = X^T (XX^T)^{-1} y $$

### 5. 总结

SGD 之所以等价于最小范数插值解，是因为：
1.  **起点**：我们从 $w=0$ 开始（最小范数点）。
2.  **路径**：梯度的方向由数据 $x$ 组成，它只能在数据张成的空间（行空间）内移动权重。
3.  **终点**：SGD 没有动力去探索正交于数据的方向（零空间），因此它在满足方程 $Xw=y$ 的所有解中，挑选了没有“多余”零空间分量的那个解。

**一句话概括：因为梯度在零空间上的投影永远为 0，所以初始为 0 的权重永远不会产生零空间分量，从而保持了最小范数。**