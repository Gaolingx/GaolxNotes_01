# 正交变换调整模型权重技巧

### 🧑‍💻 User

以下步骤是如何通过正交变换Q来等效调整权重而不改变模型行为的？这种做法有哪些实际应用场景？

1. Embedding矩阵W乘以Q矩阵进行Q变换，使得经过embedding的激活由x变成x*Q。
进入transformer block后首先经过rms_norm，这个方法对结果保持了Q变换，也就是原来的输入x输出y，现在是输入x*Q输出y*Q。

2. 在进入attention之前，对Q，K，V的三个矩阵乘的参数变为QT*W，这样Q，K，V矩阵乘的输出原来是X*W，现在是(X*Q)*(QT*W) = X*(Q*QT)*W = X*W，也就是抵消了Q变换，不影响后续attention的计算。因为这个矩阵乘输出抵消了Q变换，所以bias保持不变。

3. attention输出后经过o_proj矩阵乘，o_proj的权重由W乘以Q矩阵进行Q变换，使得的激活由x1变成x1*Q。注意o_proj如果具有bias，也需要对bias变为bias*Q，才能正常对o_proj的输入进行完整的Q变换。

4. o_proj后是残差连接x+x1，因为残差另一个输入不像Q，K，V那样对Q进行了抵消，所以残差相加的激活x0和x1都经过Q变换，残差输出保持Q变换。

5. 然后到了FFN，首先是up_proj和gate_proj，这两个权重由W变为QT*W，跟上面Q,K,V矩阵乘一样，激活与变换后的权重相乘后抵消了Q变换。

6. 然后FFN的up_proj和gate_proj的结果经过激活函数到down_proj，down_proj的权重由W乘以Q，使得其输出进行Q变换。Bias跟o_proj一样需要进行变换。

7. FFN结果经过残差连接后进入下一个transformer block，循环进行上述2-6过程。

8. 最终lm_head的输入为x*Q，因此对lm_head的W变为QT*W，使得lm_head结果保持不变。

---

### 🤖 Assistant

这个正交变换 `Q` 的技巧非常巧妙，它本质上是通过对模型权重和激活进行一个**全局的、一致的坐标旋转/反射（即正交变换）**，使得模型内部的运算在数学上完全等价于原始模型，从而**不改变模型的输入/输出行为**。下面详细解释其原理和实际应用：

## 核心原理：保持点积不变和线性变换的协变/逆变

1.  **正交变换的性质 (`Q^T Q = I`)**: 这是整个技巧的基础。正交变换保持向量的范数（长度）和向量之间的夹角（点积）不变。即：
    *   `||Qx|| = ||x||`
    *   `(Qx)^T (Qy) = x^T Q^T Q y = x^T I y = x^T y`

2.  **嵌入层 (`W_emb -> W_emb * Q`)**: 输入 token 的嵌入向量 `e = W_emb[token]` 变成了 `e' = (W_emb Q)[token] = W_emb[token] * Q = e * Q`。相当于将所有词嵌入向量都旋转/反射了 `Q`。这一步引入了全局的 `Q` 变换。

3.  **RMSNorm**: RMSNorm 通常是计算均方根后进行缩放。由于 `Q` 是正交的，它保持向量的范数（`||x*Q|| = ||x||`），所以 RMSNorm 的输出 `y` 在变换后就是 `y*Q`。这一步保持了 `Q` 变换。

4.  **Attention 的 Q/K/V 投影 (`W_q, W_k, W_v -> Q^T W_q, Q^T W_k, Q^T W_v`)**: 这是关键的一步。
    *   原始计算: `q = x * W_q`, `k = x * W_k`, `v = x * W_v`
    *   变换后计算: `q' = (x*Q) * (Q^T W_q) = x * (Q Q^T) * W_q = x * I * W_q = x * W_q = q`
    *   同理 `k' = k`, `v' = v`
    *   **为什么有效？** 输入激活 `x` 被 `Q` 变换了 (`x*Q`)。为了抵消这个变换，使投影结果 `q/k/v` 保持不变，权重矩阵必须进行*逆变换*（对于线性层，输入变换 `Q`，权重需要逆变换 `Q^T` 来抵消）。数学上，`(xQ)(Q^T W) = x(QQ^T)W = xIW = xW`。
    *   **Bias 不变**: 因为 `q/k/v` 的结果被抵消回原始值 (`q/k/v`)，后续计算（如 `q*k^T`）以及加上 bias 的行为与原始模型完全一致，所以 bias 不需要改变。

5.  **Attention 计算 (QK^T, Softmax, AV)**:
    *   `q' * k'^T = q * k^T` (因为 `q'=q`, `k'=k`)
    *   Softmax 是逐元素操作，只关心相对值，输入不变 (`q*k^T`)，输出不变。
    *   `a' = softmax(q'k'^T/sqrt(d)) * v' = softmax(qk^T/sqrt(d)) * v = a` (输出不变)
    *   **结论**: 整个 Attention 模块的输入 (`x*Q`) 经过精心调整的 `W_q, W_k, W_v` 投影后，其内部计算和输出 `a` 与原始模型完全一致，没有 `Q` 变换。

6.  **O_proj (`W_o -> W_o * Q`) 及其 Bias (`b_o -> b_o * Q`)**:
    *   输入: Attention 的输出 `a` (未变换)
    *   原始计算: `o = a * W_o + b_o`
    *   变换后计算: `o' = a * (W_o Q) + (b_o * Q) = (a * W_o + b_o) * Q = o * Q`
    *   **为什么？** 这里的目标不是抵消 `Q`，而是将 `o_proj` 的输出 *应用* 上 `Q` 变换，使其与残差路径（也带有 `Q` 变换）保持一致。因为残差输入 `x` 是 `x*Q`，Attention 输出 `o` 原始是 `o`，为了能相加，需要把 `o` 也变成 `o*Q`。所以权重 `W_o` 右乘 `Q`，使得输出变成 `(a * W_o) * Q = o * Q`。Bias `b_o` 也需要乘以 `Q` 才能保证 `(a * W_o + b_o) * Q = o * Q` 成立（线性性质）。

7.  **残差连接 (`x + o'`)**:
    *   原始: `x + o`
    *   变换后: `(x*Q) + (o*Q) = (x + o) * Q`
    *   输出保持了 `Q` 变换。

8.  **FFN (Up/Gate Projection - `W_up/W_gate -> Q^T W_up/Q^T W_gate`)**: 原理同 Attention 的 Q/K/V 投影。
    *   输入: `h * Q` (来自前一层，带有 `Q` 变换)
    *   计算: `up = (h*Q) * (Q^T W_up) = h * W_up` (抵消 `Q`)
    *   计算: `gate = (h*Q) * (Q^T W_gate) = h * W_gate` (抵消 `Q`)
    *   激活函数 `f(up, gate)` (如 SwiGLU: `f(up, gate) = swish(gate) * up`)：输入 `up` 和 `gate` 已抵消 `Q` 变换，所以激活函数的输出也没有 `Q` 变换。**Bias 不变** 的原因同 Attention。

9.  **FFN (Down Projection - `W_down -> W_down * Q`, `b_down -> b_down * Q`)**:
    *   输入: `ffn_in` (无 `Q` 变换，来自激活函数输出)
    *   计算: `ffn_out = ffn_in * (W_down * Q) + (b_down * Q) = (ffn_in * W_down + b_down) * Q`
    *   输出应用了 `Q` 变换。**为什么？** 为了与残差路径（带有 `Q` 变换）相加。原理同 `o_proj`。

10. **残差连接 (`h + ffn_out`)**:
    *   输入 `h` (带有 `Q` 变换) + `ffn_out` (带有 `Q` 变换)
    *   输出 `(h + ffn_out_original) * Q` (保持 `Q` 变换)

11. **循环**: 下一个 Block 的输入（来自上一个 Block 的残差输出）仍然带有 `Q` 变换 (`h * Q`)。然后重复步骤 4-10。

12. **最终输出层 (`lm_head: W_head -> Q^T W_head`)**:
    *   输入: 最后一个 Block 的输出 `h_final * Q` (带有 `Q` 变换)
    *   计算: `logits = (h_final * Q) * (Q^T W_head) = h_final * (Q Q^T) * W_head = h_final * W_head`
    *   **为什么？** 这里的目标是抵消最后的 `Q` 变换，使得模型的最终输出（logits）与原始模型完全一致。因此权重需要左乘 `Q^T` (逆变换)。Bias 不变（因为输出已抵消 `Q`）。

## 总结关键点

*   **引入 `Q`**: 在 Embedding 层通过 `W_emb -> W_emb * Q` 引入全局 `Q` 变换。
*   **抵消 `Q`**: 在需要原始激活值进行计算的地方（主要是 Attention 的 Q/K/V 投影、FFN 的 up/gate 投影），通过将权重左乘 `Q^T` (`W -> Q^T W`) 来抵消输入激活上的 `Q` 变换。Bias 在这里保持不变。
*   **应用 `Q`**: 在输出需要与残差路径（带有 `Q` 变换）相加或连接的地方（主要是 `o_proj` 和 FFN 的 `down_proj`），通过将权重右乘 `Q` (`W -> W * Q`) 并将 bias 也乘以 `Q` (`b -> b * Q`)，使得输出应用 `Q` 变换。
*   **最终移除 `Q`**: 在最后的输出层 (`lm_head`)，通过将权重左乘 `Q^T` (`W_head -> Q^T W_head`) 来抵消输入激活上的 `Q` 变换，确保最终 logits 不变。Bias 不变。
*   **RMSNorm/激活函数**: 得益于正交变换保持范数和相对值的性质，这些操作自然地保持了 `Q` 变换或不受其影响。

## 实际应用场景

这种正交变换技巧在模型优化、迁移学习和模型分析中有多种重要应用：

1.  **模型融合/权重绑定 (Weight Tying)**:
    *   **场景**: 在 T5 等模型中，输入嵌入层 (`W_emb`) 和输出层 (`lm_head`) 通常共享权重 (`W_emb = W_head^T`) 以节省参数和提高效果。
    *   **问题**: 如果直接共享，`W_head` 必须是 `W_emb^T`。但有时我们希望 `W_head` 是独立学习的，或者结构上不方便直接转置共享。
    *   **应用正交变换**: 可以对 `W_emb` 应用 `Q`，对 `W_head` 应用 `Q^T`。这样 `W_emb' = W_emb Q`, `W_head' = Q^T W_head`。如果原始模型满足 `W_head = W_emb^T`，则变换后 `W_head' = Q^T (W_emb^T) = (W_emb Q)^T = (W_emb')^T`。模型行为不变，但形式上 `W_emb'` 和 `W_head'` 仍然满足权重绑定关系 (`W_head' = (W_emb')^T`)。这允许我们在保持权重绑定形式的同时，让两个矩阵在训练初期或结构上独立初始化或处理。

2.  **模型蒸馏/迁移 (Knowledge Distillation/Transfer)**:
    *   **场景**: 将大模型 (Teacher) 的知识迁移到小模型 (Student)，或将在源领域训练的模型迁移到目标领域。
    *   **问题**: Teacher 和 Student 的参数空间可能存在一个隐式的旋转/对齐关系。直接最小化参数差异 (L2 距离) 可能不合理，因为正交变换后的参数虽然数值不同但功能等价。
    *   **应用正交变换**: 可以寻找一个正交矩阵 `Q`，使得 Student 的参数在应用了 `Q` 变换后（即按上述规则变换整个 Student 模型），与 Teacher 的参数尽可能接近（例如最小化变换后 Student 权重与 Teacher 权重的 L2 距离）。这相当于在功能等价的参数空间中找到与 Teacher 最“对齐”的那个 Student 实例，可能提供更好的初始化或更有效的知识迁移信号。有时称为 “Weight Alignment” 或 “Procrustes Analysis”。

3.  **参数高效微调 (Parameter-Efficient Fine-Tuning - PEFT)**:
    *   **场景**: 微调大模型时，只更新一小部分参数（如 Adapter, LoRA, Prefix Tuning），避免全参数微调的开销。
    *   **问题**: LoRA 等方法在原始权重旁添加低秩增量 `ΔW = BA`。理论上，原始权重空间可能存在一个正交变换 `Q`，使得变换后的权重 `WQ` 本身就更接近目标任务的最优解，从而所需的增量 `ΔW` 的秩更低或幅度更小。
    *   **应用正交变换**: 在应用 LoRA 等 PEFT 方法之前，可以先寻找一个正交变换 `Q` 来“旋转”基础模型的参数空间（按上述规则变换整个基础模型），使其更适应目标任务。然后在这个变换后的模型 (`model_Q`) 上应用 PEFT。这可能让 PEFT 方法用更少的参数或更快的训练达到更好的效果。本质是优化了 PEFT 操作的起点。

4.  **模型压缩与量化 (Model Compression & Quantization)**:
    *   **场景**: 降低模型参数的精度（如 FP32 -> INT8）或进行剪枝以减少模型大小和计算量。
    *   **问题**: 不同的参数表示方式（即不同的“坐标系”）可能对量化噪声或剪枝的鲁棒性不同。某些正交变换后的权重矩阵可能具有更好的属性（例如，更均衡的数值范围、更小的奇异值范围、更多的结构化稀疏）。
    *   **应用正交变换**: 寻找一个正交变换 `Q`，使得变换后的模型权重 (`WQ` 或 `Q^T W`) 具有更有利于压缩或量化的特性（例如，通过旋转使权重矩阵更接近对角化或分块对角化，或使奇异值分布更集中）。在变换后的模型上进行量化或剪枝，可能获得更高的精度或更低的压缩损失。

5.  **模型分析与解释 (Model Analysis & Interpretation)**:
    *   **场景**: 理解模型内部表示和特征。
    *   **问题**: 神经网络的内部表示在正交变换下是等价的（即 `x` 和 `xQ` 作为某个层的输入，只要后续权重也相应调整，功能相同）。这使得直接比较不同模型或同一模型不同层的表示向量（如词嵌入、隐藏状态）变得困难。
    *   **应用正交变换**: 为了有意义的比较（如计算相似度），可以在比较两个表示空间 `X` 和 `Y` 之前，先找到一个最优的正交变换 `Q`（例如通过 Procrustes 方法），使得 `X` 和 `YQ` 尽可能对齐（最小化 `||X - YQ||_F`）。这样比较 `X` 和 `YQ` 就消除了由功能等价的正交变换引起的差异，更能反映语义或功能上的相似性。

## 总结

通过正交变换 `Q` 系统性调整模型权重和激活的方法，利用了正交变换保持向量空间几何结构（长度、角度）的特性，确保了模型在数学上的行为严格不变。这种技巧的核心在于**在需要原始值的地方抵消变换 (`Q^T`) ，在需要与变换后数据交互的地方应用变换 (`Q`)**。它在模型设计（权重绑定）、知识迁移（蒸馏、对齐）、高效微调（优化PEFT起点）、模型压缩（优化权重结构）和模型分析（表示对齐）等多个领域提供了强大的工具，帮助研究者更灵活地操作和理解模型参数空间而不改变其功能。