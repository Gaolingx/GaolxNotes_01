# FlashAttention优化策略解析

## Question 1

有人说FlashAttention 优化其实就是认为“算比存快”，通过分块计算和重计算策略减少显存访问，这具体是如何实现的？

## Answer 1

FlashAttention通过分块计算和重计算策略，利用“算比存快”的理念优化显存访问，具体实现如下：

### 1. **分块计算（Tiling）**
   - **问题背景**：传统注意力计算中的中间矩阵（如QK^T）显存占用为O(N²)，当序列长度N较大时（如N=2048），显存访问成为瓶颈。
   - **分块策略**：
     - 将Q、K、V矩阵划分为多个小块（如块大小B=64），每次仅处理一对块的计算。
     - 每个块的中间结果（如Q_iK_j^T）在GPU的快速SRAM中计算，避免频繁读写显存。
   - **Softmax的在线计算**：
     - **分块归一化**：对每个块的注意力分数进行局部Softmax时，需维护全局统计量（如最大值和求和项）。例如：
       1. 逐块计算时，记录当前块的最大值，并更新全局最大值。
       2. 根据全局最大值调整指数值，累积归一化分母。
     - **数学实现**：通过递推公式更新Softmax的分子和分母，确保分块计算的数值稳定性与全局结果一致。

### 2. **重计算（Recomputation）**
   - **反向传播需求**：传统方法需存储中间矩阵（如QK^T、Softmax结果）用于梯度计算，占用大量显存。
   - **策略**：
     - 前向时不保存大矩阵，仅存储分块元数据（如块索引、归一化参数）。
     - 反向传播时，按分块顺序重新计算前向的中间结果（如Q_iK_j^T和Softmax），利用SRAM快速计算，避免显存访问。
   - **计算换显存**：重计算增加了约20%的计算量，但显存占用从O(N²)降至O(N)，整体速度显著提升。

### 3. **性能优势**
   - **显存访问减少**：分块计算使显存访问量从O(N²d)降至O(N²d/B)（d为特征维度，B为块大小）。
   - **计算效率提升**：GPU的算力（如TFLOPs）远高于显存带宽（如TB/s），通过增加计算密度隐藏显存延迟。
   - **实际效果**：相比原始注意力，FlashAttention训练速度提升2-4倍，显存占用降低5-20倍，支持更长序列（如16k tokens）。

### 示例：分块Softmax计算流程
1. **初始化**：全局最大值m = -∞，分母累加器l = 0。
2. **逐块处理**：
   - 计算块内Q_iK_j^T，得到局部注意力分数。
   - 更新当前块的最大值m_new = max(m, 局部最大值)。
   - 调整指数值：exp(scores - m_new)，并更新分母l = e^{m - m_new} * l + sum(exp_scores)。
   - 更新分子：输出块 = (e^{m - m_new} * 原输出块) + (exp_scores / l_new) * V_j。
   - 更新全局m和l为m_new和l_new。
3. **最终输出**：所有块累积后得到正确的注意力输出，无需存储中间矩阵。

### 总结
FlashAttention通过**分块计算**将大矩阵分解为SRAM友好的小块，结合**重计算**在反向时动态生成中间结果，以额外计算为代价，大幅减少显存访问。这种设计充分利用GPU计算资源的高吞吐特性，在长序列任务中实现显著加速和显存节省。