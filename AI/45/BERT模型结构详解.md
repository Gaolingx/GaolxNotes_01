# BERT模型结构详解

### 🧑‍💻 User

bert模型结构是怎么样的？

---

### 🤖 Assistant

BERT（Bidirectional Encoder Representations from Transformers）是一种基于Transformer架构的预训练语言模型，其核心结构由多个堆叠的Transformer编码器层组成。以下从模型结构、关键组件及训练目标三个层面解析其工作原理：

---

### 一、模型架构概览
1. **基础组件**  
   - **堆叠式Transformer编码器**：BERT基于Transformer的编码器结构堆叠而成，包含12层（BERT-base）或24层（BERT-large）基础模块。
   - **参数量**：Base版本约1.1亿，Large版本约3.4亿，体现深度与宽度的扩展性。

2. **输入表示**  
   输入嵌入（Input Embeddings）由三部分组成：
   - **Token Embeddings**：词元（WordPiece子词）嵌入，将输入文本分割为子词单位（如“playing” → “play+ing”）。
   - **Segment Embeddings**：二值张量（0或1），区分句子对（如问答对中的问题与答案）。
   - **Positional Embeddings**：学习的位置嵌入（Learnable Positional Embeddings），捕捉序列中的位置信息。

3. **Transformer层**  
   每层包含两个子层：
   - **多头自注意力（Multi-Head Self-Attention）**：并行计算多个注意力头（如12头或16头），建模长距离依赖关系。
   - **前馈神经网络（FFN）**：包含两层线性变换，激活函数为GELU，增强非线性表达。
   - **残差连接与层归一化**：加速训练并缓解梯度消失。

---

### 二、预训练任务
BERT通过自监督学习掌握语言模式，包含两大任务：
1. **Masked Language Model (MLM)**  
   - 随机遮蔽（mask）输入中15%的词元，模型需预测遮蔽词（如将“the cat sat”中“sat”改为`[MASK]`后预测）。
   - 遮蔽策略：80%概率替换为`[MASK]`，10%随机词，10%原样保留。

2. **Next Sentence Prediction (NSP)**  
   - 判断两句话是否连续（如文档中的下一句），辅助模型理解句间关系。

---

### 三、模型输出与微调
1. **输出表示**  
   - **`[CLS]`标记**：序列首部的特殊标记，其最终隐层状态（`[CLS]`向量）用于分类任务（如文本分类、问答系统）。
   - **Token表示**：各位置词元的隐层状态用于序列标注（如同义词消歧、NER）。

2. **微调策略**  
   - 在预训练参数基础上，针对具体任务（如情感分析、机器阅读理解）微调全部参数。
   - 下游任务结构适配：如分类任务中在`[CLS]`后添加全连接层，序列标注任务则直接使用各token的表示。

---

### 四、技术优势与变体
1. **创新性**  
   - **双向编码**：与传统单向LSTM不同，BERT通过MLM实现上下文的全向建模。
   - **全局注意力**：自注意力机制使模型动态关注长距离依赖（如指代消解）。

2. **主流变体**  
   - **ALBERT**：参数共享技术压缩模型体积。
   - **RoBERTa**：移除NSP任务并延长训练时间，提升性能。
   - **SpanBERT**：改进MLM为连续词块预测，增强实体表示。

---

### 五、局限性
1. **计算开销**：Large版本需多卡支持，推理速度较慢。
2. **领域适应**：预训练语料库多为维基百科，垂直领域效果可能下降。

BERT的提出标志着NLP进入预训练模型时代，其结构设计为后续模型（如GPT系列、T5）提供了范式参考。