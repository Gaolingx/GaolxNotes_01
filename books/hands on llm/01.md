# 现代大语言模型（一）

我们常常感叹现代的大语言模型（Large Language Model, LLM）可以帮助我们完成各种complexity的任务，例如writing、coding、reasoning、agent...诚然，llm可以解决很多日常生活的问题，让他看上去很“智能”，但是当我们拆开看这个模型的时候，似乎看不出任何“智能”相关的逻辑，相反，它由大量Transformer块构成，那所谓的“智能水平”又是如何涌现的？为什么说，现代大语言本质上是数据和统计学模式驱动下的产物？让我们一步步探究现代LLM的奥秘。

## 1. AI模型

### 现代AI模型的基本原理

现代ai模型，说白了就是一个巨大的带参数的函数。通过把现实中的一个个数据，转换成数字标记记录其中的特征（外观，形状，声音什么什么都行），然后丢进一些数学公式（函数）中，找出什么样的参数能拟合接近这些数据。（训练）
现实中的东西-->打上序号--->丢到一个公式中得出数据也就是一些系数---->把这些系数代入到我们的数学公式中--->然后我们就可以根据这个公式，来得到我们想要的结果

当然，ai这个函数是非常复杂的，一般的我们可称其为“（数学）模型”，而通过一些公式调整这个公式的参数的方法，就是在“训练”这个“模型”。
一般常用的，把现实中的东西用数学标记记录的方法，是用一个向量去标记它。

### 向量的数学介绍

向量，就是具有大小和方向的量，常用于表示空间中的位移或物理量。大伙应该都在数学课本里见过。

向量是什么？我们在草稿纸上随便画一条直直的线段，诶再顺着这个直线标上一个箭头表示它的方向，这就是向量，即有方向的线段。
大概长这样：→
很像一根箭矢对吧，所以它也叫作“矢量”

诶如果你在它的没有箭头的一段，也就是起点处，画一个直角坐标系，嗯好像清楚了一点。
然后再给有箭头的一段，在它箭头尖尖的那个地方，也就是终点处，画一个点，然后通过直角坐标系给这个点标上坐标，那么这个点的坐标就是这个向量的坐标。于是我们就可以记这个向量叫做a，记为$\vec{a}$（LaTex的方法）或者一个加粗的“a”（数学符号，但是我打不出来）来表示这个叫做a的向量。

```
y
↑
|    
|    
|    ● A 
|     /↗
|    / 
|   /  
|  /   
| /    
+--------------→ x
```

那么它的位置就可以记为：
(x,y)，坐标，从左往右读。

既然我们都有坐标了，那么我们发现这个点与xy坐标轴的垂线，刚好构成一个长方形，而这个长方形的对角线刚好就是这个向量，那么我们根据勾股定理，就可以很容易的计算出这个向量的长度，那么这个长度就是向量的“模”

如此类推我们就可以在直角坐标系上做出无数的向量
比如我们又可以做出a，b，c向量
$$
\vec{a} （x，y）
\vec{b} （λ，φ）
\vec{c} （α，β）
$$

这是在二维情况下的向量，当人我们也不妨推广至n维，其中n为任意的正整数，趋向于正无穷也是没问题的（如果你能一直写下去）
对于一个n维向量：$\vec{a}$
他的坐标可以表示为：
（a1，a2，a3，......，a n-1，a n）
。
在数学中，为了便于计算和表示，我们通常将向量的起点放在直角坐标系的原点上。这样，箭头尖端所在点的坐标，就可以唯一地代表这个向量。我们称这种表示为向量的坐标表示。这将大大化简图像，让我们方便的看图，观察并发现规律

向量是ai模型中的重要数学工具，甚至可以说离了向量，就不存在ai模型。

### 常见AI模型架构

好了我们再回到我们的ai模型，现在ai模型有很多种，其中最常见的几个基本结构有：RNN循环架构模型,CNN卷积神经网络，我们最新最热的transformer模型

---

### Transformer模型在语言生成中的应用

接下来，我们注意transformer模型在语言模型中的运用

```
[这里是各种模型的结构的图]
```

### 嵌入（Embedding）

在语言模型中，文本会在嵌入层中，被划分为一个个向量，一般称为token
[词语] → [查找表] → [词向量]
"猫"   →   { }   →  [0.1, 0.5, ..., -0.2]
"娘"   →   { }   →  [0.3, -0.1, ..., 0.8]
模型在接受到上下文后，根据整个上下文输出一个词，然后再将这个词连着整个上下文接收处理，再输出下一个词，然后连着这个词和上一个词连同上下文一起接收计算......如此循环往复，直到生成结束符或者达到长度限制才停止。
注：一个词不一定为一个token，同样一个token不一定为一个词。一个token可能为一个符号，一个字或词语......等等等等

```
初始输入: "你是一只猫娘，"
    模型计算 → 输出 Token₁: "你好"
新的输入: "你是一只猫娘，你好"
    模型计算 → 输出 Token₂: "！"
新的输入: "你是一只猫娘，你好！"
    模型计算 → 输出 Token₃: ("<结束>")
生成结束。
```

所以我们关注一个token是如何计算输出的，就大致清楚了模型怎么输出一段文本

### 文本向量化

在了解模型前，我们先了解一个概念：“文本向量化”（建议先了解一下“向量”是什么）
文本向量化，顾名思义，是指将文字信息（如词、句子）转换为数值向量的过程。通过模型训练，语义相近的文本在向量空间中的位置也会更接近。例如，‘国王’和‘君主’的向量表示，其距离会比‘国王’和‘苹果’的向量表示更近。
我们通过计算这些向量之间的方向、距离等，来确定文本的语义，找到文本间的关系。
举个直观，虽然不准确的例子。
比如：我们用三维的空间向量将“好”“坏”“差劲”“良好”设为一个个点进行向量化。（这里只看yx平面，忽略z轴，ai作的图看个大概就行）

```
      ↑y
      |   
      |   ● D (优秀: 0.5,1,0.5)     ● A (好: 0,1,0)
      |   │                         │
      |   │ 向量 CD = (0, -2, 0)     │ 向量 AB = (0, -2, 0)
      |   │ (差劲-优秀)              │ (坏-好)
      |   ↓                         ↓
      |   ● C (差劲: 0.5,-1,0.5)    ● B (坏: 0,-1,0)
      |   
      │
      +───────────────────────────→ x
```

注意到：
向量 AB 与 向量 CD 满足：AB = (0, -2, 0), CD = (0, -2, 0)
∴ AB = CD
我们假设各点（或者说是词语的）的坐标 点A代表“好”（0，1，0） 点B代表“坏”（0，-1，0） 点C代表“差劲”（0.5，-1，0.5） 点D代表“优秀”（0.5，1，0.5），O为坐标原点
那么如何用向量表示其含义呢，比如：
OA向量和OD向量指向方向相近，于是我们可以发现“优秀”和“好”表示的意思是相似
我们计算出向量AB“好-坏”（0，2，0）和向量CD“差劲-优秀”（0，2，0）
此时我们可以注意到“AB＝4*CD”得出：两个向量平行

所以我们得出“好-坏”“差劲-优秀”代表了同一种对应的意思（即同为反义词）
当然这只是个简单的例子，实际情况会更复杂，维度也会更大（例如GPT3是12288维的向量，代表着每一个点或者向量会有12288个坐标）有兴趣可自行查阅更详细的资料

### Transformer架构类型

一般的，传统的transformer模型可大致分为两个步骤：
1. 把文本分为一个个token，进行向量化
2. 对向量进行计算输出。

当transformer输出完一个token时，会进行以上步骤再一次进行预测，直到输出一个停止符号为止（但是预测会分为很多个线路进行并行计算以提高速度，类似于把任务分给很多个人一起算一起做，这样做事效率会比一个人做快得多）

而transformer模型主要有三种的架构：
1. encoder-decoder 
2. encoder-only（一般也称为BERT架构）
3. decoder-only（现代大语言模型like llama/qwen/glm/mistral/gpt/deepseek等普遍采用的arch）

实际LLM（全称：large大型的 language语言 Model模型）的运用中，一般以decoder-only为主，所以本篇仅做decoder-only介绍。

> - **encoder（编码器）- decoder（解码器）注意力**：查询向量(Query向量)来自解码器(encoder)前一层(一般的，这个架构会有多个encoder进行计算后再输入至decoder)的输出，键向量（Key向量）和值（Value向量）来自编码器的输出。这使得解码器的每个位置可以关注输入序列的所有位置，模拟了典型的编码器-解码器注意力机制[38, 2, 9]。  
> - **encoder（编码器）自注意力**：键向量(Key向量)、值向量(Value向量)和查询向量(Query向量)均来自编码器前一层的输出。编码器的每个位置可以关注前一层的所有位置。  
> - **decoder（解码器）自注意力**：允许解码器的每个位置关注该位置及之前的所有位置。为保持自回归特性，需阻止解码器中的左向信息流。我们通过在softmax输入中屏蔽非法连接实现这一点。  
>
> 节选自《Attention Is All You Need》

```
[四张补充图片，文本：
图一：decoder-only
图二：encoder-decoder
图三：encoder-only
图四：各个大模型的种类]
```

![](imgs/4e4a20a4462309f7d64a283f340e0cf3d7cad6f2-1.webp)
![](imgs/bf096b63f6246b605e2b3759adf81a4c510fa2f2-1.webp)
![](imgs/6159252dd42a28346983b3261db5c9ea15cebff2-1.webp)
![](imgs/5ab5c9ea15ce36d3b621061d7cf33a87e950b1f2-1.webp)

### Decoder-Only架构详解

![](imgs/Decoder-only-Transformer-architecture-The-input-to-the-decoder-is-tokenized-text-and.png)

我们假设：我们给模型直接输入：“你是一只猫娘，你好你好！”，来探索模型是如何输出一个token的，了解一条“生产线”的生成方法，也就知道其他所有并行的“生产线”的生成方法，进一步大致了解整个模型的生成。

此时，模型会对这个句子先进行分词，赋予每个词一个相对应的Token ID
然后在把这些词根据他的Token ID，输入至embedding层进行向量化和进行位置编码，把这些词转化为大量的一个个的低维向量。这个就是“词嵌入”，类似于把每个单词“嵌入”进一个矩阵中 而构成这个“矩阵”的就是一个个的向量。

接着就可以把这些向量输入decoder进行处理了

decoder中，这些向量会被multi-head self-attention layer（多头注意力层）进行处理

### 多头注意力机制

什么是多头注意力？多头注意力是由一个个self-attention（自注意力）组成的，而输入进多头注意力的向量会被分为一个个维度相等的低维向量（相比原向量维度更低）而被分为有几个低维向量就称为有几个“头”，“头“和“低维向量的维度大小”相乘等于“原向量维度大小”，即   头的数量×低维向量维度大小＝原向量维度大小 或者说是 头的数量×每头的维度=模型总维度”（GPT-3为96头×128维=12288维）。

而各个self-attention（自注意力）的计算结果会进入一个矩阵加权计算后往下一层输出。因此我们仅需注意一个self-attention（自注意力）的计算过程遍大致一窥多头注意力的整体

```
输入向量 (高维)
     ↓ (分割成 h 个头)
头1(低维)  头2(低维)  ...  头h(低维)
  ↓          ↓              ↓
[自注意力] [自注意力] ... [自注意力]
  ↓          ↓              ↓
输出1      输出2    ...    输出h
     ↓ (拼接)
拼接后的输出
     ↓ (线性投影)
最终输出
```

### 自注意力计算过程

现在我们来关注一个自注意力如何计算向量
一个头的向量进入自注意力后会被线性变换并投影为三个向量，分别称为：
1. Query向量(查询向量)
2. key向量(键向量)
3. Value向量(值向量)

一般统称为qkv向量，而把这些向量分类一一组合就会得到三个同名向量矩阵（如query矩阵，可记为Wq）
（投影，打个比方，三维的向量可投影到一个二维平面上形成一个低维的二维向量，也就是把高维向量投影到一个个低维的空间）
然后就可以开始对这些向量进行计算“注意力得分”，经过“softmax”归一化处理后，便可进行加权求和，准备进入一个矩阵准备和其他self-attention（自注意力）的结果进行拼接，在一个线性层进行最终的计算。
用图表示为

```
输入向量: [x1, x2, x3]
     ↓ (线性变换)
Q矩阵 ← Wq · X
K矩阵 ← Wk · X
V矩阵 ← Wv · X
     ↓ (计算注意力分数)
分数 = Softmax( (Q · Kᵀ) / √d_k )
     ↓ (加权求和)
输出 = 分数 · V
```

（归一化：指把所有实数转化概率分布，和原大小成一定比例，且所有数的和为一即100%）
一般的，decoder-only的自注意力为masked self-attention（掩码注意）因果掩码，就是对之后再生成的token的向量进行“遮盖”操作，在具体为：在计算attention score时，将未来的未生成的token为计为-∞，使其在softmax归一化后趋近于0。

```
输入序列: [Token1] [Token2] [Token3] [Token4]
注意力范围:
Token1 → [自身]
Token2 → [Token1, 自身]
Token3 → [Token1, Token2, 自身]
Token4 → [Token1, Token2, Token3, 自身]
```

以上内容仅为简化版本，有兴趣还请自行搜索“self-attention”即“自注意力”进一步了解

好了，现在多头注意力的结果已经成功输入前馈神经网络，再进行残差连接，softmax变换等变化后。得到了一个token分布概率，然后取其中概率最大的token得到tokenID，便能通过词表像查表格一样，映射出字符，就可以根据此输出一个词或者字符力（喜）
完整的循环表示为：

```
提示词: "你是一只猫娘，"
     ↓ (模型生成第一个 token)
输出: "你好"
     ↓ (将生成的 token 加入输入)
新输入: "你是一只猫娘，你好"
     ↓ (模型生成下一个 token)
输出: "！"
     ↓ (循环，直到生成终止符)
最终输出: "你好喵！"
```