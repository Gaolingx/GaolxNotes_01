# 现代大语言模型（三）

## 语言模型的生成分析

---

## 一、引言：如何分析模型生成？

我们对模型的分析，无非来自于模型生成的内容。当然这也是最难的一点，到底怎么分析，分析什么内容，怎么又知道这代表什么？

在那之前，我们看一下一个很简单的数学知识和思维方式。

---

## 二、概率论基础回顾

### 2.1 抛硬币的例子

假设我们在抛一个硬币玩。

但是单抛着没啥意思对吧，于是我们可以记一下，如果我们抛到正面，那么我们记这个情况为事件A。同理，记反面为事件B。

此时我们自然注意到在我们抛的过程中（设我们正在持续不断的抛无穷次），A事件的概率为50%，那么我们就可以记A事件的概率为P（A）＝50%，因为我们很容易发现我们此次游戏中，只有事件A和事件B两种情况，所以我们很容易发现P（A）和P（B）的概率和应当是100%，所以不难得出P（B）的概率应当为100%-P（A）＝50%

### 2.2 概率

如果我们有两个硬币，我们记分别为硬币a与硬币b，那么其A事件概率分别为P（Aa）与P（Ab），同理B事件概率分别为P（Ba）与P（Bb）

如果我们想知道，当硬币a抛到正面时，硬币b抛到正面的的概率为多少，我们不妨记这个概率为P（Ab｜Aa），表示当Aa事件发生时，Ab的概率是多少。

∵ P（Ab｜Aa）＝P（Aa∩Ab）/P（Aa）     

> P（Aa∩Ab）指的时Aa和Ab同时发生的概率

我们当然知道，硬币a什么情况，都和硬币b屁关系没有，即不影响硬币b，即Aa与Ab互相独立

∴ P（Ab｜Aa）＝P（Aa）P（Ab）/P（Aa）＝P（Ab）＝50%

∴ P（Ab｜Aa）＝50%

**以上就是一个很简单的数学推理，这些看起来跟废话一样的思维将会贯穿全文。**

---

## 三、模型生成的概率分析

### 3.1 理想提示词下的生成

同样的,我们对模型进行生成着玩。设:上下文极其干净,输入的提示词是理想提示词。理想提示词为一句长度极短,内容极少,对模型的生成产生的权重影响极小,仅引出内容生成的提示词。

> **注:** 一般我们必须考虑提示词的影响,但是这个我们往后再说。且再此小段中我们主要关注相对大小,不考虑其具体大小

### 3.2 生成示例

很快我们可以得到以下结果:

**示例1:**

【喵呜~主人好呀！(开心地竖起耳朵，尾巴轻轻摇晃) 

人家是软乎乎的猫娘，最喜欢主人啦~今天也要一起玩耍吗？(歪着头用亮晶晶的眼睛看着你)

喵~可以摸摸头吗？或者...主人想听人家呼噜呼噜的声音？(凑近蹭蹭你的手) 

(突然发现自己的蝴蝶结歪了) 啊呜...帮人家整理一下嘛~】  

> *感谢deepseek v3（0324）友情出演。*

**示例2:**

【喵呜~主人好呀！(ฅ^•ﻌ•^ฅ) 人家是软乎乎的小猫娘，今天也充满元气呢！尾巴开心地摇来摇去~

要摸摸头吗？还是想和人家一起玩毛线球呀？(≧▽≦) 人家最喜欢主人啦！喵~

（蹭蹭你的手）今天有什么想和猫娘一起做的事情吗？无论是晒太阳发呆还是吃小鱼干都可以哦！】

> *deepseek v3（0324）*

**示例3:**

【喵呜~主人好呀！(≧▽≦) 人家是软乎乎的小猫娘，尾巴正开心地摇来摇去呢~今天也要和主人贴贴！有什么想和人家玩的吗？(*´∀`)~♥

（竖起耳朵凑近，爪爪轻轻搭在主人膝盖上）】

> *还是deepseek v3（0324）捏*

---

### 3.3 初步分析的问题

注意到，在同一段提示词下，它生成的内容总是确定在一定的范围内。就算如果我们让它生成随机数，也会发现3和7额外的多。

易知我们可以像分析抛硬币一样，来找寻分析模型的内容生成概率的约值，并通过减小精度来使他的结论准确。

嗯，通过这个思路，我们统计一下概率。诶"猫娘"占总词数的1.4%，"主人"占种词数的3.7%，哇！我们越是生成，发现结果应当越是准确，偏差很少。所以我们可以得出该模型对"猫娘"的权重是1.4%，"主人"的权重是3.7%

**对！这个分析显然是符合统计方法的。但这也个分析结果很显然，肯定是不符实际的**，因为如果我们再换个理想提示词就会发现，利用这些分析结果得到的预期生成和结果天差地别。那应当怎么分析呢？

---