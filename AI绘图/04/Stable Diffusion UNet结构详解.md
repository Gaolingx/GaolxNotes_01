# Stable Diffusion UNet结构详解

### 🧑‍💻 User

stable diffusion模型的unet结构是怎么样的？

---

### 🤖 Assistant

Stable Diffusion 模型的核心是它的 **UNet 架构**。这个 UNet 并不是传统图像分割中的原始 UNet，而是一个经过**深度修改和扩展**的版本，专门设计用于在**潜空间**中进行**扩散过程**的**噪声预测**任务。

以下是 Stable Diffusion UNet 的主要结构和关键特点：

1.  **核心任务**：
    *   输入：一个带噪声的**潜空间表示**（`latent`，通常是 64x64 或 96x96 等较小分辨率），以及对应的**时间步**（`timestep t`）和**文本嵌入**（`text_embeddings`）。
    *   输出：预测的**噪声**（`predicted noise`），这个噪声需要从输入中减去以逐步去噪。

2.  **整体架构：编码器-解码器 + 跳跃连接**：
    *   遵循经典的 UNet 范式，包含一个**下采样路径（编码器）** 和一个**上采样路径（解码器）**。
    *   在编码器和解码器的**相同分辨率层级**之间，存在大量的**跳跃连接**。这些连接将编码器提取的细节信息直接传递到解码器，帮助解码器在重建去噪图像时恢复精细结构。

3.  **基础构建块：残差块**：
    *   UNet 主要由多个**残差块**堆叠而成。这是现代深度神经网络的核心组件，有助于训练更深的网络。
    *   每个残差块通常包含：
        *   卷积层（通常是 3x3 卷积）
        *   归一化层（通常是 `GroupNorm` 或 `LayerNorm`，因为 `BatchNorm` 在扩散模型中效果不佳）
        *   激活函数（通常是 `SiLU` 或 `Swish`）
        *   可选的**自注意力**或**交叉注意力**层（见下文）。
        *   残差连接：将块的输入直接加到输出上。

4.  **关键创新：条件注入 - 时间步和文本**：
    *   **时间步嵌入**：
        *   扩散过程的时间步 `t` 至关重要，它决定了当前噪声的强度。
        *   `t` 被编码为一个高维向量（通常使用正弦/余弦位置编码或学习嵌入）。
        *   这个时间嵌入向量通常通过**加法**或**仿射变换**注入到**每个残差块**中（例如，在归一化层之后，通过一个小的 MLP 调整特征图的缩放因子 `scale` 和偏置 `shift`）。
    *   **文本条件嵌入（交叉注意力）**：
        *   Stable Diffusion 是**文本到图像**模型，文本提示通过**交叉注意力机制**注入到 UNet 中。
        *   文本提示被一个文本编码器（如 CLIP 的文本编码器）转换为一组嵌入向量 `text_embeddings`。
        *   在 UNet 的**特定层（通常是每个下采样和上采样层之后）** 的残差块中，会加入一个**交叉注意力层**。
        *   在这个注意力层中：
            *   `Query` 来自当前 UNet 层的图像特征。
            *   `Key` 和 `Value` 来自文本嵌入向量 `text_embeddings`。
        *   注意力机制计算图像特征（`Query`）和文本特征（`Key`）之间的相关性，并用文本信息（`Value`）加权求和来调制图像特征。这允许模型根据文本描述来指导去噪过程。

5.  **下采样路径（编码器）**：
    *   由多个**下采样阶段**组成。
    *   每个阶段通常包含：
        *   若干个**残差块**（包含时间嵌入注入，可能包含自/交叉注意力）。
        *   一个**下采样层**：通常使用 **步长为 2 的卷积**（`Conv2d with stride=2`）或 **最大/平均池化**来将空间分辨率减半，同时增加通道数（特征深度）。下采样操作将特征图尺寸缩小（如 64x64 -> 32x32 -> 16x16 -> 8x8 -> 4x4），同时通道数增加（如 128 -> 256 -> 512 -> 768 -> 1024）。

6.  **上采样路径（解码器）**：
    *   由多个**上采样阶段**组成，结构与编码器大致对称。
    *   每个阶段通常包含：
        *   若干个**残差块**（包含时间嵌入注入，可能包含自/交叉注意力）。
        *   一个**上采样层**：通常使用**转置卷积**或简单的**最近邻/双线性上采样**后接一个卷积层，将空间分辨率加倍，同时减少通道数（如 4x4 -> 8x8 -> 16x16 -> 32x32 -> 64x64，通道 1024 -> 768 -> 512 -> 256 -> 128）。
        *   **跳跃连接**：将**编码器**中**相同分辨率层级**的特征图通过连接（`concatenation`）或加法（`addition`）的方式融合到解码器的输入中。这是恢复细节的关键。

7.  **瓶颈层**：
    *   位于编码器末端（最低分辨率）和解码器开始之间。
    *   通常由几个**残差块**组成，处理最抽象的特征表示。这些块也会接收时间步和文本信息。

8.  **输出层**：
    *   在解码器末端（最高分辨率，与输入分辨率相同），通常由一个或几个卷积层组成（有时也包含残差块）。
    *   最终的卷积层（通常是 3x3 Conv）将通道数映射到与输入噪声相同的维度（通常是 4 或 8 个通道，对应潜空间的维度），输出预测的噪声。

9.  **自注意力**：
    *   除了交叉注意力，在 UNet 的某些层（尤其是在瓶颈层或较低分辨率层）还会加入**自注意力层**。
    *   自注意力让模型能够在**空间维度**上关注图像特征的不同部分，有助于建模图像内部的长期依赖关系和全局结构。

**总结 Stable Diffusion UNet 的关键特点：**

*   **在潜空间操作**：输入输出都是低维潜向量（如 64x64x4），极大降低了计算量。
*   **深度条件 UNet**：核心是基于残差块的编码器-解码器结构。
*   **时间步注入**：通过每个残差块调制网络行为。
*   **文本条件注入**：通过交叉注意力机制，在特定层将文本语义信息融入图像特征。
*   **跳跃连接**：连接编码器和解码器，保留空间细节。
*   **注意力机制**：包含交叉注意力（文本-图像）和自注意力（图像内部）。
*   **预测噪声**：网络目标是预测添加到潜变量上的噪声。

这个复杂的、深度条件化的 UNet 结构使得 Stable Diffusion 能够根据文本描述，在潜空间中一步步精确地去除噪声，最终生成高质量的图像。它是整个模型生成能力的核心引擎。

---

### 🧑‍💻 User

stable diffusion 1.x/2.x模型的vit在经典的卷积unet基础上进行了哪些改进，哪些部分被替换成了transformer块，能不能画草图说明？

---

### 🤖 Assistant

好的，我们来详细拆解 Stable Diffusion (SD) 1.x/2.x 模型是如何在经典卷积 UNet 基础上，通过融入 **Vision Transformer (ViT)** 的思想（更准确地说，是 **Transformer 块**）进行改进的，以及哪些部分被替换了。核心改进在于引入了 **注意力机制（特别是交叉注意力）** 来处理文本条件信息。

**核心改进：用 Transformer 块（注意力机制）增强条件控制**

SD 的 UNet **不是**直接用 ViT 替换了整个 CNN，而是 **在原有的卷积残差块结构中，策略性地插入 Transformer 块（主要包含自注意力和交叉注意力层）**。这些块负责将 **文本嵌入 (Text Embeddings)** 和 **时间步嵌入 (Timestep Embedding)** 有效地注入到图像特征中。

**哪些部分被替换/修改成了 Transformer 块？**

1.  **残差块内部的改造 (核心变化！):**
    *   **经典 CNN UNet 残差块：** 通常是 `Conv -> Norm (e.g., GroupNorm) -> Activation (e.g., SiLU)`，可能串联多个这样的组合，最后加残差连接。主要用于空间特征提取。
    *   **SD UNet 的“条件化”残差块：** 在原有卷积操作的基础上，**插入 Transformer 层**。一个典型的 SD 残差块结构顺序如下：
        1.  **卷积层 (可选)：** 初始特征变换。
        2.  **GroupNorm：** 归一化。
        3.  **SiLU 激活：** 非线性变换。
        4.  **时间步注入：** *时间步嵌入 `t` 通过一个小型 MLP（通常是全连接层）生成缩放因子 `scale` 和偏移因子 `shift`。这些因子作用于上一步归一化后的特征：`feature = feature * scale + shift`。* **（这是对原有块的修改，不是替换成 Transformer）**
        5.  **自注意力层 (Spatial Self-Attention)：** *这是加入的第一个 Transformer 组件。* 它让特征图上的每个空间位置都能关注其他所有位置。这有助于建模图像内部的**长距离依赖关系**和**全局结构一致性**，这是纯 CNN 在深层小特征图上相对薄弱的方面。输入和输出维度相同。
        6.  **交叉注意力层 (Cross-Attention)：** *这是加入的第二个也是最关键的 Transformer 组件。* 这是文本条件信息注入的核心通道。
            *   **Query (`Q`)**: 来自当前步骤 (5) 输出的**图像特征** (通常会被线性投影)。
            *   **Key (`K`), Value (`V`)**: 来自**文本嵌入** `text_embeddings` (由 CLIP 文本编码器产生，也被线性投影)。
            *   注意力机制计算 `Q` 和 `K` 的相似度，得到一个权重矩阵，然后用这个权重对 `V` (文本信息) 进行加权求和。最终输出是图像特征根据文本语义信息调制后的结果。输出维度与输入 `Q` 相同（即图像特征维度）。
        7.  **后续卷积层 (可选)：** 可能还有额外的卷积层进行进一步处理。
        8.  **残差连接：** 将原始输入加到当前处理后的特征上。

    **关键点：** Transformer 块（自注意力 + 交叉注意力）是**嵌入**在原有的卷积残差块结构中的，**替换了原本可能存在的额外卷积层的位置，或者作为新增的组件插入**。它让每个“残差块”具备了同时处理空间信息（CNN + 自注意力）和文本语义信息（交叉注意力）的能力。

2.  **插入位置：**
    *   这些包含 Transformer 层（自注意力 + 交叉注意力）的条件化残差块，**主要被放置在 UNet 的 Bottleneck（瓶颈层）以及下采样和上采样路径的中间层（非最底层和最顶层）**。
    *   **SD 1.x:** Transformer 块通常只在 Bottleneck 和部分中间分辨率层（如下采样到 16x16, 8x8，上采样对应的层）加入。
    *   **SD 2.x (关键改进):** **显著增加了 Transformer 块的插入密度和规模。**
        *   在更多的分辨率层级（包括更高分辨率层，如 32x32）加入了条件化残差块。
        *   增加了 Transformer 块内部的 **通道维度 (宽度)** 和 **Transformer 层的数量 (深度)**。例如，一个条件化残差块内可能包含多个串联的自注意力 + 交叉注意力层对。
        *   使用了更大的 **文本上下文长度**（77 -> 77 或更长）和 **文本嵌入维度**（768 -> 1024 或更高）。
    *   **输入层 (高分辨率) 和 输出层 (高分辨率) 通常保留为纯卷积层或只含自注意力的层**，因为在这些层级做全局交叉注意力计算代价非常高（空间位置太多），且低层特征更关注局部纹理而非全局语义。

**草图说明 (文字描述结构)**

想象一个典型的 UNet 沙漏形状：

```
          Encoder (Downsample Path)                 Decoder (Upsample Path)
High Res (e.g., 64x64) ------------------------> Low Res (e.g., 8x8) ------------------------> High Res (e.g., 64x64)
       |          |          |          |              |          |          |          |
      [C]        [C]        [C+T]     [C+T]           [C+T]      [C+T]      [C]        [C]
       |          |          |          |              |          |          |          |
       V          V          V          V              V          V          V          V
      DS         DS         DS         DS             US         US         US         US
       |          |          |          |              |          |          |          |
       V          V          V          V              V          V          V          V
      ...        ...       [C+T]     [C+T] <-------- [C+T]      [C+T]      ...        ...
       |          |          |          |   Skip      |          |          |          |
       |          |          |          | Connection  |          |          |          |
       V          V          V          V              V          V          V          V
      ...        ...        ...      [C+T] (Bottleneck) [C+T]      ...        ...        ...
```

**图例：**

*   `[C]`: **纯卷积残差块** (可能包含时间步注入 `t`，但不包含注意力层)。通常用于最高/最低分辨率层。
*   `[C+T]`: **条件化残差块 (包含 Transformer)**。包含：
    *   `C` (卷积层)
    *   `GN` (GroupNorm)
    *   `Act` (SiLU)
    *   `t` (时间步注入：`scale & shift`)
    *   `SA` (空间自注意力 Spatial Self-Attention)
    *   `CA` (交叉注意力 Cross-Attention: Image Q, Text K/V)
    *   (可能还有后续 `C`/`GN`/`Act`)
    *   残差连接。
*   `DS`: 下采样操作 (如 Strided Conv)。
*   `US`: 上采样操作 (如 Transposed Conv 或 Nearest Upsample + Conv)。
*   `--->`: 主干路径。
*   `Skip Connection`: 跳跃连接，将编码器特征传递到解码器对应层（通常通过拼接或加法）。
*   `Bottleneck`: 最低分辨率层（信息最压缩）。

**SD 1.x vs SD 2.x 在 Transformer 使用上的区别 (草图示意)：**

```
假设 UNet 有 4 个下采样/上采样阶段 (分辨率：64 -> 32 -> 16 -> 8 -> 4 -> 8 -> 16 -> 32 -> 64)

SD 1.x Transformer 插入 (示例)：
  64: [C]     -> [C]     (通常无 T)
  32: [C]     -> [C]     (可能无 T 或少量层有 T)
  16: [C+T]   -> [C+T]   (主要插入层)
   8: [C+T]   -> [C+T]   (主要插入层)
   4: [C+T]   (Bottleneck - 必有 T)
   8: [C+T]   -> [C+T]
  16: [C+T]   -> [C+T]
  32: [C]     -> [C]
  64: [C]     -> [C]

SD 2.x Transformer 插入 (示例 - 更密集、更深、更宽)：
  64: [C]       -> [C]       (顶层可能仍无 T)
  32: [C+T]     -> [C+T]     (新增了 T 层！)
  16: [C+T]xN   -> [C+T]xN   (N 可能 >1，块内多层 T)
   8: [C+T]xN   -> [C+T]xN   (N 可能 >1)
   4: [C+T]xM   (Bottleneck - M 可能更大)
   8: [C+T]xN   -> [C+T]xN
  16: [C+T]xN   -> [C+T]xN
  32: [C+T]     -> [C+T]     (新增了 T 层！)
  64: [C]       -> [C]
```

**总结关键改进：**

1.  **核心机制：** 在 CNN UNet 的残差块中**嵌入 Transformer 层**（自注意力 + 交叉注意力），而非整体替换。
2.  **关键替换/新增组件：**
    *   **自注意力层 (SA)：** 替换了部分用于建模长距离依赖的深层卷积，或作为新增组件。增强图像内部结构一致性。
    *   **交叉注意力层 (CA)：** **这是最重要的新增组件！** 它建立了图像特征 (`Q`) 和文本嵌入 (`K`, `V`) 之间的直接联系，是文本条件控制图像生成的核心。它替换了传统条件生成模型中简单的特征拼接或相加等较弱的条件注入方式。
    *   **时间步注入：** 修改了残差块，通过 `scale & shift` 调制特征，更有效地融入扩散过程的时间信息。
3.  **SD 2.x 的增强：**
    *   **更广：** 在更多分辨率层级（包括较高分辨率如 32x32）加入条件化残差块。
    *   **更深：** 单个条件化残差块内可能包含多个串联的 (SA + CA) 层对 (`xN`, `xM`)。
    *   **更宽：** 增加了 Transformer 层内部的通道维度（特征向量长度）。
    *   **更强的文本编码：** 使用更大的文本嵌入（OpenCLIP），提供更丰富的语义信息。

**结论：**

Stable Diffusion 的成功很大程度上归功于这种 **CNN-Transformer 混合架构**。CNN 保留了强大的局部特征提取和空间归纳能力，尤其适合处理图像数据。而嵌入的 Transformer 层，特别是**交叉注意力机制**，提供了强大的**条件信息融合能力**，使得模型能够精确理解文本提示并指导图像生成过程。自注意力层则弥补了 CNN 在建模全局依赖上的不足。SD 2.x 通过增加 Transformer 的密度、深度和宽度，进一步提升了模型的表达能力和生成质量。这种混合架构在潜扩散模型中取得了显著的成效。

---

### 🧑‍💻 User

Stable Diffusion 3：采用了DiT架构作为其骨干网络。为什么要放弃了UNet结构，使用纯Transformer（DiT Block）完全取代Unet？它的结构又是怎么样的？

---

### 🤖 Assistant

Stable Diffusion 3 (SD3) 的核心变革在于**彻底摒弃了 UNet 架构，转而采用基于纯 Transformer 的 Diffusion Transformer (DiT) 作为其骨干网络**。这一决策是生成模型架构演进的重要里程碑，其背后有深刻的动机，结构也与之前的 UNet 有本质区别。

**一、 为什么放弃 UNet，采用纯 Transformer (DiT)？**

1.  **Transformer 的全局建模能力优势：**
    *   **UNet 的局限：** 尽管 SD1.x/2.x 的 UNet 通过引入注意力机制（自注意力、交叉注意力）增强了全局理解，但其基础仍是**卷积操作**。卷积本质上是**局部操作**，依靠堆叠层和降采样来扩大感受野。在建模图像中**长距离依赖关系**和**高度复杂的全局结构一致性**（如多个物体的空间关系、透视、复杂纹理的整体协调）时，效率不如自注意力机制。
    *   **Transformer 的优势：** 自注意力机制天然具有**全局建模能力**。每个元素（在图像中是 patch 或潜在表示中的 token）都能直接与序列中所有其他元素交互，无论距离多远。这对于生成具有**高度细节一致性、复杂构图、精确空间关系**的图像至关重要。

2.  **架构的统一性与可扩展性：**
    *   **UNet 的异构性：** SD 的 UNet 是一个混合体，包含卷积层、归一化层、激活函数、上/下采样层、以及各种注意力层（空间自注意力、交叉注意力）。这种异构性增加了架构的复杂性。
    *   **DiT 的纯 Transformer 统一性：** DiT 架构几乎完全由相同的 **DiT Block** 堆叠而成，结构高度统一。这种统一性带来了：
        *   **更简洁的设计：** 理解和实现更简单。
        *   **更好的可扩展性：** 更容易通过增加 **Block 数量 (深度)**、**隐藏层维度 (宽度)** 或 **注意力头数** 来扩大模型规模。实验表明，**更大的 DiT 模型性能提升显著且规律**，展现了 Transformer 经典的“Scaling Law”。
        *   **硬件优化友好：** 现代 AI 加速硬件（如 TPU, GPU）对密集的矩阵乘法（Transformer 的核心）有极其高效的优化，而对异构操作融合的优化相对复杂。纯 Transformer 结构能更充分地利用硬件算力。

3.  **条件注入的优雅融合：**
    *   在 UNet 中，时间步 `t` 和文本条件（`text_embeddings`）需要以特定方式（如特征调制 `scale/shift`，交叉注意力）注入到不同层级的残差块中，设计相对复杂。
    *   在 DiT 中，条件信息可以更统一、更自然地融入到每个 **DiT Block** 中。通常通过 **自适应层归一化 (Adaptive Layer Norm, AdaLN)** 或其变种（如 **AdaLN-Zero**）来调制整个 Block 的行为。文本条件通常通过 **交叉注意力层** 注入。这种融合方式在 Transformer 框架下显得更原生和高效。

4.  **性能提升的实证：**
    *   论文《Scalable Diffusion Models with Transformers》系统地比较了 DiT 和基于 CNN 的扩散模型（包括类 UNet 结构）。结果表明，在相同的参数量、训练步骤和数据集上，**DiT 模型在 FID (Fréchet Inception Distance) 和 IS (Inception Score) 等指标上显著优于 CNN 骨干**。
    *   更重要的是，**DiT 的 Scaling Law 非常明显**：增大模型规模（更多参数、更多计算）持续带来生成质量的提升，且这种提升趋势在实验范围内没有饱和迹象。这为未来生成模型的发展指明了方向。

5.  **顺应大模型趋势：**
    *   Transformer 已成为 NLP、多模态等领域大模型（LLM）的绝对主导架构。SD3 采用 DiT 标志着图像生成领域也全面拥抱了 Transformer 范式，有利于知识、技术和基础设施（如训练框架、优化技术）的跨领域迁移和统一。

**二、 Stable Diffusion 3 (DiT) 的结构是什么样的？**

SD3 的骨干 DiT 架构可以概括如下（示意图见下方文字描述）：

1.  **输入：** 与 SD1/2 类似，DiT 仍然在**潜空间 (Latent Space)** 中操作。
    *   输入是带噪声的潜变量 `z_t` (形状如 `[B, C, H, W]`，例如 `[batch_size, 4, 64, 64]`)。
    *   同时输入时间步 `t` 的嵌入向量和文本嵌入 `c_text`。

2.  **Patchify (分块嵌入)：**
    *   将 2D 的潜变量 `z_t` 划分为 `P x P` 个非重叠的 patch（类似于 ViT）。例如，`64x64` 的潜变量划分为 `8x8` 的 patch，则得到 `(64/8) * (64/8) = 64` 个 patch。
    *   每个 patch 被展平为一个向量（长度为 `C * P * P`，例如 `4 * 8 * 8 = 256`）。
    *   这些 patch 向量通过一个**线性投影层 (Linear Projection)** 映射到 Transformer 的隐藏维度 `D`（例如 `D=1152`）。
    *   结果是一个**序列的 token**：形状为 `[B, N, D]`，其中 `N = (H/P) * (W/P)` 是 token 的数量（例如 `[batch_size, 64, 1152]`）。
    *   为序列添加标准的**可学习位置编码 (Position Embedding)** 或 **正弦位置编码**，以保留空间位置信息。

3.  **DiT Blocks 堆叠 (核心)：**
    *   序列 token 输入到一系列完全相同的 **DiT Blocks** 中。SD3 使用了**大量**的 DiT Blocks（具体数量取决于模型大小，如 DiT-XL 有 28 层）。
    *   **每个 DiT Block 的核心组件：**
        *   **自适应层归一化 (AdaLN) 或 AdaLN-Zero：**
            *   这是**条件注入的关键**。
            *   首先，将**时间步嵌入 `t`** 和（通常也会包含）**文本嵌入的全局池化向量 `c_text`** 连接或融合（例如通过一个小 MLP）。
            *   用融合后的条件向量预测**缩放因子 `γ`** 和**平移因子 `β`** (维度均为 `D`)。
            *   **AdaLN：** 对 Block 的输入 `x` 应用 LayerNorm，然后用 `γ * LN(x) + β` 进行调制。`γ` 和 `β` 是向量，作用于每个 token 的所有特征维度。
            *   **AdaLN-Zero：** 一种改进版本，通常初始化时令 `γ` 预测分支的最后一层权重初始化为 0，使得 Block 的初始状态是一个恒等函数，有助于训练稳定性。这是 DiT 论文提出的重要技巧。
        *   **多头自注意力 (Multi-head Self-Attention, MSA)：**
            *   让所有 token 之间进行全局交互，建模 patch 之间的长距离依赖关系。
            *   是 Transformer 的核心，也是替代 CNN 局部建模的关键。
        *   **（可选/条件性）交叉注意力 (Cross-Attention, CA)：**
            *   为了融合文本条件 `c_text`。
            *   **Query (`Q`)**: 来自自注意力输出（或 AdaLN 调制后）的 token (`[B, N, D]`)。
            *   **Key (`K`), Value (`V`)**: 来自文本嵌入序列 `c_text` (`[B, L_text, D_text]`，L_text 是文本 token 长度)。通常需要通过一个线性层将 `D_text` 投影到 `D` 以匹配维度。
            *   注意力机制计算图像 token (`Q`) 和文本 token (`K`) 之间的相关性，并用文本信息 (`V`) 加权求和来调制图像特征。
            *   *注意：在基础 DiT 中，文本条件主要通过 AdaLN 注入。但在 SD3 这样的文本到图像模型中，通常会在部分或所有 DiT Block 中显式加入交叉注意力层，以更精细地融合文本信息。具体实现可能结合 AdaLN 和 CA。*
        *   **多层感知机 (MLP)：**
            *   包含两个全连接层和一个激活函数（如 GELU），提供非线性变换能力。
            *   通常在 MLP 之前和之后也应用 AdaLN 或 LayerNorm（取决于具体设计）。
    *   每个 DiT Block 可以抽象为：`x = x + MSA(AdaLN(x, t, c))` 或 `x = x + MSA(AdaLN(x, t, c)) + CA(AdaLN(x, t, c), c_text)`，然后是 `x = x + MLP(AdaLN(x, t, c))`。其中 AdaLN 的位置和是否在 CA 前后使用可能有变体。

4.  **输出层：**
    *   经过所有 DiT Blocks 处理后，得到处理后的 token 序列 `[B, N, D]`。
    *   应用一个**最终的 AdaLN 层**（用 `t` 和 `c_text` 调制）。
    *   通过一个**线性投影层**将每个 token 从隐藏维度 `D` 投影回 `C * P * P` 维度（即一个 patch 展平后的原始维度）。
    *   将投影后的 token 序列 `[B, N, C*P*P]` **重新排列 (Unpatchify)** 成 2D 特征图 `[B, C, H, W]`。
    *   这个 2D 特征图就是 DiT 预测的**噪声 `ε_θ(z_t, t, c_text)`** 或**去噪后的潜变量**（具体目标取决于损失函数定义，通常是预测噪声）。

**三、 结构草图 (文字描述)**

```
          [Input: Noisy Latent z_t (B, C, H, W)]       [Timestep t]       [Text Embeddings c_text]
                     |                                       |                     |
                     v                                       v                     v
             +----------------+                      +----------------+      +------------+
             |   Patchify &   |                      |  Condition     |      |Text Encoder|
             | Linear Project |------------------->  | Fusion (MLP?)  |<-----| (e.g., T5) |
             | (B, N, D)      |                      | (t + c_text)   |      +------------+
             +----------------+                      +----------------+
                     |                                       |
                     | (Add Position Embedding)              |
                     v                                       v
          +------------------------------------------------------+
          |                   DiT Blocks (x L layers)             |
          |                                                      |
          |  +------------+  +------------+       +------------+  |
          |  | DiT Block  |  | DiT Block  |  ...  | DiT Block  |  |
          |  | +--------+ |  | +--------+ |       | +--------+ |  |
          |  | | AdaLN  | |  | | AdaLN  | |       | | AdaLN  | |  |<--- (Condition vector for γ, β)
          |  | +----|---+ |  | +----|---+ |       | +----|---+ |  |
          |  |      v     |  |      v     |       |      v     |  |
          |  | +--------+ |  | +--------+ |       | +--------+ |  |
          |  | |  MSA   | |  | |  MSA   | |       | |  MSA   | |  |
          |  | +----|---+ |  | +----|---+ |       | +----|---+ |  |
          |  |      |     |  |      |     |       |      |     |  |
          |  | +----|---+ |  | +----|---+ |       | +----|---+ |  |<--- (Optional: Cross-Attention with c_text)
          |  | |  CA?  | |  | |  CA?  | |       | |  CA?  | |  |
          |  | +----|---+ |  | +----|---+ |       | +----|---+ |  |
          |  |      v     |  |      v     |       |      v     |  |
          |  | +--------+ |  | +--------+ |       | +--------+ |  |
          |  | | AdaLN  | |  | | AdaLN  | |       | | AdaLN  | |  |<--- (Condition vector for γ, β)
          |  | +----|---+ |  | +----|---+ |       | +----|---+ |  |
          |  |      v     |  |      v     |       |      v     |  |
          |  | +--------+ |  | +--------+ |       | +--------+ |  |
          |  | |  MLP   | |  | |  MLP   | |       | |  MLP   | |  |
          |  | +--------+ |  | +--------+ |       | +--------+ |  |
          |  +------------+  +------------+       +------------+  |
          +------------------------------------------------------+
                     |
                     v
             +-----------------+
             | Final AdaLN     |<------------------- (Condition vector for γ, β)
             +-----------------+
                     |
                     v
             +-----------------+
             | Linear Project  | (B, N, C*P*P)
             +-----------------+
                     |
                     v
             +-----------------+
             |  Unpatchify     | (Reshape to [B, C, H, W])
             +-----------------+
                     |
                     v
        [Output: Predicted Noise ε_θ or Denoised z_0 (B, C, H, W)]
```

**关键点总结：**

1.  **彻底 Transformer 化：** SD3 的核心是堆叠的 **DiT Block**，完全取代了 CNN UNet。基础操作是自注意力和 MLP。
2.  **条件注入：** 时间步 `t` 和文本条件 `c_text` 主要通过 **自适应层归一化 (AdaLN 或 AdaLN-Zero)** 调制每个 Block 的行为。**交叉注意力 (CA)** 通常被显式用于更精细的文本-图像交互。
3.  **潜空间操作：** 输入输出仍然是 VAE 编码/解码的潜变量。
4.  **Patch 处理：** 使用 ViT 风格的 **Patchify/Unpatchify** 将 2D 潜变量转换为 1D token 序列供 Transformer 处理，处理后再还原。
5.  **可扩展性：** 通过增加 Block 数 (L)、隐藏维度 (D)、注意力头数等可以平滑地扩展模型规模，且性能提升显著（Scaling Law）。
6.  **性能驱动：** 放弃 UNet 的根本原因是实证证明 DiT 架构在扩散模型中具有**更优越的性能和可扩展性**，尤其在建模全局一致性和复杂场景方面。

Stable Diffusion 3 采用 DiT 架构标志着文本到图像生成模型进入了一个新的时代，纯 Transformer 架构展现了其在视觉生成任务中替代 CNN 的强大潜力，并为未来更大规模、更高性能的模型铺平了道路。